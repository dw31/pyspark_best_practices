{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Understanding Spark's Lazy Evaluation and Immutability\n",
    "\n",
    "This notebook demonstrates the fundamental concepts of Apache Spark's lazy evaluation and immutability, which form the foundation for functional programming in PySpark.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how lazy evaluation works in Spark\n",
    "- Explore DataFrame immutability and its implications\n",
    "- Learn how these concepts support functional programming patterns\n",
    "- See practical examples of lazy evaluation in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Lazy Evaluation\n",
    "\n",
    "Apache Spark's lazy evaluation means that transformations on DataFrames are not executed immediately. Instead, they build up a **Directed Acyclic Graph (DAG)** of operations that gets optimized and executed only when an **action** is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create sample data for demonstration\n",
    "sample_data = [\n",
    "    (\"Alice\", 25, \"Engineer\"),\n",
    "    (\"Bob\", 30, \"Manager\"),\n",
    "    (\"Charlie\", 35, \"Analyst\"),\n",
    "    (\"Diana\", 28, \"Designer\"),\n",
    "    (\"Eve\", 32, \"Developer\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"role\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(sample_data, schema)\n",
    "print(\"Original DataFrame created\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Lazy Evaluation\n",
    "\n",
    "Let's chain multiple transformations and observe that no actual computation happens until we call an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Applying Transformations (Lazy Evaluation) ===\")\n",
    "\n",
    "# These are all transformations - they build the execution plan but don't execute\n",
    "print(\"1. Filtering records where age > 28...\")\n",
    "filtered_df = df.filter(F.col(\"age\") > 28)\n",
    "\n",
    "print(\"2. Adding a new column 'senior_level'...\")\n",
    "with_senior_df = filtered_df.withColumn(\"senior_level\", \n",
    "                                      F.when(F.col(\"age\") >= 30, \"Senior\").otherwise(\"Junior\"))\n",
    "\n",
    "print(\"3. Selecting specific columns...\")\n",
    "final_df = with_senior_df.select(\"name\", \"age\", \"role\", \"senior_level\")\n",
    "\n",
    "print(\"4. Ordering by age...\")\n",
    "ordered_df = final_df.orderBy(F.col(\"age\").desc())\n",
    "\n",
    "print(\"\\nAll transformations defined! But no computation has happened yet.\")\n",
    "print(\"The execution plan (DAG) has been built, but not executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triggering Execution with Actions\n",
    "\n",
    "Now let's trigger the actual computation by calling an action. This is when Spark's Catalyst optimizer analyzes the entire DAG and creates an optimized execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Triggering Execution with Actions ===\")\n",
    "\n",
    "# This action triggers the execution of all transformations in the chain\n",
    "print(\"Calling show() - this is an ACTION that triggers execution:\")\n",
    "ordered_df.show()\n",
    "\n",
    "print(\"\\nNow all the transformations were executed in an optimized manner!\")\n",
    "\n",
    "# Let's see the execution plan\n",
    "print(\"\\n=== Execution Plan ===\")\n",
    "ordered_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding DataFrame Immutability\n",
    "\n",
    "DataFrames in Spark are immutable - once created, they cannot be modified. Any operation that appears to \"change\" a DataFrame actually creates a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Demonstrating DataFrame Immutability ===\")\n",
    "\n",
    "# Original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# \"Modify\" the DataFrame by adding a column\n",
    "df_with_category = df.withColumn(\"category\", F.lit(\"Employee\"))\n",
    "\n",
    "print(\"\\nDataFrame with added column:\")\n",
    "df_with_category.show()\n",
    "\n",
    "print(\"\\nOriginal DataFrame (unchanged):\")\n",
    "df.show()\n",
    "\n",
    "print(\"Notice: The original DataFrame remains unchanged!\")\n",
    "print(f\"Original DataFrame ID: {id(df)}\")\n",
    "print(f\"Modified DataFrame ID: {id(df_with_category)}\")\n",
    "print(\"These are different objects - immutability preserved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Building a Functional Pipeline\n",
    "\n",
    "Let's create a more complex example that demonstrates how lazy evaluation and immutability support functional programming patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for more realistic demonstration\n",
    "large_sample_data = [\n",
    "    (\"Alice\", 25, \"Engineer\", 75000),\n",
    "    (\"Bob\", 30, \"Manager\", 85000),\n",
    "    (\"Charlie\", 35, \"Analyst\", 65000),\n",
    "    (\"Diana\", 28, \"Designer\", 70000),\n",
    "    (\"Eve\", 32, \"Developer\", 80000),\n",
    "    (\"Frank\", 29, \"Engineer\", 76000),\n",
    "    (\"Grace\", 31, \"Manager\", 90000),\n",
    "    (\"Henry\", 26, \"Analyst\", 62000),\n",
    "    (\"Ivy\", 33, \"Designer\", 72000),\n",
    "    (\"Jack\", 27, \"Developer\", 78000)\n",
    "]\n",
    "\n",
    "large_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"role\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employees_df = spark.createDataFrame(large_sample_data, large_schema)\n",
    "\n",
    "print(\"Employee Dataset:\")\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Transformation Pipeline\n",
    "\n",
    "Now let's build a functional pipeline that demonstrates how transformations compose naturally due to immutability and lazy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_by_age(df):\n",
    "    \"\"\"Pure function: categorizes employees by age group\"\"\"\n",
    "    return df.withColumn(\"age_group\", \n",
    "                        F.when(F.col(\"age\") < 28, \"Young\")\n",
    "                         .when(F.col(\"age\") < 32, \"Mid-Career\")\n",
    "                         .otherwise(\"Senior\"))\n",
    "\n",
    "def calculate_salary_band(df):\n",
    "    \"\"\"Pure function: adds salary band classification\"\"\"\n",
    "    return df.withColumn(\"salary_band\",\n",
    "                        F.when(F.col(\"salary\") < 70000, \"Low\")\n",
    "                         .when(F.col(\"salary\") < 80000, \"Medium\")\n",
    "                         .otherwise(\"High\"))\n",
    "\n",
    "def add_performance_score(df):\n",
    "    \"\"\"Pure function: adds a calculated performance score\"\"\"\n",
    "    return df.withColumn(\"performance_score\",\n",
    "                        (F.col(\"salary\") / 1000 + F.col(\"age\") * 2).cast(\"integer\"))\n",
    "\n",
    "# Build the functional pipeline using method chaining\n",
    "print(\"=== Building Functional Pipeline ===\")\n",
    "\n",
    "# Each transformation returns a new DataFrame (immutability)\n",
    "# The entire chain is lazy-evaluated until an action is called\n",
    "result_df = (employees_df\n",
    "             .transform(categorize_by_age)\n",
    "             .transform(calculate_salary_band) \n",
    "             .transform(add_performance_score)\n",
    "             .filter(F.col(\"performance_score\") > 100)\n",
    "             .select(\"name\", \"age\", \"role\", \"age_group\", \"salary_band\", \"performance_score\")\n",
    "             .orderBy(F.col(\"performance_score\").desc()))\n",
    "\n",
    "print(\"Pipeline built! No execution yet - still lazy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Pipeline\n",
    "\n",
    "Now let's trigger execution and see the results. Notice how Spark optimizes the entire pipeline as one unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Executing the Functional Pipeline ===\")\n",
    "\n",
    "# Trigger execution with an action\n",
    "result_df.show()\n",
    "\n",
    "print(\"\\n=== Pipeline Execution Statistics ===\")\n",
    "print(f\"Number of high-performing employees: {result_df.count()}\")\n",
    "\n",
    "# Let's also see how the original DataFrame is unchanged\n",
    "print(\"\\n=== Original DataFrame (Unchanged) ===\")\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Benefits\n",
    "\n",
    "Let's examine why lazy evaluation and immutability are powerful for functional programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Benefits Demonstration ===\")\n",
    "\n",
    "# 1. Optimization: Spark can optimize the entire chain\n",
    "print(\"1. OPTIMIZATION:\")\n",
    "print(\"Spark's Catalyst optimizer can see the entire transformation chain and optimize it globally.\")\n",
    "result_df.explain()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 2. Reusability: Pure functions can be reused\n",
    "print(\"2. REUSABILITY:\")\n",
    "print(\"Pure transformation functions can be reused with different DataFrames:\")\n",
    "\n",
    "# Create a different dataset\n",
    "test_data = [(\"Test1\", 30, \"Tester\", 72000), (\"Test2\", 35, \"QA\", 68000)]\n",
    "test_df = spark.createDataFrame(test_data, large_schema)\n",
    "\n",
    "# Reuse the same transformations\n",
    "reused_result = (test_df\n",
    "                .transform(categorize_by_age)\n",
    "                .transform(calculate_salary_band))\n",
    "\n",
    "reused_result.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 3. Composability: Functions can be easily combined\n",
    "print(\"3. COMPOSABILITY:\")\n",
    "print(\"Transformations compose naturally due to immutability:\")\n",
    "\n",
    "# We can branch from any point in our pipeline\n",
    "alternate_branch = (employees_df\n",
    "                   .transform(categorize_by_age)\n",
    "                   .filter(F.col(\"age_group\") == \"Young\")\n",
    "                   .select(\"name\", \"age\", \"role\", \"age_group\"))\n",
    "\n",
    "print(\"Young employees branch:\")\n",
    "alternate_branch.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Patterns and Anti-Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== GOOD PATTERNS ===\")\n",
    "\n",
    "# ✅ Good: Chain transformations for readability\n",
    "good_pattern = (employees_df\n",
    "               .filter(F.col(\"salary\") > 70000)\n",
    "               .withColumn(\"bonus\", F.col(\"salary\") * 0.1)\n",
    "               .select(\"name\", \"salary\", \"bonus\")\n",
    "               .orderBy(\"salary\"))\n",
    "\n",
    "print(\"Good: Chained transformations\")\n",
    "good_pattern.show(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "print(\"=== ANTI-PATTERNS TO AVOID ===\")\n",
    "\n",
    "# ❌ Bad: Overusing actions in the middle of transformations\n",
    "print(\"❌ Bad: Don't call actions unnecessarily in the middle of transformations\")\n",
    "print(\"This breaks lazy evaluation and forces premature computation:\")\n",
    "\n",
    "# This is inefficient - calling show() in the middle forces execution\n",
    "temp_df = employees_df.filter(F.col(\"salary\") > 70000)\n",
    "print(\"Intermediate result (forces execution):\")\n",
    "temp_df.show(3)  # Action that forces execution\n",
    "\n",
    "# Then continuing with more transformations\n",
    "final_bad = temp_df.withColumn(\"bonus\", F.col(\"salary\") * 0.1)\n",
    "print(\"Final result:\")\n",
    "final_bad.show(3)\n",
    "\n",
    "print(\"\\nThis pattern prevents Spark from optimizing the entire pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Lazy Evaluation**: Transformations build an execution plan (DAG) without immediate execution\n",
    "2. **Immutability**: DataFrames cannot be modified; operations return new DataFrames\n",
    "3. **Optimization**: Spark's Catalyst optimizer can analyze and optimize the entire transformation chain\n",
    "4. **Functional Programming**: These concepts naturally support pure functions and composition\n",
    "5. **Best Practices**: \n",
    "   - Chain transformations for readability\n",
    "   - Use actions wisely (only when results are needed)\n",
    "   - Extract transformation logic into pure functions\n",
    "   - Leverage immutability for predictable, testable code\n",
    "\n",
    "**Next Steps**: In the next notebook, we'll explore how to embrace pure functions and minimize side effects in PySpark code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try creating your own functional pipeline:\n",
    "1. Start with the employees_df\n",
    "2. Create 2-3 pure transformation functions\n",
    "3. Chain them together using the .transform() method\n",
    "4. Observe that no computation happens until you call an action\n",
    "5. Examine the execution plan using .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "# def your_transformation1(df):\n",
    "#     # Your transformation logic\n",
    "#     pass\n",
    "\n",
    "# def your_transformation2(df):\n",
    "#     # Your transformation logic  \n",
    "#     pass\n",
    "\n",
    "# Exercise pipeline:\n",
    "# your_result = (employees_df\n",
    "#               .transform(your_transformation1)\n",
    "#               .transform(your_transformation2)\n",
    "#               # Add more transformations\n",
    "#               )\n",
    "\n",
    "# Don't forget to trigger execution with an action!\n",
    "# your_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}