{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Leveraging PySpark's Built-in Functions and Higher-Order Functions\n",
    "\n",
    "This notebook demonstrates how to maximize performance and maintain functional purity by using PySpark's built-in functions and higher-order functions instead of UDFs.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the performance benefits of built-in functions over UDFs\n",
    "- Learn to use higher-order functions for complex array operations\n",
    "- Practice transforming UDF-based code to built-in function equivalents\n",
    "- Explore advanced built-in functions for common data processing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: Built-ins vs UDFs\n",
    "\n",
    "PySpark's built-in functions are highly optimized by the Catalyst optimizer and execute in the JVM, avoiding the overhead of Python serialization/deserialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, pandas_udf\n",
    "import time\n",
    "\n",
    "# Create sample data for performance comparison\n",
    "large_data = [(i, f\"user_{i}\", i * 10.5, f\"category_{i % 5}\") for i in range(10000)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "large_df = spark.createDataFrame(large_data, schema)\n",
    "large_df.cache()  # Cache for fair performance comparison\n",
    "\n",
    "print(\"Sample data created:\")\n",
    "large_df.show(5)\n",
    "print(f\"Total records: {large_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Built-in Functions vs UDFs\n",
    "\n",
    "Let's compare the performance of built-in functions versus UDFs for common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Performance Comparison: Built-in vs UDF ===\")\n",
    "\n",
    "# Task: Calculate a derived value based on input\n",
    "\n",
    "# ‚ùå UDF Approach (slower)\n",
    "def calculate_score_udf(value, id_val):\n",
    "    \"\"\"Python UDF - runs in Python interpreter\"\"\"\n",
    "    return (value * 1.5 + id_val * 2) / 10\n",
    "\n",
    "calculate_score_udf_func = udf(calculate_score_udf, DoubleType())\n",
    "\n",
    "def test_udf_performance():\n",
    "    start_time = time.time()\n",
    "    result = large_df.withColumn(\"score_udf\", \n",
    "                                calculate_score_udf_func(F.col(\"value\"), F.col(\"id\")))\n",
    "    count = result.count()  # Force execution\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, count\n",
    "\n",
    "# ‚úÖ Built-in Functions Approach (faster)\n",
    "def test_builtin_performance():\n",
    "    start_time = time.time()\n",
    "    result = large_df.withColumn(\"score_builtin\", \n",
    "                                (F.col(\"value\") * 1.5 + F.col(\"id\") * 2) / 10)\n",
    "    count = result.count()  # Force execution\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, count\n",
    "\n",
    "# Performance test\n",
    "udf_time, udf_count = test_udf_performance()\n",
    "builtin_time, builtin_count = test_builtin_performance()\n",
    "\n",
    "print(f\"UDF Performance: {udf_time:.3f} seconds for {udf_count:,} records\")\n",
    "print(f\"Built-in Performance: {builtin_time:.3f} seconds for {builtin_count:,} records\")\n",
    "print(f\"Performance improvement: {udf_time/builtin_time:.1f}x faster with built-ins\")\n",
    "\n",
    "# Verify results are equivalent\n",
    "comparison_df = (large_df\n",
    "                .withColumn(\"score_udf\", calculate_score_udf_func(F.col(\"value\"), F.col(\"id\")))\n",
    "                .withColumn(\"score_builtin\", (F.col(\"value\") * 1.5 + F.col(\"id\") * 2) / 10)\n",
    "                .withColumn(\"difference\", F.abs(F.col(\"score_udf\") - F.col(\"score_builtin\"))))\n",
    "\n",
    "max_diff = comparison_df.agg(F.max(\"difference\")).collect()[0][0]\n",
    "print(f\"\\nResults verification - Maximum difference: {max_diff} (should be ~0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Functions for Common Operations\n",
    "\n",
    "Let's explore the rich set of built-in functions available in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Built-in Functions Showcase ===\")\n",
    "\n",
    "# Create test data with various data types\n",
    "test_data = [\n",
    "    (1, \"John Doe\", \"john.doe@email.com\", \"2023-01-15\", 1200.50, \"USD\", [\"python\", \"spark\", \"sql\"]),\n",
    "    (2, \"jane smith\", \"JANE.SMITH@EMAIL.COM\", \"2023-02-20\", 1500.75, \"EUR\", [\"java\", \"scala\"]),\n",
    "    (3, \"Bob Johnson\", \"bob@company.org\", \"2023-03-10\", 980.25, \"USD\", [\"python\", \"pandas\", \"numpy\"]),\n",
    "    (4, \"Alice Brown\", \"alice.brown@tech.io\", \"2023-04-05\", 2200.00, \"GBP\", [\"spark\", \"hadoop\", \"kafka\"])\n",
    "]\n",
    "\n",
    "test_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "test_df = spark.createDataFrame(test_data, test_schema)\n",
    "print(\"Test dataset:\")\n",
    "test_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== String Functions ===\")\n",
    "\n",
    "string_transformations = (test_df\n",
    "    # String case operations\n",
    "    .withColumn(\"name_upper\", F.upper(F.col(\"name\")))\n",
    "    .withColumn(\"name_lower\", F.lower(F.col(\"name\")))\n",
    "    .withColumn(\"name_title\", F.initcap(F.col(\"name\")))\n",
    "    \n",
    "    # String manipulation\n",
    "    .withColumn(\"name_length\", F.length(F.col(\"name\")))\n",
    "    .withColumn(\"first_name\", F.split(F.col(\"name\"), \" \").getItem(0))\n",
    "    .withColumn(\"email_domain\", F.split(F.col(\"email\"), \"@\").getItem(1))\n",
    "    \n",
    "    # Pattern matching\n",
    "    .withColumn(\"is_gmail\", F.col(\"email\").contains(\"gmail\"))\n",
    "    .withColumn(\"email_valid\", F.col(\"email\").rlike(r\"^[\\w\\.-]+@[\\w\\.-]+\\.[a-zA-Z]{2,}$\"))\n",
    "    \n",
    "    .select(\"name\", \"name_title\", \"first_name\", \"email\", \"email_domain\", \"is_gmail\", \"email_valid\")\n",
    ")\n",
    "\n",
    "string_transformations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Date and Numeric Functions ===\")\n",
    "\n",
    "date_numeric_transformations = (test_df\n",
    "    # Date functions\n",
    "    .withColumn(\"date_parsed\", F.to_date(F.col(\"date\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"year\", F.year(F.to_date(F.col(\"date\"), \"yyyy-MM-dd\")))\n",
    "    .withColumn(\"month\", F.month(F.to_date(F.col(\"date\"), \"yyyy-MM-dd\")))\n",
    "    .withColumn(\"day_of_year\", F.dayofyear(F.to_date(F.col(\"date\"), \"yyyy-MM-dd\")))\n",
    "    \n",
    "    # Numeric functions\n",
    "    .withColumn(\"amount_rounded\", F.round(F.col(\"amount\"), 0))\n",
    "    .withColumn(\"amount_ceil\", F.ceil(F.col(\"amount\")))\n",
    "    .withColumn(\"amount_floor\", F.floor(F.col(\"amount\")))\n",
    "    .withColumn(\"amount_log\", F.log(F.col(\"amount\")))\n",
    "    \n",
    "    .select(\"id\", \"date\", \"year\", \"month\", \"day_of_year\", \n",
    "           \"amount\", \"amount_rounded\", \"amount_ceil\", \"amount_floor\")\n",
    ")\n",
    "\n",
    "date_numeric_transformations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Conditional Logic with Built-ins ===\")\n",
    "\n",
    "conditional_transformations = (test_df\n",
    "    # Complex when/otherwise logic\n",
    "    .withColumn(\"amount_category\",\n",
    "               F.when(F.col(\"amount\") < 1000, \"Low\")\n",
    "                .when(F.col(\"amount\") < 2000, \"Medium\")\n",
    "                .otherwise(\"High\"))\n",
    "    \n",
    "    # Case-insensitive matching\n",
    "    .withColumn(\"currency_region\",\n",
    "               F.when(F.upper(F.col(\"currency\")) == \"USD\", \"North America\")\n",
    "                .when(F.upper(F.col(\"currency\")) == \"EUR\", \"Europe\")\n",
    "                .when(F.upper(F.col(\"currency\")) == \"GBP\", \"UK\")\n",
    "                .otherwise(\"Other\"))\n",
    "    \n",
    "    # Null handling\n",
    "    .withColumn(\"safe_amount\", F.coalesce(F.col(\"amount\"), F.lit(0.0)))\n",
    "    .withColumn(\"amount_or_default\", F.when(F.col(\"amount\").isNull(), F.lit(999.99)).otherwise(F.col(\"amount\")))\n",
    "    \n",
    "    .select(\"id\", \"amount\", \"amount_category\", \"currency\", \"currency_region\")\n",
    ")\n",
    "\n",
    "conditional_transformations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-Order Functions for Array Operations\n",
    "\n",
    "PySpark provides powerful higher-order functions for working with array and map columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Higher-Order Functions for Arrays ===\")\n",
    "\n",
    "# Array operations using higher-order functions\n",
    "array_operations = (test_df\n",
    "    # Transform array elements\n",
    "    .withColumn(\"skills_upper\", \n",
    "               F.transform(F.col(\"skills\"), lambda x: F.upper(x)))\n",
    "    \n",
    "    # Filter array elements\n",
    "    .withColumn(\"python_skills\", \n",
    "               F.filter(F.col(\"skills\"), lambda x: x.contains(\"python\")))\n",
    "    \n",
    "    # Check if array contains elements matching condition\n",
    "    .withColumn(\"has_python\", \n",
    "               F.exists(F.col(\"skills\"), lambda x: x == \"python\"))\n",
    "    \n",
    "    # Aggregate array elements\n",
    "    .withColumn(\"skills_count\", F.size(F.col(\"skills\")))\n",
    "    .withColumn(\"skills_concat\", \n",
    "               F.array_join(F.col(\"skills\"), \", \"))\n",
    "    \n",
    "    # Array sorting and distinct\n",
    "    .withColumn(\"skills_sorted\", F.array_sort(F.col(\"skills\")))\n",
    "    .withColumn(\"skills_distinct\", F.array_distinct(F.col(\"skills\")))\n",
    "    \n",
    "    .select(\"id\", \"name\", \"skills\", \"skills_upper\", \"python_skills\", \n",
    "           \"has_python\", \"skills_count\", \"skills_concat\")\n",
    ")\n",
    "\n",
    "array_operations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Advanced Array Operations ===\")\n",
    "\n",
    "# More complex array transformations\n",
    "advanced_array_ops = (test_df\n",
    "    # Add skill levels (simulate complex transformation)\n",
    "    .withColumn(\"skill_levels\", \n",
    "               F.transform(F.col(\"skills\"), \n",
    "                         lambda skill: F.concat(skill, F.lit(\"_advanced\"))))\n",
    "    \n",
    "    # Conditional array transformation\n",
    "    .withColumn(\"enhanced_skills\",\n",
    "               F.transform(F.col(\"skills\"),\n",
    "                         lambda skill: F.when(skill == \"python\", \"python_expert\")\n",
    "                                        .when(skill == \"spark\", \"spark_ninja\")\n",
    "                                        .otherwise(skill)))\n",
    "    \n",
    "    # Aggregate with reduce (simulate counting characters)\n",
    "    .withColumn(\"total_skill_chars\",\n",
    "               F.aggregate(F.col(\"skills\"), \n",
    "                         F.lit(0),  # Initial value\n",
    "                         lambda acc, x: acc + F.length(x)))  # Accumulator function\n",
    "    \n",
    "    .select(\"id\", \"name\", \"skills\", \"skill_levels\", \"enhanced_skills\", \"total_skill_chars\")\n",
    ")\n",
    "\n",
    "advanced_array_ops.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting UDF Logic to Built-in Functions\n",
    "\n",
    "Let's practice converting common UDF patterns to built-in function equivalents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Converting UDFs to Built-in Functions ===\")\n",
    "\n",
    "# Example 1: Email validation\n",
    "print(\"\\n1. Email Validation:\")\n",
    "\n",
    "# ‚ùå UDF approach\n",
    "def validate_email_udf(email):\n",
    "    import re\n",
    "    pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.[a-zA-Z]{2,}$'\n",
    "    return bool(re.match(pattern, email))\n",
    "\n",
    "validate_email_udf_func = udf(validate_email_udf, BooleanType())\n",
    "\n",
    "# ‚úÖ Built-in approach\n",
    "def validate_email_builtin(df):\n",
    "    return df.withColumn(\"email_valid_builtin\", \n",
    "                        F.col(\"email\").rlike(r'^[\\w\\.-]+@[\\w\\.-]+\\.[a-zA-Z]{2,}$'))\n",
    "\n",
    "# Compare results\n",
    "comparison1 = (test_df\n",
    "              .withColumn(\"email_valid_udf\", validate_email_udf_func(F.col(\"email\")))\n",
    "              .transform(validate_email_builtin)\n",
    "              .select(\"email\", \"email_valid_udf\", \"email_valid_builtin\"))\n",
    "\n",
    "comparison1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Complex scoring logic\n",
    "print(\"\\n2. Complex Scoring Logic:\")\n",
    "\n",
    "# ‚ùå UDF approach\n",
    "def calculate_user_score_udf(amount, skills_count, email_domain):\n",
    "    base_score = amount / 100\n",
    "    skill_bonus = skills_count * 10\n",
    "    domain_bonus = 5 if email_domain in ['gmail.com', 'company.org'] else 0\n",
    "    return min(base_score + skill_bonus + domain_bonus, 100)\n",
    "\n",
    "calculate_score_udf_func = udf(calculate_user_score_udf, DoubleType())\n",
    "\n",
    "# ‚úÖ Built-in approach\n",
    "def calculate_user_score_builtin(df):\n",
    "    return (df\n",
    "            # Extract domain first\n",
    "            .withColumn(\"email_domain\", F.split(F.col(\"email\"), \"@\").getItem(1))\n",
    "            .withColumn(\"skills_count\", F.size(F.col(\"skills\")))\n",
    "            \n",
    "            # Calculate components using built-ins\n",
    "            .withColumn(\"base_score\", F.col(\"amount\") / 100)\n",
    "            .withColumn(\"skill_bonus\", F.col(\"skills_count\") * 10)\n",
    "            .withColumn(\"domain_bonus\", \n",
    "                       F.when(F.col(\"email_domain\").isin(['gmail.com', 'company.org']), 5)\n",
    "                        .otherwise(0))\n",
    "            \n",
    "            # Combine and cap at 100\n",
    "            .withColumn(\"user_score_builtin\", \n",
    "                       F.least(F.col(\"base_score\") + F.col(\"skill_bonus\") + F.col(\"domain_bonus\"), \n",
    "                              F.lit(100))))\n",
    "\n",
    "# Compare results\n",
    "comparison2 = (test_df\n",
    "              .withColumn(\"email_domain\", F.split(F.col(\"email\"), \"@\").getItem(1))\n",
    "              .withColumn(\"skills_count\", F.size(F.col(\"skills\")))\n",
    "              .withColumn(\"user_score_udf\", \n",
    "                         calculate_score_udf_func(F.col(\"amount\"), \n",
    "                                                 F.col(\"skills_count\"), \n",
    "                                                 F.col(\"email_domain\")))\n",
    "              .transform(calculate_user_score_builtin)\n",
    "              .select(\"id\", \"name\", \"amount\", \"skills_count\", \"email_domain\",\n",
    "                     \"user_score_udf\", \"user_score_builtin\"))\n",
    "\n",
    "comparison2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Pandas UDFs\n",
    "\n",
    "Sometimes UDFs are necessary. When you must use them, Pandas UDFs are preferred over regular Python UDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== When UDFs Are Necessary - Use Pandas UDFs ===\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Scenario: Complex statistical calculation not available as built-in\n",
    "# Example: Calculate rolling z-score within groups\n",
    "\n",
    "@pandas_udf(returnType=\"double\")\n",
    "def calculate_zscore_pandas_udf(values: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Pandas UDF for complex statistical calculations\n",
    "    Vectorized execution is much faster than row-by-row UDF\n",
    "    \"\"\"\n",
    "    mean_val = values.mean()\n",
    "    std_val = values.std()\n",
    "    if std_val == 0:\n",
    "        return pd.Series([0.0] * len(values))\n",
    "    return (values - mean_val) / std_val\n",
    "\n",
    "# Create test data with groups\n",
    "group_data = [\n",
    "    (\"A\", 10), (\"A\", 15), (\"A\", 20), (\"A\", 25), (\"A\", 30),\n",
    "    (\"B\", 100), (\"B\", 110), (\"B\", 120), (\"B\", 130), (\"B\", 140),\n",
    "    (\"C\", 5), (\"C\", 8), (\"C\", 12), (\"C\", 15), (\"C\", 18)\n",
    "]\n",
    "\n",
    "group_df = spark.createDataFrame(group_data, [\"group\", \"value\"])\n",
    "\n",
    "# Use Pandas UDF with window function\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"group\").orderBy(\"value\")\n",
    "\n",
    "result_with_zscore = (group_df\n",
    "                     .withColumn(\"zscore\", \n",
    "                                calculate_zscore_pandas_udf(F.col(\"value\")).over(window_spec)))\n",
    "\n",
    "print(\"Z-score calculation using Pandas UDF:\")\n",
    "result_with_zscore.orderBy(\"group\", \"value\").show()\n",
    "\n",
    "print(\"\\n‚úÖ Use Pandas UDFs when:\")\n",
    "print(\"- Complex statistical operations not available as built-ins\")\n",
    "print(\"- Leveraging existing Pandas/NumPy libraries\")\n",
    "print(\"- Vectorized operations that can't be expressed with built-ins\")\n",
    "print(\"\\n‚ùå Avoid regular Python UDFs due to row-by-row processing overhead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Function Library\n",
    "\n",
    "Let's create a library of reusable transformation functions using built-in functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Building a Reusable Function Library ===\")\n",
    "\n",
    "class DataTransformations:\n",
    "    \"\"\"Library of pure transformation functions using built-in functions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def standardize_names(df, name_column=\"name\"):\n",
    "        \"\"\"Standardize name format to Title Case\"\"\"\n",
    "        return df.withColumn(f\"{name_column}_standardized\", \n",
    "                           F.initcap(F.trim(F.col(name_column))))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_email_components(df, email_column=\"email\"):\n",
    "        \"\"\"Extract username and domain from email\"\"\"\n",
    "        return (df\n",
    "               .withColumn(\"email_username\", F.split(F.col(email_column), \"@\").getItem(0))\n",
    "               .withColumn(\"email_domain\", F.split(F.col(email_column), \"@\").getItem(1))\n",
    "               .withColumn(\"email_tld\", \n",
    "                          F.split(F.col(\"email_domain\"), \"\\\\.\").getItem(-1)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def categorize_amounts(df, amount_column=\"amount\", \n",
    "                          thresholds=[1000, 2000, 5000]):\n",
    "        \"\"\"Categorize amounts into buckets\"\"\"\n",
    "        condition = F.when(F.col(amount_column) < thresholds[0], \"Low\")\n",
    "        \n",
    "        for i, threshold in enumerate(thresholds[1:], 1):\n",
    "            condition = condition.when(F.col(amount_column) < threshold, \n",
    "                                     f\"Medium_{i}\")\n",
    "        \n",
    "        return df.withColumn(f\"{amount_column}_category\", \n",
    "                           condition.otherwise(\"High\"))\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_array_metrics(df, array_column=\"skills\"):\n",
    "        \"\"\"Add various metrics for array columns\"\"\"\n",
    "        return (df\n",
    "               .withColumn(f\"{array_column}_count\", F.size(F.col(array_column)))\n",
    "               .withColumn(f\"{array_column}_unique_count\", \n",
    "                          F.size(F.array_distinct(F.col(array_column))))\n",
    "               .withColumn(f\"{array_column}_joined\", \n",
    "                          F.array_join(F.col(array_column), \", \"))\n",
    "               .withColumn(f\"{array_column}_sorted\", \n",
    "                          F.array_sort(F.col(array_column))))\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_date_features(df, date_column=\"date\"):\n",
    "        \"\"\"Add comprehensive date-based features\"\"\"\n",
    "        date_col = F.to_date(F.col(date_column), \"yyyy-MM-dd\")\n",
    "        \n",
    "        return (df\n",
    "               .withColumn(f\"{date_column}_parsed\", date_col)\n",
    "               .withColumn(f\"{date_column}_year\", F.year(date_col))\n",
    "               .withColumn(f\"{date_column}_month\", F.month(date_col))\n",
    "               .withColumn(f\"{date_column}_quarter\", F.quarter(date_col))\n",
    "               .withColumn(f\"{date_column}_day_of_week\", F.dayofweek(date_col))\n",
    "               .withColumn(f\"{date_column}_day_of_year\", F.dayofyear(date_col))\n",
    "               .withColumn(f\"{date_column}_is_weekend\", \n",
    "                          F.dayofweek(date_col).isin([1, 7])))\n",
    "\n",
    "# Demonstrate the function library\n",
    "print(\"\\nApplying transformation library:\")\n",
    "\n",
    "enhanced_df = (test_df\n",
    "              .transform(DataTransformations.standardize_names)\n",
    "              .transform(DataTransformations.extract_email_components)\n",
    "              .transform(DataTransformations.categorize_amounts)\n",
    "              .transform(DataTransformations.add_array_metrics)\n",
    "              .transform(DataTransformations.add_date_features))\n",
    "\n",
    "print(f\"Original columns: {len(test_df.columns)}\")\n",
    "print(f\"Enhanced columns: {len(enhanced_df.columns)}\")\n",
    "print(f\"New columns added: {len(enhanced_df.columns) - len(test_df.columns)}\")\n",
    "\n",
    "# Show sample of enhanced data\n",
    "enhanced_df.select(\"id\", \"name_standardized\", \"email_domain\", \"email_tld\",\n",
    "                  \"amount_category\", \"skills_count\", \"date_quarter\", \"date_is_weekend\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Best Practices Summary\n",
    "\n",
    "Let's summarize the key performance considerations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Performance Best Practices Summary ===\")\n",
    "\n",
    "performance_table = [\n",
    "    (\"Built-in Functions\", \"High\", \"JVM execution, Catalyst optimization\", \"Always prefer\"),\n",
    "    (\"Pandas UDFs\", \"Medium\", \"Vectorized, Arrow serialization\", \"When built-ins insufficient\"),\n",
    "    (\"Python UDFs\", \"Low\", \"Row-by-row Python execution\", \"Avoid if possible\"),\n",
    "    (\"Higher-order Functions\", \"High\", \"Array operations in JVM\", \"For complex array logic\"),\n",
    "    (\"SQL Functions\", \"High\", \"Native Catalyst optimization\", \"Alternative to DataFrame API\")\n",
    "]\n",
    "\n",
    "perf_schema = StructType([\n",
    "    StructField(\"Function_Type\", StringType(), True),\n",
    "    StructField(\"Performance\", StringType(), True),\n",
    "    StructField(\"Execution_Details\", StringType(), True),\n",
    "    StructField(\"Usage_Recommendation\", StringType(), True)\n",
    "])\n",
    "\n",
    "perf_df = spark.createDataFrame(performance_table, perf_schema)\n",
    "perf_df.show(truncate=False)\n",
    "\n",
    "print(\"\\nüéØ Key Takeaways:\")\n",
    "print(\"1. Built-in functions are optimized by Catalyst and execute in JVM\")\n",
    "print(\"2. Higher-order functions enable complex array operations without UDFs\")\n",
    "print(\"3. When UDFs are necessary, prefer Pandas UDFs for vectorization\")\n",
    "print(\"4. Most data transformations can be expressed with built-in functions\")\n",
    "print(\"5. Performance improvement can be 2-10x with built-ins vs UDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Built-in Function Advantages**:\n",
    "   - JVM execution (no Python serialization overhead)\n",
    "   - Catalyst optimizer can analyze and optimize\n",
    "   - Significant performance improvements over UDFs\n",
    "\n",
    "2. **Higher-Order Functions**:\n",
    "   - Enable complex array and map operations\n",
    "   - Functions like `transform()`, `filter()`, `exists()`, `aggregate()`\n",
    "   - Maintain functional programming patterns\n",
    "\n",
    "3. **UDF Guidelines**:\n",
    "   - Avoid Python UDFs when possible\n",
    "   - Use Pandas UDFs for vectorized operations when built-ins are insufficient\n",
    "   - Reserve for truly complex logic not expressible with built-ins\n",
    "\n",
    "4. **Function Library Design**:\n",
    "   - Create reusable transformation functions using built-ins\n",
    "   - Maintain functional purity and composability\n",
    "   - Document performance characteristics\n",
    "\n",
    "**Next Steps**: In the next notebook, we'll explore effective chaining and composition patterns to build complex, readable transformation pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Practice converting UDF logic to built-in functions:\n",
    "\n",
    "1. Write a UDF that calculates a \"risk score\" based on multiple factors\n",
    "2. Convert the same logic using only built-in functions\n",
    "3. Compare performance between the two approaches\n",
    "4. Create a higher-order function to process an array of risk factors\n",
    "5. Build a reusable transformation function for your domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "\n",
    "# 1. UDF approach\n",
    "def calculate_risk_score_udf(amount, age, transaction_count):\n",
    "    \"\"\"Calculate risk score using UDF\"\"\"\n",
    "    # Your UDF implementation\n",
    "    pass\n",
    "\n",
    "# 2. Built-in functions approach\n",
    "def calculate_risk_score_builtin(df):\n",
    "    \"\"\"Calculate same risk score using built-in functions\"\"\"\n",
    "    # Your built-in implementation\n",
    "    pass\n",
    "\n",
    "# 3. Performance comparison\n",
    "def compare_performance(df):\n",
    "    \"\"\"Compare performance of both approaches\"\"\"\n",
    "    # Your performance test\n",
    "    pass\n",
    "\n",
    "# 4. Higher-order function for arrays\n",
    "def process_risk_factors(df, risk_factors_column):\n",
    "    \"\"\"Process array of risk factors using higher-order functions\"\"\"\n",
    "    # Your higher-order function implementation\n",
    "    pass\n",
    "\n",
    "# Run your exercise\n",
    "# compare_performance(your_test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}