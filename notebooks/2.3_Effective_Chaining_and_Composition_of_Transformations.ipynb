{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Effective Chaining and Composition of Transformations\n",
    "\n",
    "This notebook demonstrates advanced patterns for chaining and composing PySpark transformations while maintaining code readability and maintainability.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master transformation chaining patterns in PySpark\n",
    "- Use schema contracts with structured `select` statements\n",
    "- Balance functional composition with code readability\n",
    "- Break down complex transformations into manageable functions\n",
    "- Apply best practices for method chaining\n",
    "- Leverage `.transform()` for clean pipeline composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local development: Uncomment the next line\n",
    "# %run 00_Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Chaining and Composition Matter\n",
    "\n",
    "**Functional Composition** is the heart of functional programming - combining simple functions to build complex operations.\n",
    "\n",
    "**In PySpark**:\n",
    "- Transformations return new DataFrames (immutability)\n",
    "- Lazy evaluation allows building complex transformation chains without execution overhead\n",
    "- Catalyst optimizer analyzes the entire chain for optimization\n",
    "\n",
    "**Benefits**:\n",
    "1. **Declarative Code**: Express *what* you want, not *how* to get it\n",
    "2. **Optimizable**: Spark optimizes the entire transformation chain\n",
    "3. **Composable**: Build complex pipelines from simple building blocks\n",
    "4. **Testable**: Each transformation can be tested independently\n",
    "\n",
    "**Challenge**: Balance composition power with code readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from typing import List, Callable\n",
    "import random\n",
    "\n",
    "# Create sample employee data for demonstrations\n",
    "def create_employee_data(num_records: int = 1000) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Generate sample employee data.\n",
    "    Pure function - deterministic data generation.\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "    \n",
    "    departments = ['Engineering', 'Sales', 'Marketing', 'HR', 'Finance', 'Operations']\n",
    "    locations = ['New York', 'San Francisco', 'Austin', 'Seattle', 'Boston']\n",
    "    job_levels = ['Junior', 'Mid', 'Senior', 'Lead', 'Principal']\n",
    "    \n",
    "    data = []\n",
    "    for i in range(num_records):\n",
    "        data.append((\n",
    "            f\"EMP{i+1:05d}\",  # employee_id\n",
    "            f\"Employee {i+1}\",  # name\n",
    "            random.choice(departments),  # department\n",
    "            random.choice(locations),  # location\n",
    "            random.choice(job_levels),  # job_level\n",
    "            round(random.uniform(50000, 200000), 2),  # salary\n",
    "            random.randint(0, 20),  # years_experience\n",
    "            random.randint(0, 30),  # vacation_days\n",
    "            f\"2020-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}\",  # hire_date\n",
    "            random.choice([True, False])  # is_remote\n",
    "        ))\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"employee_id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"department\", StringType(), False),\n",
    "        StructField(\"location\", StringType(), False),\n",
    "        StructField(\"job_level\", StringType(), False),\n",
    "        StructField(\"salary\", DoubleType(), False),\n",
    "        StructField(\"years_experience\", IntegerType(), False),\n",
    "        StructField(\"vacation_days\", IntegerType(), False),\n",
    "        StructField(\"hire_date\", StringType(), False),\n",
    "        StructField(\"is_remote\", BooleanType(), False)\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "# Generate sample data\n",
    "employees_df = create_employee_data(500)\n",
    "employees_df = employees_df.withColumn(\"hire_date\", F.to_date(\"hire_date\"))\n",
    "\n",
    "print(f\"Generated {employees_df.count():,} employee records\")\n",
    "print(\"\\nSample data:\")\n",
    "employees_df.show(5, truncate=False)\n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chaining Patterns\n",
    "\n",
    "PySpark naturally supports method chaining due to immutability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Basic Method Chaining ===\")\n",
    "\n",
    "# ✅ GOOD: Simple, readable chain\n",
    "result = (employees_df\n",
    "    .filter(F.col(\"department\") == \"Engineering\")\n",
    "    .filter(F.col(\"salary\") > 100000)\n",
    "    .select(\"employee_id\", \"name\", \"salary\", \"job_level\")\n",
    "    .orderBy(F.desc(\"salary\"))\n",
    ")\n",
    "\n",
    "print(\"High-salary engineers:\")\n",
    "result.show(5)\n",
    "\n",
    "print(\"\\n✅ Why this works well:\")\n",
    "print(\"  - Each method returns a new DataFrame\")\n",
    "print(\"  - Operations are clearly sequenced\")\n",
    "print(\"  - Easy to read top-to-bottom\")\n",
    "print(\"  - Lazy evaluation - nothing executed until show()\")\n",
    "print(\"  - Catalyst optimizer sees entire chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anti-Pattern: Overly Long Chains\n",
    "\n",
    "While chaining is powerful, excessive chaining can harm readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ❌ ANTI-PATTERN: Overly Long Chain ===\")\n",
    "\n",
    "# ❌ BAD: Too many operations in one chain (hard to read and debug)\n",
    "bad_result = (employees_df\n",
    "    .filter(F.col(\"department\").isin([\"Engineering\", \"Sales\"]))\n",
    "    .filter(F.col(\"years_experience\") >= 3)\n",
    "    .withColumn(\"salary_grade\", F.when(F.col(\"salary\") < 75000, \"L1\").when(F.col(\"salary\") < 100000, \"L2\").when(F.col(\"salary\") < 150000, \"L3\").otherwise(\"L4\"))\n",
    "    .withColumn(\"bonus\", F.when(F.col(\"job_level\") == \"Senior\", F.col(\"salary\") * 0.15).when(F.col(\"job_level\") == \"Lead\", F.col(\"salary\") * 0.20).when(F.col(\"job_level\") == \"Principal\", F.col(\"salary\") * 0.25).otherwise(F.col(\"salary\") * 0.10))\n",
    "    .withColumn(\"total_comp\", F.col(\"salary\") + F.col(\"bonus\"))\n",
    "    .withColumn(\"tenure_months\", F.months_between(F.current_date(), F.col(\"hire_date\")))\n",
    "    .withColumn(\"is_high_performer\", (F.col(\"salary\") > 120000) & (F.col(\"years_experience\") > 5))\n",
    "    .filter(F.col(\"total_comp\") > 100000)\n",
    "    .groupBy(\"department\", \"salary_grade\").agg(F.avg(\"total_comp\").alias(\"avg_comp\"), F.count(\"*\").alias(\"employee_count\"))\n",
    "    .orderBy(F.desc(\"avg_comp\"))\n",
    ")\n",
    "\n",
    "print(\"Result (but hard to understand how we got here):\")\n",
    "bad_result.show(5)\n",
    "\n",
    "print(\"\\n⚠️  Problems with this approach:\")\n",
    "print(\"  - Hard to read and understand\")\n",
    "print(\"  - Difficult to debug intermediate steps\")\n",
    "print(\"  - Complex withColumn expressions are unreadable\")\n",
    "print(\"  - Can't easily test individual transformations\")\n",
    "print(\"  - Violates 'max 5 statements' guideline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practice: Breaking Down Complex Chains\n",
    "\n",
    "Extract complex logic into named, reusable functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ✅ BEST PRACTICE: Modular Transformation Functions ===\")\n",
    "\n",
    "# ✅ GOOD: Extract complex logic into pure functions\n",
    "\n",
    "def add_salary_grade(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pure function: Categorize employees by salary grade.\n",
    "    Single responsibility - easy to test.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"salary_grade\",\n",
    "        F.when(F.col(\"salary\") < 75000, \"L1\")\n",
    "         .when(F.col(\"salary\") < 100000, \"L2\")\n",
    "         .when(F.col(\"salary\") < 150000, \"L3\")\n",
    "         .otherwise(\"L4\")\n",
    "    )\n",
    "\n",
    "def calculate_bonus(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pure function: Calculate performance bonus by job level.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"bonus\",\n",
    "        F.when(F.col(\"job_level\") == \"Senior\", F.col(\"salary\") * 0.15)\n",
    "         .when(F.col(\"job_level\") == \"Lead\", F.col(\"salary\") * 0.20)\n",
    "         .when(F.col(\"job_level\") == \"Principal\", F.col(\"salary\") * 0.25)\n",
    "         .otherwise(F.col(\"salary\") * 0.10)\n",
    "    )\n",
    "\n",
    "def calculate_total_compensation(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pure function: Calculate total compensation.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"total_comp\", F.col(\"salary\") + F.col(\"bonus\"))\n",
    "\n",
    "def add_tenure_metrics(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pure function: Add tenure-related metrics.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"tenure_months\", \n",
    "                        F.months_between(F.current_date(), F.col(\"hire_date\")))\n",
    "\n",
    "def identify_high_performers(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pure function: Identify high performers.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"is_high_performer\",\n",
    "        (F.col(\"salary\") > 120000) & (F.col(\"years_experience\") > 5)\n",
    "    )\n",
    "\n",
    "# ✅ GOOD: Clean, readable chain using modular functions\n",
    "good_result = (employees_df\n",
    "    .filter(F.col(\"department\").isin([\"Engineering\", \"Sales\"]))\n",
    "    .filter(F.col(\"years_experience\") >= 3)\n",
    "    .transform(add_salary_grade)\n",
    "    .transform(calculate_bonus)\n",
    "    .transform(calculate_total_compensation)\n",
    "    .transform(add_tenure_metrics)\n",
    "    .transform(identify_high_performers)\n",
    "    .filter(F.col(\"total_comp\") > 100000)\n",
    "    .groupBy(\"department\", \"salary_grade\")\n",
    "    .agg(\n",
    "        F.avg(\"total_comp\").alias(\"avg_comp\"),\n",
    "        F.count(\"*\").alias(\"employee_count\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"avg_comp\"))\n",
    ")\n",
    "\n",
    "print(\"Result (with readable transformation chain):\")\n",
    "good_result.show(5)\n",
    "\n",
    "print(\"\\n✅ Benefits of modular functions:\")\n",
    "print(\"  - Each function has single responsibility\")\n",
    "print(\"  - Easy to test independently\")\n",
    "print(\"  - Reusable across pipelines\")\n",
    "print(\"  - Self-documenting code\")\n",
    "print(\"  - Chain remains readable\")\n",
    "print(\"  - Uses .transform() for clean composition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Contracts with Structured Select Statements\n",
    "\n",
    "Use `select` statements to explicitly define schema contracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Schema Contracts with Select ===\")\n",
    "\n",
    "# ✅ GOOD: Select as schema contract at pipeline boundaries\n",
    "def prepare_employee_summary(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform employee data with explicit output schema.\n",
    "    \"\"\"\n",
    "    # Input schema contract (implicit - could add assertions)\n",
    "    required_columns = {\"employee_id\", \"name\", \"department\", \"salary\", \"job_level\"}\n",
    "    assert required_columns.issubset(set(df.columns)), \"Missing required columns\"\n",
    "    \n",
    "    # Transformations\n",
    "    result = (df\n",
    "        .transform(add_salary_grade)\n",
    "        .transform(calculate_bonus)\n",
    "        .transform(calculate_total_compensation)\n",
    "    )\n",
    "    \n",
    "    # Output schema contract (explicit select)\n",
    "    return result.select(\n",
    "        F.col(\"employee_id\"),\n",
    "        F.col(\"name\"),\n",
    "        F.col(\"department\"),\n",
    "        F.col(\"job_level\"),\n",
    "        F.col(\"salary\"),\n",
    "        F.col(\"salary_grade\"),\n",
    "        F.col(\"bonus\"),\n",
    "        F.col(\"total_comp\").alias(\"total_compensation\")\n",
    "    )\n",
    "\n",
    "# Test the function\n",
    "summary_df = prepare_employee_summary(employees_df)\n",
    "\n",
    "print(\"Employee summary with explicit schema:\")\n",
    "summary_df.show(5)\n",
    "print(\"\\nOutput schema:\")\n",
    "summary_df.printSchema()\n",
    "\n",
    "print(\"\\n✅ Schema contract benefits:\")\n",
    "print(\"  - Explicit output schema definition\")\n",
    "print(\"  - Downstream consumers know what to expect\")\n",
    "print(\"  - Easy to spot breaking changes\")\n",
    "print(\"  - Self-documenting data contracts\")\n",
    "print(\"  - Helps Catalyst optimizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Select Statement Guidelines\n",
    "\n",
    "Best practices for structuring select statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Select Statement Best Practices ===\")\n",
    "\n",
    "# ❌ BAD: Complex logic inside select (hard to test)\n",
    "bad_select = employees_df.select(\n",
    "    F.col(\"employee_id\"),\n",
    "    F.col(\"name\"),\n",
    "    F.when(F.col(\"salary\") < 75000, \"L1\").when(F.col(\"salary\") < 100000, \"L2\").when(F.col(\"salary\") < 150000, \"L3\").otherwise(\"L4\").alias(\"salary_grade\"),\n",
    "    (F.col(\"salary\") * F.when(F.col(\"job_level\") == \"Senior\", 0.15).when(F.col(\"job_level\") == \"Lead\", 0.20).otherwise(0.10)).alias(\"bonus\"),\n",
    "    (F.col(\"salary\") + (F.col(\"salary\") * 0.10)).alias(\"total_comp\")\n",
    ")\n",
    "\n",
    "print(\"❌ Complex select (hard to read):\")\n",
    "bad_select.show(3)\n",
    "\n",
    "# ✅ GOOD: Simple select with one function per column\n",
    "# First, compute complex columns\n",
    "computed_df = (employees_df\n",
    "    .transform(add_salary_grade)\n",
    "    .transform(calculate_bonus)\n",
    "    .transform(calculate_total_compensation)\n",
    ")\n",
    "\n",
    "# Then, select with simple column references\n",
    "good_select = computed_df.select(\n",
    "    F.col(\"employee_id\"),\n",
    "    F.col(\"name\"),\n",
    "    F.col(\"department\"),\n",
    "    F.col(\"salary\"),\n",
    "    F.col(\"salary_grade\"),\n",
    "    F.col(\"bonus\"),\n",
    "    F.col(\"total_comp\")\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Simple select (readable):\")\n",
    "good_select.show(3)\n",
    "\n",
    "print(\"\\n✅ Select statement guidelines:\")\n",
    "print(\"  1. Keep select statements simple\")\n",
    "print(\"  2. One function per selected column (ideally just F.col())\")\n",
    "print(\"  3. Complex expressions → separate withColumn or function\")\n",
    "print(\"  4. Use select at beginning (input schema) and end (output schema)\")\n",
    "print(\"  5. Avoid nesting complex when() logic in select\")\n",
    "print(\"  6. Use aliases for clarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Limit Guidelines\n",
    "\n",
    "Balance between composition power and readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Chaining Limit Guidelines ===\")\n",
    "\n",
    "# ✅ GOOD: Chain of 5 or fewer statements\n",
    "short_chain = (employees_df\n",
    "    .filter(F.col(\"department\") == \"Engineering\")\n",
    "    .withColumn(\"bonus\", F.col(\"salary\") * 0.10)\n",
    "    .select(\"employee_id\", \"name\", \"salary\", \"bonus\")\n",
    "    .orderBy(F.desc(\"salary\"))\n",
    ")\n",
    "\n",
    "print(\"✅ Good: Short chain (4 operations)\")\n",
    "print(\"Easy to read and understand at a glance\")\n",
    "\n",
    "# ⚠️  ACCEPTABLE: Longer chain with .transform() and named functions\n",
    "medium_chain = (employees_df\n",
    "    .filter(F.col(\"department\").isin([\"Engineering\", \"Sales\"]))\n",
    "    .filter(F.col(\"years_experience\") >= 3)\n",
    "    .transform(add_salary_grade)  # Named function - self-documenting\n",
    "    .transform(calculate_bonus)    # Named function - self-documenting\n",
    "    .transform(calculate_total_compensation)  # Named function\n",
    "    .select(\"employee_id\", \"name\", \"department\", \"salary_grade\", \"total_comp\")\n",
    "    .orderBy(F.desc(\"total_comp\"))\n",
    ")\n",
    "\n",
    "print(\"\\n⚠️  Acceptable: Medium chain (7 operations)\")\n",
    "print(\"Still readable due to named .transform() functions\")\n",
    "\n",
    "# ✅ BEST: Break very long chains into logical steps\n",
    "# Step 1: Filtering\n",
    "filtered_df = (employees_df\n",
    "    .filter(F.col(\"department\").isin([\"Engineering\", \"Sales\"]))\n",
    "    .filter(F.col(\"years_experience\") >= 3)\n",
    ")\n",
    "\n",
    "# Step 2: Enrichment\n",
    "enriched_df = (filtered_df\n",
    "    .transform(add_salary_grade)\n",
    "    .transform(calculate_bonus)\n",
    "    .transform(calculate_total_compensation)\n",
    "    .transform(add_tenure_metrics)\n",
    ")\n",
    "\n",
    "# Step 3: Aggregation\n",
    "aggregated_df = (enriched_df\n",
    "    .groupBy(\"department\", \"salary_grade\")\n",
    "    .agg(\n",
    "        F.avg(\"total_comp\").alias(\"avg_compensation\"),\n",
    "        F.count(\"*\").alias(\"employee_count\"),\n",
    "        F.max(\"tenure_months\").alias(\"max_tenure\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 4: Output formatting\n",
    "final_df = (aggregated_df\n",
    "    .orderBy(F.desc(\"avg_compensation\"))\n",
    "    .select(\n",
    "        F.col(\"department\"),\n",
    "        F.col(\"salary_grade\"),\n",
    "        F.col(\"employee_count\"),\n",
    "        F.round(\"avg_compensation\", 2).alias(\"avg_compensation\"),\n",
    "        F.round(\"max_tenure\", 0).alias(\"max_tenure_months\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Best: Logical grouping of operations\")\n",
    "final_df.show(5)\n",
    "\n",
    "print(\"\\n📋 Chaining Guidelines:\")\n",
    "print(\"  ✅ 1-5 operations: Single chain is fine\")\n",
    "print(\"  ⚠️  6-10 operations: Use .transform() with named functions\")\n",
    "print(\"  ❌ 10+ operations: Break into logical step variables\")\n",
    "print(\"  💡 Use meaningful variable names for intermediate steps\")\n",
    "print(\"  💡 Group operations by logical phase (filter, enrich, aggregate, format)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-Order Functions for Composition\n",
    "\n",
    "Create composable transformation pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Higher-Order Function Composition ===\")\n",
    "\n",
    "def compose(*functions: Callable[[DataFrame], DataFrame]) -> Callable[[DataFrame], DataFrame]:\n",
    "    \"\"\"\n",
    "    Compose multiple transformation functions into a single function.\n",
    "    Higher-order function that returns a function.\n",
    "    \n",
    "    Example:\n",
    "        pipeline = compose(add_salary_grade, calculate_bonus, calculate_total_compensation)\n",
    "        result = pipeline(df)\n",
    "    \"\"\"\n",
    "    def composed_function(df: DataFrame) -> DataFrame:\n",
    "        result = df\n",
    "        for func in functions:\n",
    "            result = func(result)\n",
    "        return result\n",
    "    return composed_function\n",
    "\n",
    "# Create reusable pipelines\n",
    "compensation_pipeline = compose(\n",
    "    add_salary_grade,\n",
    "    calculate_bonus,\n",
    "    calculate_total_compensation\n",
    ")\n",
    "\n",
    "full_enrichment_pipeline = compose(\n",
    "    add_salary_grade,\n",
    "    calculate_bonus,\n",
    "    calculate_total_compensation,\n",
    "    add_tenure_metrics,\n",
    "    identify_high_performers\n",
    ")\n",
    "\n",
    "# Use the composed pipelines\n",
    "print(\"Using composed compensation pipeline:\")\n",
    "comp_result = compensation_pipeline(employees_df)\n",
    "comp_result.select(\"employee_id\", \"name\", \"salary\", \"salary_grade\", \"bonus\", \"total_comp\").show(5)\n",
    "\n",
    "print(\"\\nUsing full enrichment pipeline:\")\n",
    "full_result = full_enrichment_pipeline(employees_df)\n",
    "print(f\"Added columns: {[c for c in full_result.columns if c not in employees_df.columns]}\")\n",
    "\n",
    "print(\"\\n✅ Benefits of higher-order composition:\")\n",
    "print(\"  - Reusable transformation pipelines\")\n",
    "print(\"  - Declarative pipeline definitions\")\n",
    "print(\"  - Easy to test composed pipelines\")\n",
    "print(\"  - Can be parameterized and customized\")\n",
    "print(\"  - Functional programming pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Builder Pattern\n",
    "\n",
    "Advanced pattern for configurable transformation pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Pipeline Builder Pattern ===\")\n",
    "\n",
    "class TransformationPipeline:\n",
    "    \"\"\"\n",
    "    Builder pattern for composing transformation pipelines.\n",
    "    Fluent API for readable pipeline construction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: DataFrame):\n",
    "        self._df = df\n",
    "        self._transformations: List[Callable[[DataFrame], DataFrame]] = []\n",
    "    \n",
    "    def add_transformation(self, func: Callable[[DataFrame], DataFrame]) -> 'TransformationPipeline':\n",
    "        \"\"\"Add a transformation function to the pipeline.\"\"\"\n",
    "        self._transformations.append(func)\n",
    "        return self  # Return self for chaining\n",
    "    \n",
    "    def filter_by(self, condition) -> 'TransformationPipeline':\n",
    "        \"\"\"Add a filter operation.\"\"\"\n",
    "        def filter_func(df: DataFrame) -> DataFrame:\n",
    "            return df.filter(condition)\n",
    "        return self.add_transformation(filter_func)\n",
    "    \n",
    "    def with_columns(self, **col_definitions) -> 'TransformationPipeline':\n",
    "        \"\"\"Add multiple columns.\"\"\"\n",
    "        def add_cols(df: DataFrame) -> DataFrame:\n",
    "            result = df\n",
    "            for col_name, col_expr in col_definitions.items():\n",
    "                result = result.withColumn(col_name, col_expr)\n",
    "            return result\n",
    "        return self.add_transformation(add_cols)\n",
    "    \n",
    "    def select_columns(self, *columns) -> 'TransformationPipeline':\n",
    "        \"\"\"Add a select operation.\"\"\"\n",
    "        def select_func(df: DataFrame) -> DataFrame:\n",
    "            return df.select(*columns)\n",
    "        return self.add_transformation(select_func)\n",
    "    \n",
    "    def build(self) -> DataFrame:\n",
    "        \"\"\"Execute all transformations and return final DataFrame.\"\"\"\n",
    "        result = self._df\n",
    "        for transform in self._transformations:\n",
    "            result = transform(result)\n",
    "        return result\n",
    "\n",
    "# Use the builder pattern\n",
    "pipeline_result = (\n",
    "    TransformationPipeline(employees_df)\n",
    "    .filter_by(F.col(\"department\") == \"Engineering\")\n",
    "    .filter_by(F.col(\"years_experience\") >= 5)\n",
    "    .add_transformation(add_salary_grade)\n",
    "    .add_transformation(calculate_bonus)\n",
    "    .add_transformation(calculate_total_compensation)\n",
    "    .with_columns(\n",
    "        comp_ratio=F.col(\"total_comp\") / F.col(\"salary\"),\n",
    "        is_senior_eng=(F.col(\"job_level\").isin([\"Senior\", \"Lead\", \"Principal\"]))\n",
    "    )\n",
    "    .select_columns(\n",
    "        \"employee_id\", \"name\", \"job_level\", \"salary\", \n",
    "        \"salary_grade\", \"total_comp\", \"comp_ratio\"\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"Pipeline builder result:\")\n",
    "pipeline_result.show(5)\n",
    "\n",
    "print(\"\\n✅ Builder pattern benefits:\")\n",
    "print(\"  - Fluent, readable API\")\n",
    "print(\"  - Separates pipeline construction from execution\")\n",
    "print(\"  - Easy to add conditional transformations\")\n",
    "print(\"  - Can inspect pipeline before execution\")\n",
    "print(\"  - Reusable pipeline templates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Complex Business Logic\n",
    "\n",
    "Strategies for managing complex transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Handling Complex Business Logic ===\")\n",
    "\n",
    "# Complex business rule: Employee rating system\n",
    "def calculate_employee_rating(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Complex business logic broken into readable components.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate performance score components\n",
    "    df = df.withColumn(\"salary_score\",\n",
    "        F.when(F.col(\"salary\") > 150000, 5)\n",
    "         .when(F.col(\"salary\") > 120000, 4)\n",
    "         .when(F.col(\"salary\") > 90000, 3)\n",
    "         .when(F.col(\"salary\") > 60000, 2)\n",
    "         .otherwise(1)\n",
    "    )\n",
    "    \n",
    "    # Step 2: Experience score\n",
    "    df = df.withColumn(\"experience_score\",\n",
    "        F.when(F.col(\"years_experience\") > 15, 5)\n",
    "         .when(F.col(\"years_experience\") > 10, 4)\n",
    "         .when(F.col(\"years_experience\") > 5, 3)\n",
    "         .when(F.col(\"years_experience\") > 2, 2)\n",
    "         .otherwise(1)\n",
    "    )\n",
    "    \n",
    "    # Step 3: Job level score\n",
    "    job_level_scores = {\n",
    "        \"Junior\": 1,\n",
    "        \"Mid\": 2,\n",
    "        \"Senior\": 3,\n",
    "        \"Lead\": 4,\n",
    "        \"Principal\": 5\n",
    "    }\n",
    "    \n",
    "    mapping_expr = F.create_map([F.lit(x) for pair in job_level_scores.items() for x in pair])\n",
    "    df = df.withColumn(\"level_score\", mapping_expr[F.col(\"job_level\")])\n",
    "    \n",
    "    # Step 4: Calculate overall rating\n",
    "    df = df.withColumn(\"overall_rating\",\n",
    "        (\n",
    "            (F.col(\"salary_score\") * 0.4) +\n",
    "            (F.col(\"experience_score\") * 0.3) +\n",
    "            (F.col(\"level_score\") * 0.3)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Step 5: Rating category\n",
    "    df = df.withColumn(\"rating_category\",\n",
    "        F.when(F.col(\"overall_rating\") >= 4.5, \"Exceptional\")\n",
    "         .when(F.col(\"overall_rating\") >= 3.5, \"Strong\")\n",
    "         .when(F.col(\"overall_rating\") >= 2.5, \"Solid\")\n",
    "         .when(F.col(\"overall_rating\") >= 1.5, \"Developing\")\n",
    "         .otherwise(\"Entry\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply complex business logic\n",
    "rated_employees = calculate_employee_rating(employees_df)\n",
    "\n",
    "print(\"Employee ratings:\")\n",
    "rated_employees.select(\n",
    "    \"employee_id\", \"name\", \"job_level\", \"years_experience\", \"salary\",\n",
    "    \"salary_score\", \"experience_score\", \"level_score\", \n",
    "    \"overall_rating\", \"rating_category\"\n",
    ").show(10)\n",
    "\n",
    "# Distribution of ratings\n",
    "print(\"\\nRating distribution:\")\n",
    "rated_employees.groupBy(\"rating_category\").count().orderBy(F.desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n✅ Complex logic best practices:\")\n",
    "print(\"  - Break into logical steps with comments\")\n",
    "print(\"  - Use intermediate columns for clarity\")\n",
    "print(\"  - Extract to separate function\")\n",
    "print(\"  - Document business rules\")\n",
    "print(\"  - Make weights/thresholds configurable\")\n",
    "print(\"  - Test each component separately\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "Chaining and composition impact on performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Performance Considerations ===\")\n",
    "\n",
    "import time\n",
    "\n",
    "# Create larger dataset for performance testing\n",
    "large_df = create_employee_data(5000)\n",
    "\n",
    "# Approach 1: Single long chain\n",
    "start = time.time()\n",
    "result1 = (\n",
    "    large_df\n",
    "    .filter(F.col(\"department\") == \"Engineering\")\n",
    "    .withColumn(\"bonus\", F.col(\"salary\") * 0.10)\n",
    "    .withColumn(\"total_comp\", F.col(\"salary\") + F.col(\"bonus\"))\n",
    "    .groupBy(\"job_level\")\n",
    "    .agg(F.avg(\"total_comp\").alias(\"avg_comp\"))\n",
    ").count()  # Trigger action\n",
    "time1 = time.time() - start\n",
    "\n",
    "# Approach 2: Broken into steps\n",
    "start = time.time()\n",
    "filtered = large_df.filter(F.col(\"department\") == \"Engineering\")\n",
    "with_bonus = filtered.withColumn(\"bonus\", F.col(\"salary\") * 0.10)\n",
    "with_total = with_bonus.withColumn(\"total_comp\", F.col(\"salary\") + F.col(\"bonus\"))\n",
    "result2 = with_total.groupBy(\"job_level\").agg(F.avg(\"total_comp\").alias(\"avg_comp\"))\n",
    "count2 = result2.count()  # Trigger action\n",
    "time2 = time.time() - start\n",
    "\n",
    "# Approach 3: Using .transform()\n",
    "start = time.time()\n",
    "result3 = (\n",
    "    large_df\n",
    "    .filter(F.col(\"department\") == \"Engineering\")\n",
    "    .transform(calculate_bonus)\n",
    "    .transform(calculate_total_compensation)\n",
    "    .groupBy(\"job_level\")\n",
    "    .agg(F.avg(\"total_comp\").alias(\"avg_comp\"))\n",
    ").count()  # Trigger action\n",
    "time3 = time.time() - start\n",
    "\n",
    "print(f\"Approach 1 (single chain): {time1:.3f}s\")\n",
    "print(f\"Approach 2 (broken steps): {time2:.3f}s\")\n",
    "print(f\"Approach 3 (.transform()): {time3:.3f}s\")\n",
    "\n",
    "print(\"\\n📊 Performance insights:\")\n",
    "print(\"  - All approaches have similar performance (lazy evaluation)\")\n",
    "print(\"  - Catalyst optimizer analyzes entire transformation graph\")\n",
    "print(\"  - Breaking into steps doesn't hurt performance\")\n",
    "print(\"  - Choose based on readability, not performance\")\n",
    "print(\"  - .transform() adds minimal overhead\")\n",
    "print(\"  - Real performance gains come from:\")\n",
    "print(\"    • Predicate pushdown\")\n",
    "print(\"    • Column pruning\")\n",
    "print(\"    • Using built-in functions vs UDFs\")\n",
    "print(\"    • Proper partitioning and caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Chaining Fundamentals**:\n",
    "   - Leverage immutability for natural method chaining\n",
    "   - Lazy evaluation enables complex chains without overhead\n",
    "   - Catalyst optimizer analyzes entire transformation graph\n",
    "\n",
    "2. **Readability Guidelines**:\n",
    "   - Keep chains to 5 operations or fewer\n",
    "   - Use `.transform()` with named functions for longer chains\n",
    "   - Break very long chains into logical step variables\n",
    "   - Group operations by phase (filter, enrich, aggregate, format)\n",
    "\n",
    "3. **Schema Contracts**:\n",
    "   - Use `select` at pipeline boundaries for explicit schemas\n",
    "   - Keep select statements simple (one function per column)\n",
    "   - Extract complex logic to separate functions\n",
    "   - Document input/output expectations\n",
    "\n",
    "4. **Composition Patterns**:\n",
    "   - Pure functions for reusable transformations\n",
    "   - `.transform()` for clean pipeline composition\n",
    "   - Higher-order functions for composable pipelines\n",
    "   - Builder pattern for fluent API construction\n",
    "\n",
    "5. **Complex Logic Management**:\n",
    "   - Break complex rules into logical steps\n",
    "   - Use intermediate columns for clarity\n",
    "   - Document business logic thoroughly\n",
    "   - Make thresholds and weights configurable\n",
    "\n",
    "**Best Practices for Chaining and Composition**:\n",
    "- Prioritize readability over cleverness\n",
    "- Extract complex logic into named functions\n",
    "- Use explicit schema contracts with select\n",
    "- Compose transformations functionally\n",
    "- Test individual transformations independently\n",
    "- Trust Spark's optimizer - don't micro-optimize chains\n",
    "- Document complex business rules\n",
    "- Use consistent patterns across team\n",
    "\n",
    "**Next Steps**: In Section 3, we'll explore test-first development patterns to ensure our composed transformations work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Practice effective chaining and composition:\n",
    "\n",
    "1. Take a complex transformation and break it into modular functions\n",
    "2. Create a pipeline using `.transform()` with your functions\n",
    "3. Implement schema contracts with select statements\n",
    "4. Build a higher-order compose function\n",
    "5. Create a pipeline builder for your domain\n",
    "6. Document input/output schemas for your transformations\n",
    "7. Refactor a long chain into logical step variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "\n",
    "# 1. Create modular transformation functions\n",
    "def your_transformation_1(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Your first transformation\"\"\"\n",
    "    # Your implementation\n",
    "    pass\n",
    "\n",
    "def your_transformation_2(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Your second transformation\"\"\"\n",
    "    # Your implementation\n",
    "    pass\n",
    "\n",
    "# 2. Compose into a pipeline\n",
    "def your_pipeline(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Your composed pipeline\"\"\"\n",
    "    return (df\n",
    "        .transform(your_transformation_1)\n",
    "        .transform(your_transformation_2)\n",
    "        # Add more transformations\n",
    "    )\n",
    "\n",
    "# 3. Add schema contract\n",
    "def your_pipeline_with_schema(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pipeline with explicit output schema\"\"\"\n",
    "    result = your_pipeline(df)\n",
    "    \n",
    "    # Output schema contract\n",
    "    return result.select(\n",
    "        # Your output columns\n",
    "    )\n",
    "\n",
    "# 4. Test your pipeline\n",
    "# result = your_pipeline_with_schema(your_data)\n",
    "# result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
