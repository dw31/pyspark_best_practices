{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Unit Testing PySpark Code with Pytest in Databricks\n",
    "\n",
    "This notebook demonstrates how to implement effective unit testing for PySpark code using Pytest, with patterns that work both locally and in Databricks environments.\n",
    "\n",
    "## Learning Objectives\n",
    "- Structure PySpark code for testability\n",
    "- Create Pytest fixtures for SparkSession and test data\n",
    "- Mock Databricks-specific utilities for testing\n",
    "- Write comprehensive unit tests for transformation functions\n",
    "- Organize tests for CI/CD integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring Code for Testability\n",
    "\n",
    "The key to testing PySpark code is to separate pure transformation logic from I/O operations and Databricks-specific utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from unittest.mock import MagicMock, patch\n",
    "import pytest\n",
    "\n",
    "# Example: Testable PySpark Module Structure\n",
    "class SalesDataProcessor:\n",
    "    \"\"\"\n",
    "    Example of well-structured PySpark code for testing:\n",
    "    - Pure transformation functions\n",
    "    - Dependency injection for external dependencies\n",
    "    - Clear separation of concerns\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_sales_data(df):\n",
    "        \"\"\"Pure transformation function - easily testable\"\"\"\n",
    "        return (df\n",
    "                .filter(F.col(\"amount\") > 0)  # Remove invalid amounts\n",
    "                .withColumn(\"amount\", F.round(F.col(\"amount\"), 2))  # Round to 2 decimals\n",
    "                .withColumn(\"customer_name\", F.trim(F.initcap(F.col(\"customer_name\"))))  # Standardize names\n",
    "                .withColumn(\"sale_date\", F.to_date(F.col(\"sale_date\"), \"yyyy-MM-dd\"))  # Parse dates\n",
    "                .dropDuplicates([\"transaction_id\"])  # Remove duplicates\n",
    "               )\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_derived_columns(df):\n",
    "        \"\"\"Pure transformation function for derived columns\"\"\"\n",
    "        return (df\n",
    "                .withColumn(\"sale_year\", F.year(F.col(\"sale_date\")))\n",
    "                .withColumn(\"sale_month\", F.month(F.col(\"sale_date\")))\n",
    "                .withColumn(\"revenue_category\",\n",
    "                           F.when(F.col(\"amount\") < 100, \"Low\")\n",
    "                            .when(F.col(\"amount\") < 1000, \"Medium\")\n",
    "                            .otherwise(\"High\"))\n",
    "               )\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_customer_metrics(df):\n",
    "        \"\"\"Pure aggregation function\"\"\"\n",
    "        return (df\n",
    "                .groupBy(\"customer_name\")\n",
    "                .agg(\n",
    "                    F.sum(\"amount\").alias(\"total_spent\"),\n",
    "                    F.avg(\"amount\").alias(\"avg_purchase\"),\n",
    "                    F.count(\"*\").alias(\"purchase_count\"),\n",
    "                    F.min(\"sale_date\").alias(\"first_purchase\"),\n",
    "                    F.max(\"sale_date\").alias(\"last_purchase\")\n",
    "                )\n",
    "               )\n",
    "    \n",
    "    @classmethod\n",
    "    def process_sales_pipeline(cls, df):\n",
    "        \"\"\"Complete transformation pipeline - composed of pure functions\"\"\"\n",
    "        return (df\n",
    "                .transform(cls.clean_sales_data)\n",
    "                .transform(cls.add_derived_columns)\n",
    "               )\n",
    "    \n",
    "    # Example of function with external dependencies (needs mocking for tests)\n",
    "    def save_to_delta(self, df, path, dbutils=None):\n",
    "        \"\"\"Function with external dependency - requires mocking in tests\"\"\"\n",
    "        if dbutils:\n",
    "            # This would need to be mocked in tests\n",
    "            dbutils.fs.rm(path, True)  # Clear existing data\n",
    "        \n",
    "        # Write to Delta (this is a side effect)\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "        \n",
    "        return f\"Data saved to {path}\"\n",
    "\n",
    "print(\"SalesDataProcessor class defined with testable structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pytest Fixtures\n",
    "\n",
    "Fixtures provide reusable resources for tests, such as SparkSession instances and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Pytest fixtures (typically in conftest.py)\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark_session():\n",
    "    \"\"\"\n",
    "    Fixture providing a local SparkSession for testing.\n",
    "    Session-scoped for efficiency across all tests.\n",
    "    \"\"\"\n",
    "    spark = (SparkSession.builder\n",
    "             .master(\"local[*]\")\n",
    "             .appName(\"PySparkUnitTests\")\n",
    "             .config(\"spark.sql.adaptive.enabled\", \"false\")  # Disable for consistent tests\n",
    "             .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "             .getOrCreate())\n",
    "    \n",
    "    yield spark\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_sales_data(spark_session):\n",
    "    \"\"\"\n",
    "    Fixture providing test sales data\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        (1, \"john doe\", 150.75, \"2023-01-15\"),\n",
    "        (2, \"jane smith\", 89.99, \"2023-01-16\"), \n",
    "        (3, \"bob johnson\", 0.0, \"2023-01-17\"),  # Invalid amount\n",
    "        (4, \"alice brown\", 1200.50, \"2023-01-18\"),\n",
    "        (1, \"john doe\", 150.75, \"2023-01-15\"),  # Duplicate\n",
    "        (5, \" charlie wilson \", 75.25, \"2023-01-19\"),  # Needs trimming\n",
    "    ]\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"transaction_id\", IntegerType(), True),\n",
    "        StructField(\"customer_name\", StringType(), True),\n",
    "        StructField(\"amount\", DoubleType(), True),\n",
    "        StructField(\"sale_date\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    return spark_session.createDataFrame(data, schema)\n",
    "\n",
    "@pytest.fixture\n",
    "def expected_cleaned_data(spark_session):\n",
    "    \"\"\"\n",
    "    Fixture providing expected results after cleaning\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        (1, \"John Doe\", 150.75, \"2023-01-15\"),\n",
    "        (2, \"Jane Smith\", 89.99, \"2023-01-16\"),\n",
    "        (4, \"Alice Brown\", 1200.50, \"2023-01-18\"),\n",
    "        (5, \"Charlie Wilson\", 75.25, \"2023-01-19\"),\n",
    "    ]\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"transaction_id\", IntegerType(), True),\n",
    "        StructField(\"customer_name\", StringType(), True),\n",
    "        StructField(\"amount\", DoubleType(), True),\n",
    "        StructField(\"sale_date\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    return spark_session.createDataFrame(data, schema)\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_dbutils():\n",
    "    \"\"\"\n",
    "    Fixture providing mocked dbutils for testing\n",
    "    \"\"\"\n",
    "    mock_dbutils = MagicMock()\n",
    "    mock_dbutils.fs.rm.return_value = True\n",
    "    mock_dbutils.widgets.get.return_value = \"test_value\"\n",
    "    return mock_dbutils\n",
    "\n",
    "print(\"Pytest fixtures defined for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests for Pure Functions\n",
    "\n",
    "Let's write comprehensive unit tests for our pure transformation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests for pure transformation functions\n",
    "# (This would typically be in a separate test_sales_processor.py file)\n",
    "\n",
    "class TestSalesDataProcessor:\n",
    "    \"\"\"\n",
    "    Unit tests for SalesDataProcessor transformation functions\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_clean_sales_data_removes_invalid_amounts(self, spark_session, sample_sales_data):\n",
    "        \"\"\"\n",
    "        Test that clean_sales_data removes records with amount <= 0\n",
    "        \"\"\"\n",
    "        # Act\n",
    "        result = SalesDataProcessor.clean_sales_data(sample_sales_data)\n",
    "        \n",
    "        # Assert\n",
    "        result_amounts = [row[\"amount\"] for row in result.collect()]\n",
    "        assert all(amount > 0 for amount in result_amounts), \"All amounts should be positive\"\n",
    "        assert 0.0 not in result_amounts, \"Zero amounts should be filtered out\"\n",
    "    \n",
    "    def test_clean_sales_data_removes_duplicates(self, spark_session, sample_sales_data):\n",
    "        \"\"\"\n",
    "        Test that clean_sales_data removes duplicate transaction_ids\n",
    "        \"\"\"\n",
    "        # Act\n",
    "        result = SalesDataProcessor.clean_sales_data(sample_sales_data)\n",
    "        \n",
    "        # Assert\n",
    "        total_count = result.count()\n",
    "        distinct_count = result.select(\"transaction_id\").distinct().count()\n",
    "        assert total_count == distinct_count, \"Should have no duplicate transaction_ids\"\n",
    "    \n",
    "    def test_clean_sales_data_standardizes_names(self, spark_session, sample_sales_data):\n",
    "        \"\"\"\n",
    "        Test that customer names are properly formatted\n",
    "        \"\"\"\n",
    "        # Act\n",
    "        result = SalesDataProcessor.clean_sales_data(sample_sales_data)\n",
    "        \n",
    "        # Assert\n",
    "        names = [row[\"customer_name\"] for row in result.collect()]\n",
    "        \n",
    "        # Check specific transformations\n",
    "        assert \"John Doe\" in names, \"Should convert 'john doe' to 'John Doe'\"\n",
    "        assert \"Charlie Wilson\" in names, \"Should trim and title case ' charlie wilson '\"\n",
    "        assert all(name == name.strip() for name in names), \"All names should be trimmed\"\n",
    "    \n",
    "    def test_clean_sales_data_parses_dates(self, spark_session, sample_sales_data):\n",
    "        \"\"\"\n",
    "        Test that dates are properly parsed to DateType\n",
    "        \"\"\"\n",
    "        # Act\n",
    "        result = SalesDataProcessor.clean_sales_data(sample_sales_data)\n",
    "        \n",
    "        # Assert\n",
    "        date_field = next(field for field in result.schema.fields if field.name == \"sale_date\")\n",
    "        assert isinstance(date_field.dataType, DateType), \"sale_date should be DateType\"\n",
    "    \n",
    "    def test_add_derived_columns_creates_correct_columns(self, spark_session, sample_sales_data):\n",
    "        \"\"\"\n",
    "        Test that derived columns are correctly added\n",
    "        \"\"\"\n",
    "        # Arrange\n",
    "        cleaned_data = SalesDataProcessor.clean_sales_data(sample_sales_data)\n",
    "        \n",
    "        # Act\n",
    "        result = SalesDataProcessor.add_derived_columns(cleaned_data)\n",
    "        \n",
    "        # Assert\n",
    "        expected_columns = {\"sale_year\", \"sale_month\", \"revenue_category\"}\n",
    "        actual_columns = set(result.columns)\n",
    "        assert expected_columns.issubset(actual_columns), f\"Missing columns: {expected_columns - actual_columns}\"\n",
    "        \n",
    "        # Test specific values\n",
    "        sample_row = result.filter(F.col(\"transaction_id\") == 1).collect()[0]\n",
    "        assert sample_row[\"sale_year\"] == 2023, \"Year should be extracted correctly\"\n",
    "        assert sample_row[\"sale_month\"] == 1, \"Month should be extracted correctly\"\n",
    "        assert sample_row[\"revenue_category\"] == \"Medium\", \"150.75 should be categorized as Medium\"\n",
    "    \n",
    "    def test_revenue_categorization(self, spark_session):\n",
    "        \"\"\"\n",
    "        Test revenue categorization logic with specific test cases\n",
    "        \"\"\"\n",
    "        # Arrange\n",
    "        test_data = [\n",
    "            (1, \"Test Customer\", 50.0, \"2023-01-01\"),    # Low\n",
    "            (2, \"Test Customer\", 500.0, \"2023-01-01\"),   # Medium\n",
    "            (3, \"Test Customer\", 1500.0, \"2023-01-01\"),  # High\n",
    "        ]\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"transaction_id\", IntegerType(), True),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"amount\", DoubleType(), True),\n",
    "            StructField(\"sale_date\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        df = spark_session.createDataFrame(test_data, schema)\n",
    "        cleaned_df = SalesDataProcessor.clean_sales_data(df)\n",
    "        \n",
    "        # Act\n",
    "        result = SalesDataProcessor.add_derived_columns(cleaned_df)\n",
    "        \n",
    "        # Assert\n",
    "        categories = {row[\"transaction_id\"]: row[\"revenue_category\"] for row in result.collect()}\n",
    "        assert categories[1] == \"Low\", \"$50 should be Low category\"\n",
    "        assert categories[2] == \"Medium\", \"$500 should be Medium category\"\n",
    "        assert categories[3] == \"High\", \"$1500 should be High category\"\n",
    "    \n",
    "    def test_calculate_customer_metrics(self, spark_session):\n",
    "        \"\"\"\n",
    "        Test customer metrics calculation\n",
    "        \"\"\"\n",
    "        # Arrange\n",
    "        test_data = [\n",
    "            (1, \"John Doe\", 100.0, \"2023-01-01\"),\n",
    "            (2, \"John Doe\", 200.0, \"2023-01-15\"),\n",
    "            (3, \"Jane Smith\", 150.0, \"2023-01-10\"),\n",
    "        ]\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"transaction_id\", IntegerType(), True),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"amount\", DoubleType(), True),\n",
    "            StructField(\"sale_date\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        df = spark_session.createDataFrame(test_data, schema)\n",
    "        processed_df = SalesDataProcessor.process_sales_pipeline(df)\n",
    "        \n",
    "        # Act\n",
    "        result = SalesDataProcessor.calculate_customer_metrics(processed_df)\n",
    "        \n",
    "        # Assert\n",
    "        john_metrics = result.filter(F.col(\"customer_name\") == \"John Doe\").collect()[0]\n",
    "        assert john_metrics[\"total_spent\"] == 300.0, \"John's total should be $300\"\n",
    "        assert john_metrics[\"avg_purchase\"] == 150.0, \"John's average should be $150\"\n",
    "        assert john_metrics[\"purchase_count\"] == 2, \"John should have 2 purchases\"\n",
    "        \n",
    "        jane_metrics = result.filter(F.col(\"customer_name\") == \"Jane Smith\").collect()[0]\n",
    "        assert jane_metrics[\"purchase_count\"] == 1, \"Jane should have 1 purchase\"\n",
    "    \n",
    "    def test_complete_pipeline_integration(self, spark_session, sample_sales_data):\n",
    "        \"\"\"\n",
    "        Integration test for the complete transformation pipeline\n",
    "        \"\"\"\n",
    "        # Act\n",
    "        result = SalesDataProcessor.process_sales_pipeline(sample_sales_data)\n",
    "        \n",
    "        # Assert pipeline correctness\n",
    "        assert result.count() == 4, \"Should have 4 records after cleaning (removed invalid and duplicate)\"\n",
    "        \n",
    "        # Check all expected columns exist\n",
    "        expected_columns = {\n",
    "            \"transaction_id\", \"customer_name\", \"amount\", \"sale_date\",\n",
    "            \"sale_year\", \"sale_month\", \"revenue_category\"\n",
    "        }\n",
    "        actual_columns = set(result.columns)\n",
    "        assert expected_columns.issubset(actual_columns), \"Pipeline should create all expected columns\"\n",
    "        \n",
    "        # Verify data quality\n",
    "        amounts = [row[\"amount\"] for row in result.collect()]\n",
    "        assert all(amount > 0 for amount in amounts), \"All amounts should be positive after cleaning\"\n",
    "\n",
    "print(\"Unit test class defined for SalesDataProcessor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Functions with External Dependencies\n",
    "\n",
    "When functions have external dependencies (like dbutils), we need to use mocking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functions with external dependencies\n",
    "\n",
    "class TestSalesDataProcessorWithMocking:\n",
    "    \"\"\"\n",
    "    Tests for functions that have external dependencies\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_save_to_delta_with_mocked_dbutils(self, spark_session, sample_sales_data, mock_dbutils):\n",
    "        \"\"\"\n",
    "        Test save_to_delta function with mocked dbutils\n",
    "        \"\"\"\n",
    "        # Arrange\n",
    "        processor = SalesDataProcessor()\n",
    "        test_path = \"/tmp/test_delta_table\"\n",
    "        \n",
    "        # Act\n",
    "        result = processor.save_to_delta(sample_sales_data, test_path, mock_dbutils)\n",
    "        \n",
    "        # Assert\n",
    "        assert result == f\"Data saved to {test_path}\"\n",
    "        mock_dbutils.fs.rm.assert_called_once_with(test_path, True)\n",
    "    \n",
    "    @patch('your_module.dbutils')  # Replace with actual import path\n",
    "    def test_function_that_uses_dbutils_directly(self, mock_dbutils, spark_session):\n",
    "        \"\"\"\n",
    "        Example of patching dbutils when it's imported directly\n",
    "        \"\"\"\n",
    "        # Configure mock\n",
    "        mock_dbutils.widgets.get.return_value = \"production\"\n",
    "        \n",
    "        # Your test logic here\n",
    "        # result = function_that_uses_dbutils()\n",
    "        # assert result == expected_value\n",
    "        pass\n",
    "    \n",
    "    def test_error_handling_in_transformation(self, spark_session):\n",
    "        \"\"\"\n",
    "        Test error handling in transformation functions\n",
    "        \"\"\"\n",
    "        # Arrange: Create data that might cause errors\n",
    "        bad_data = [\n",
    "            (1, None, 100.0, \"invalid-date\"),  # Null customer name, invalid date\n",
    "        ]\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"transaction_id\", IntegerType(), True),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"amount\", DoubleType(), True),\n",
    "            StructField(\"sale_date\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        df = spark_session.createDataFrame(bad_data, schema)\n",
    "        \n",
    "        # Act & Assert: Depending on your error handling strategy\n",
    "        try:\n",
    "            result = SalesDataProcessor.clean_sales_data(df)\n",
    "            # If your function handles errors gracefully:\n",
    "            collected_result = result.collect()  # This should not fail\n",
    "            assert len(collected_result) >= 0, \"Function should handle bad data gracefully\"\n",
    "        except Exception as e:\n",
    "            # If your function is designed to fail fast:\n",
    "            assert isinstance(e, (ValueError, TypeError)), f\"Expected specific error type, got {type(e)}\"\n",
    "\n",
    "print(\"Mocking tests defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Assertion Utilities\n",
    "\n",
    "Let's create utilities for asserting DataFrame equality and schema validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame assertion utilities for comprehensive testing\n",
    "\n",
    "class DataFrameAssertions:\n",
    "    \"\"\"\n",
    "    Utility class for DataFrame-specific assertions\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def assert_dataframe_equal(actual_df, expected_df, ignore_order=True, ignore_nullable=True):\n",
    "        \"\"\"\n",
    "        Assert that two DataFrames are equal\n",
    "        \"\"\"\n",
    "        # Check schema equality\n",
    "        if not ignore_nullable:\n",
    "            assert actual_df.schema == expected_df.schema, \"Schemas do not match\"\n",
    "        else:\n",
    "            # Compare schemas ignoring nullable property\n",
    "            actual_fields = {(f.name, f.dataType) for f in actual_df.schema.fields}\n",
    "            expected_fields = {(f.name, f.dataType) for f in expected_df.schema.fields}\n",
    "            assert actual_fields == expected_fields, \"Schema fields do not match\"\n",
    "        \n",
    "        # Check data equality\n",
    "        actual_data = actual_df.collect()\n",
    "        expected_data = expected_df.collect()\n",
    "        \n",
    "        if ignore_order:\n",
    "            actual_data = sorted(actual_data, key=str)\n",
    "            expected_data = sorted(expected_data, key=str)\n",
    "        \n",
    "        assert len(actual_data) == len(expected_data), f\"Row count mismatch: {len(actual_data)} vs {len(expected_data)}\"\n",
    "        \n",
    "        for i, (actual_row, expected_row) in enumerate(zip(actual_data, expected_data)):\n",
    "            assert actual_row == expected_row, f\"Row {i} mismatch: {actual_row} vs {expected_row}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def assert_schema_equal(actual_df, expected_schema):\n",
    "        \"\"\"\n",
    "        Assert DataFrame schema matches expected schema\n",
    "        \"\"\"\n",
    "        actual_schema = actual_df.schema\n",
    "        \n",
    "        assert len(actual_schema.fields) == len(expected_schema.fields), \"Field count mismatch\"\n",
    "        \n",
    "        for actual_field, expected_field in zip(actual_schema.fields, expected_schema.fields):\n",
    "            assert actual_field.name == expected_field.name, f\"Field name mismatch: {actual_field.name} vs {expected_field.name}\"\n",
    "            assert actual_field.dataType == expected_field.dataType, f\"Data type mismatch for {actual_field.name}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def assert_columns_exist(df, expected_columns):\n",
    "        \"\"\"\n",
    "        Assert that DataFrame contains expected columns\n",
    "        \"\"\"\n",
    "        actual_columns = set(df.columns)\n",
    "        expected_columns = set(expected_columns)\n",
    "        missing_columns = expected_columns - actual_columns\n",
    "        \n",
    "        assert len(missing_columns) == 0, f\"Missing columns: {missing_columns}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def assert_no_nulls_in_columns(df, columns):\n",
    "        \"\"\"\n",
    "        Assert that specified columns contain no null values\n",
    "        \"\"\"\n",
    "        for column in columns:\n",
    "            null_count = df.filter(F.col(column).isNull()).count()\n",
    "            assert null_count == 0, f\"Column '{column}' contains {null_count} null values\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def assert_column_values_in_range(df, column, min_val=None, max_val=None):\n",
    "        \"\"\"\n",
    "        Assert that column values are within specified range\n",
    "        \"\"\"\n",
    "        if min_val is not None:\n",
    "            min_actual = df.agg(F.min(column)).collect()[0][0]\n",
    "            assert min_actual >= min_val, f\"Minimum value {min_actual} is below threshold {min_val}\"\n",
    "        \n",
    "        if max_val is not None:\n",
    "            max_actual = df.agg(F.max(column)).collect()[0][0]\n",
    "            assert max_actual <= max_val, f\"Maximum value {max_actual} is above threshold {max_val}\"\n",
    "\n",
    "# Example usage in tests\n",
    "class TestWithDataFrameAssertions:\n",
    "    \"\"\"\n",
    "    Examples of using DataFrame assertion utilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_data_quality_assertions(self, spark_session, sample_sales_data):\n",
    "        \"\"\"\n",
    "        Test using DataFrame assertion utilities\n",
    "        \"\"\"\n",
    "        # Act\n",
    "        result = SalesDataProcessor.clean_sales_data(sample_sales_data)\n",
    "        \n",
    "        # Assert using utilities\n",
    "        DataFrameAssertions.assert_columns_exist(result, \n",
    "                                               [\"transaction_id\", \"customer_name\", \"amount\", \"sale_date\"])\n",
    "        \n",
    "        DataFrameAssertions.assert_no_nulls_in_columns(result, \n",
    "                                                      [\"transaction_id\", \"customer_name\"])\n",
    "        \n",
    "        DataFrameAssertions.assert_column_values_in_range(result, \"amount\", min_val=0.01)\n",
    "    \n",
    "    def test_schema_validation(self, spark_session, sample_sales_data):\n",
    "        \"\"\"\n",
    "        Test schema validation\n",
    "        \"\"\"\n",
    "        # Arrange\n",
    "        expected_schema = StructType([\n",
    "            StructField(\"transaction_id\", IntegerType(), True),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"amount\", DoubleType(), True),\n",
    "            StructField(\"sale_date\", DateType(), True),  # Note: DateType after cleaning\n",
    "            StructField(\"sale_year\", IntegerType(), True),\n",
    "            StructField(\"sale_month\", IntegerType(), True),\n",
    "            StructField(\"revenue_category\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Act\n",
    "        result = SalesDataProcessor.process_sales_pipeline(sample_sales_data)\n",
    "        \n",
    "        # Assert\n",
    "        DataFrameAssertions.assert_schema_equal(result, expected_schema)\n",
    "\n",
    "print(\"DataFrame assertion utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tests and Test Organization\n",
    "\n",
    "Here's how to organize and run your PySpark tests effectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test organization structure\n",
    "# This demonstrates how to organize your test files\n",
    "\n",
    "\"\"\"\n",
    "Recommended project structure for testable PySpark code:\n",
    "\n",
    "project/\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── transformations/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── sales_processor.py\n",
    "│   │   └── data_quality.py\n",
    "│   └── utils/\n",
    "│       ├── __init__.py\n",
    "│       └── spark_utils.py\n",
    "├── tests/\n",
    "│   ├── __init__.py\n",
    "│   ├── conftest.py  # Pytest fixtures\n",
    "│   ├── test_sales_processor.py\n",
    "│   ├── test_data_quality.py\n",
    "│   └── utils/\n",
    "│       ├── __init__.py\n",
    "│       └── test_assertions.py\n",
    "├── pytest.ini\n",
    "├── requirements.txt\n",
    "└── README.md\n",
    "\"\"\"\n",
    "\n",
    "# Example pytest.ini configuration\n",
    "pytest_ini_content = \"\"\"\n",
    "[tool:pytest]\n",
    "testpaths = tests\n",
    "python_files = test_*.py\n",
    "python_classes = Test*\n",
    "python_functions = test_*\n",
    "addopts = \n",
    "    -v\n",
    "    --tb=short\n",
    "    --strict-markers\n",
    "    --disable-warnings\n",
    "    --cov=src\n",
    "    --cov-report=html\n",
    "    --cov-report=term-missing\n",
    "\n",
    "markers =\n",
    "    unit: Unit tests\n",
    "    integration: Integration tests\n",
    "    slow: Slow running tests\n",
    "\"\"\"\n",
    "\n",
    "# Example conftest.py for shared fixtures\n",
    "conftest_content = \"\"\"\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark_session():\n",
    "    spark = (SparkSession.builder\n",
    "             .master(\"local[*]\")\n",
    "             .appName(\"PySparkUnitTests\")\n",
    "             .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "             .getOrCreate())\n",
    "    yield spark\n",
    "    spark.stop()\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_dbutils():\n",
    "    return MagicMock()\n",
    "\n",
    "# Add other shared fixtures here\n",
    "\"\"\"\n",
    "\n",
    "print(\"Test organization structure and configuration examples provided\")\n",
    "\n",
    "# Example commands to run tests\n",
    "print(\"\\n=== Test Execution Commands ===\")\n",
    "print(\"Run all tests:                    pytest\")\n",
    "print(\"Run with coverage:                pytest --cov=src\")\n",
    "print(\"Run specific test file:           pytest tests/test_sales_processor.py\")\n",
    "print(\"Run specific test:                pytest tests/test_sales_processor.py::TestSalesDataProcessor::test_clean_sales_data\")\n",
    "print(\"Run tests with specific marker:   pytest -m unit\")\n",
    "print(\"Run tests in parallel:            pytest -n auto\")\n",
    "print(\"Run tests with output:            pytest -v -s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Databricks and CI/CD\n",
    "\n",
    "Here's how to integrate your tests with Databricks development workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration patterns for Databricks and CI/CD\n",
    "\n",
    "# Example GitHub Actions workflow for PySpark tests\n",
    "github_actions_yml = \"\"\"\n",
    "name: PySpark Tests\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main, develop ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    strategy:\n",
    "      matrix:\n",
    "        python-version: [3.8, 3.9]\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    \n",
    "    - name: Set up Python ${{ matrix.python-version }}\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: ${{ matrix.python-version }}\n",
    "    \n",
    "    - name: Set up Java 11\n",
    "      uses: actions/setup-java@v2\n",
    "      with:\n",
    "        java-version: '11'\n",
    "        distribution: 'temurin'\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r requirements.txt\n",
    "        pip install pytest pytest-cov pytest-xdist\n",
    "    \n",
    "    - name: Run tests\n",
    "      run: |\n",
    "        pytest --cov=src --cov-report=xml --cov-report=term-missing\n",
    "    \n",
    "    - name: Upload coverage to Codecov\n",
    "      uses: codecov/codecov-action@v1\n",
    "      with:\n",
    "        file: ./coverage.xml\n",
    "\"\"\"\n",
    "\n",
    "# Example Databricks notebook for running tests\n",
    "databricks_test_notebook = \"\"\"\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # PySpark Unit Tests Runner\n",
    "# MAGIC \n",
    "# MAGIC This notebook runs unit tests in the Databricks environment\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Install test dependencies\n",
    "%pip install pytest\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Import your modules\n",
    "from src.transformations.sales_processor import SalesDataProcessor\n",
    "from tests.utils.test_assertions import DataFrameAssertions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Run a simple smoke test\n",
    "def run_smoke_tests():\n",
    "    # Create test data\n",
    "    test_data = [(1, \"Test User\", 100.0, \"2023-01-01\")]\n",
    "    schema = StructType([\n",
    "        StructField(\"transaction_id\", IntegerType(), True),\n",
    "        StructField(\"customer_name\", StringType(), True),\n",
    "        StructField(\"amount\", DoubleType(), True),\n",
    "        StructField(\"sale_date\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.createDataFrame(test_data, schema)\n",
    "    \n",
    "    # Test transformations\n",
    "    result = SalesDataProcessor.process_sales_pipeline(df)\n",
    "    \n",
    "    # Basic assertions\n",
    "    assert result.count() == 1, \"Should have 1 record\"\n",
    "    assert \"sale_year\" in result.columns, \"Should have derived columns\"\n",
    "    \n",
    "    print(\"✅ Smoke tests passed!\")\n",
    "\n",
    "run_smoke_tests()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Integration with Databricks Asset Bundles\n",
    "# MAGIC \n",
    "# MAGIC For full CI/CD integration, use Databricks Asset Bundles (DABs)\n",
    "\"\"\"\n",
    "\n",
    "# Example requirements.txt for PySpark testing\n",
    "requirements_txt = \"\"\"\n",
    "pyspark>=3.3.0\n",
    "pytest>=6.0.0\n",
    "pytest-cov>=2.10.0\n",
    "pytest-xdist>=2.0.0\n",
    "pytest-mock>=3.0.0\n",
    "delta-spark>=2.0.0\n",
    "pandas>=1.3.0\n",
    "pyarrow>=5.0.0\n",
    "\"\"\"\n",
    "\n",
    "print(\"CI/CD integration examples provided\")\n",
    "print(\"\\n=== Key Integration Points ===\")\n",
    "print(\"1. Local testing with pytest before committing\")\n",
    "print(\"2. CI/CD pipeline runs tests on every PR\")\n",
    "print(\"3. Databricks notebooks can run smoke tests\")\n",
    "print(\"4. Use Databricks Connect for local development\")\n",
    "print(\"5. Databricks Asset Bundles for deployment automation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Code Structure for Testability**:\n",
    "   - Extract pure transformation functions\n",
    "   - Separate I/O and external dependencies\n",
    "   - Use dependency injection for testability\n",
    "\n",
    "2. **Pytest Fixtures**:\n",
    "   - Session-scoped SparkSession for efficiency\n",
    "   - Reusable test data fixtures\n",
    "   - Mock external dependencies (dbutils)\n",
    "\n",
    "3. **Comprehensive Testing**:\n",
    "   - Unit tests for pure functions\n",
    "   - DataFrame-specific assertions\n",
    "   - Error handling and edge cases\n",
    "   - Integration tests for complete pipelines\n",
    "\n",
    "4. **CI/CD Integration**:\n",
    "   - GitHub Actions for automated testing\n",
    "   - Databricks notebooks for smoke testing\n",
    "   - Databricks Asset Bundles for deployment\n",
    "\n",
    "**Benefits of Test-First PySpark Development**:\n",
    "- Faster feedback loops\n",
    "- Higher code quality and reliability\n",
    "- Easier refactoring and maintenance\n",
    "- Better documentation through tests\n",
    "- Confidence in production deployments\n",
    "\n",
    "**Next Steps**: In the next notebook, we'll explore DataFrame and schema validation techniques to ensure data quality throughout your pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Practice test-driven development:\n",
    "\n",
    "1. Create a new transformation function for your domain\n",
    "2. Write tests first (TDD approach)\n",
    "3. Implement the function to make tests pass\n",
    "4. Add edge case tests\n",
    "5. Create integration tests for a complete pipeline\n",
    "6. Set up pytest fixtures for your test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "\n",
    "class TestYourTransformation:\n",
    "    \"\"\"Practice TDD with your own transformation\"\"\"\n",
    "    \n",
    "    def test_your_function_basic_case(self, spark_session):\n",
    "        \"\"\"Write your test first\"\"\"\n",
    "        # Arrange\n",
    "        # Create test data\n",
    "        \n",
    "        # Act\n",
    "        # Call your transformation function\n",
    "        \n",
    "        # Assert\n",
    "        # Verify expected behavior\n",
    "        pass\n",
    "    \n",
    "    def test_your_function_edge_cases(self, spark_session):\n",
    "        \"\"\"Test edge cases\"\"\"\n",
    "        pass\n",
    "\n",
    "def your_transformation_function(df):\n",
    "    \"\"\"Implement your function to make tests pass\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Run your tests\n",
    "# pytest your_test_file.py -v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}