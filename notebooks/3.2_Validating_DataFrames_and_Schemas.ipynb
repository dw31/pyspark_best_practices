{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Summary\n# MAGIC \n# MAGIC In this notebook, we've explored functional approaches to DataFrame and schema validation in PySpark:\n# MAGIC \n# MAGIC ### Key Concepts Covered\n# MAGIC \n# MAGIC 1. **Functional Validation Patterns**\n# MAGIC    - Pure validation functions with no side effects\n# MAGIC    - Structured error handling with ValidationResult types\n# MAGIC    - Immutable validation pipelines\n# MAGIC \n# MAGIC 2. **Schema Validation**\n# MAGIC    - Column existence checking\n# MAGIC    - Data type compatibility validation\n# MAGIC    - Schema evolution handling\n# MAGIC \n# MAGIC 3. **Data Constraint Validation**\n# MAGIC    - Null value checking\n# MAGIC    - Range validation for numeric data\n# MAGIC    - Pattern matching with regex\n# MAGIC    - Custom business rule validation\n# MAGIC \n# MAGIC 4. **Composable Validation Pipeline**\n# MAGIC    - Rule composition with higher-order functions\n# MAGIC    - Validation result aggregation\n# MAGIC    - Comprehensive error reporting\n# MAGIC \n# MAGIC 5. **Integration Patterns**\n# MAGIC    - Great Expectations integration\n# MAGIC    - External validation library adaptation\n# MAGIC    - Custom validation rule creation\n# MAGIC \n# MAGIC ### Best Practices Demonstrated\n# MAGIC \n# MAGIC - ✅ **Pure Functions**: All validation functions are side-effect free\n# MAGIC - ✅ **Immutable Results**: ValidationResult and ValidationReport are immutable\n# MAGIC - ✅ **Composability**: Validation rules can be easily combined and reused\n# MAGIC - ✅ **Error Handling**: Comprehensive error information with structured results\n# MAGIC - ✅ **Testability**: All functions are easy to unit test\n# MAGIC \n# MAGIC ### Next Steps\n# MAGIC \n# MAGIC - Practice implementing the exercises provided\n# MAGIC - Integrate validation patterns into your data pipelines\n# MAGIC - Explore Great Expectations for advanced validation scenarios\n# MAGIC - Build reusable validation libraries for your organization\n# MAGIC - Implement validation in CI/CD pipelines for data quality assurance\n# MAGIC \n# MAGIC This functional approach to validation ensures reliable, maintainable, and composable data quality checking in your PySpark applications.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\nprint(\"=== EXERCISES: Practice DataFrame Validation ===\\n\")\n\n# Exercise 1: Create a custom validation rule\nprint(\"📝 EXERCISE 1: Create a unique constraint validator\")\nprint(\"   Task: Implement validate_unique_values() function\")\nprint(\"   Requirements:\")\nprint(\"   - Pure function that checks for duplicate values in a column\")\nprint(\"   - Return ValidationResult with appropriate details\")\nprint(\"   - Handle edge cases (missing column, empty DataFrame)\")\n\ndef validate_unique_values(df: DataFrame, column: str) -> ValidationResult:\n    \"\"\"\n    YOUR TASK: Implement this function to validate unique values.\n    Should return ValidationResult indicating if all values in column are unique.\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\nprint(\"   Hint: Use df.select(column).distinct().count() vs df.count()\")\n\n# Exercise 2: Create a composite validation rule\nprint(\"\\n📝 EXERCISE 2: Create a date format validator\")\nprint(\"   Task: Implement validate_date_format() function\")\nprint(\"   Requirements:\")\nprint(\"   - Validate that string dates match specific format (e.g., 'YYYY-MM-DD')\")\nprint(\"   - Use regex pattern matching\")\nprint(\"   - Return detailed information about invalid dates\")\n\ndef validate_date_format(df: DataFrame, column: str, date_pattern: str = r'^\\d{4}-\\d{2}-\\d{2}$') -> ValidationResult:\n    \"\"\"\n    YOUR TASK: Implement date format validation.\n    Should validate that all dates in column match the specified pattern.\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\nprint(\"   Hint: Use F.col(column).rlike(pattern) for regex matching\")\n\n# Exercise 3: Build a validation pipeline\nprint(\"\\n📝 EXERCISE 3: Build a customer data validator\")\nprint(\"   Task: Create validation pipeline for customer data\")\nprint(\"   Requirements:\")\nprint(\"   - Customer ID: unique, not null\")\nprint(\"   - Email: valid email format, not null\")\nprint(\"   - Age: between 18 and 120\")\nprint(\"   - Registration date: valid date format\")\n\n# Sample customer data for testing\ncustomer_test_data = [\n    (1, \"john@email.com\", 25, \"2023-01-15\"),\n    (2, \"jane@email.com\", 30, \"2023-02-20\"),\n    (3, \"bob@email.com\", 45, \"2023-03-10\"),\n    (4, \"alice@email.com\", 28, \"2023-04-05\"),\n]\n\ncustomer_schema = StructType([\n    StructField(\"customer_id\", IntegerType(), False),\n    StructField(\"email\", StringType(), False),\n    StructField(\"age\", IntegerType(), False),\n    StructField(\"registration_date\", StringType(), False)\n])\n\ncustomer_df = spark.createDataFrame(customer_test_data, customer_schema)\n\ndef create_customer_validator() -> DataFrameValidator:\n    \"\"\"\n    YOUR TASK: Create a complete validator for customer data.\n    Use the validation functions we've created and new ones from exercises 1 & 2.\n    \"\"\"\n    # TODO: Implement this function\n    # Should return DataFrameValidator with appropriate rules\n    pass\n\nprint(\"   Hint: Combine existing validation functions with your custom ones\")\n\n# Exercise 4: Error aggregation and reporting\nprint(\"\\n📝 EXERCISE 4: Create enhanced validation reporting\")\nprint(\"   Task: Implement detailed validation reporting\")\nprint(\"   Requirements:\")\nprint(\"   - Group validation results by error type\")\nprint(\"   - Calculate error percentages\")\nprint(\"   - Generate actionable recommendations\")\n\ndef create_detailed_validation_report(report: ValidationReport, df: DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    YOUR TASK: Create enhanced reporting from ValidationReport.\n    Should include error summaries, percentages, and recommendations.\n    \"\"\"\n    # TODO: Implement this function\n    # Should return dictionary with detailed analysis\n    pass\n\n# Solutions (uncomment to see reference implementations)\nprint(\"\\n\" + \"=\"*60)\nprint(\"💡 SOLUTIONS (Reference Implementations)\")\nprint(\"=\"*60)\n\n# Solution 1: Unique values validator\ndef validate_unique_values_solution(df: DataFrame, column: str) -> ValidationResult:\n    \"\"\"Reference implementation for unique values validation.\"\"\"\n    if column not in df.columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' does not exist\",\n            rule_name=\"unique_values\",\n            details={\"column\": column}\n        )\n    \n    total_count = df.count()\n    if total_count == 0:\n        return ValidationResult(\n            is_valid=True,\n            level=ValidationLevel.INFO,\n            message=f\"Column '{column}' is empty (no duplicates possible)\",\n            rule_name=\"unique_values\"\n        )\n    \n    unique_count = df.select(column).distinct().count()\n    duplicate_count = total_count - unique_count\n    \n    if duplicate_count > 0:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' has {duplicate_count} duplicate values\",\n            rule_name=\"unique_values\",\n            failed_records=duplicate_count,\n            details={\"total_count\": total_count, \"unique_count\": unique_count}\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=f\"Column '{column}' has all unique values\",\n        rule_name=\"unique_values\"\n    )\n\n# Test the solution\nprint(\"\\n✅ Testing unique values validator:\")\nunique_result = validate_unique_values_solution(customer_df, \"customer_id\")\nprint(f\"   Result: {unique_result.message}\")\n\n# Solution 2: Date format validator\ndef validate_date_format_solution(df: DataFrame, column: str, date_pattern: str = r'^\\d{4}-\\d{2}-\\d{2}$') -> ValidationResult:\n    \"\"\"Reference implementation for date format validation.\"\"\"\n    if column not in df.columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' does not exist\",\n            rule_name=\"date_format\",\n            details={\"column\": column}\n        )\n    \n    invalid_count = df.filter(~F.col(column).rlike(date_pattern)).count()\n    total_count = df.count()\n    \n    if invalid_count > 0:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' has {invalid_count} invalid date formats\",\n            rule_name=\"date_format\",\n            failed_records=invalid_count,\n            details={\"pattern\": date_pattern, \"total_count\": total_count}\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=f\"All dates in '{column}' match expected format\",\n        rule_name=\"date_format\"\n    )\n\n# Test the solution\nprint(\"\\n✅ Testing date format validator:\")\ndate_result = validate_date_format_solution(customer_df, \"registration_date\")\nprint(f\"   Result: {date_result.message}\")\n\nprint(f\"\\n🎯 Practice implementing these validators to master functional validation patterns!\")\nprint(f\"   Remember: Pure functions, immutable results, comprehensive error handling\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 7. Exercises and Practice\n# MAGIC \n# MAGIC Practice implementing functional validation patterns with these exercises.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# BEST PRACTICES for functional DataFrame validation\n\nprint(\"=== BEST PRACTICES ===\\n\")\n\n# ✅ DO: Use pure functions that return validation results\ndef good_validation_pattern(df: DataFrame, column: str) -> ValidationResult:\n    \"\"\"\n    Pure function that validates data and returns structured results.\n    No side effects, easy to test, composable.\n    \"\"\"\n    null_count = df.filter(F.col(column).isNull()).count()\n    return ValidationResult(\n        is_valid=null_count == 0,\n        level=ValidationLevel.ERROR if null_count > 0 else ValidationLevel.INFO,\n        message=f\"Column '{column}' has {null_count} null values\",\n        rule_name=\"null_check\"\n    )\n\nprint(\"✅ GOOD: Pure validation function with structured results\")\nprint(\"   - Returns ValidationResult with all necessary information\")\nprint(\"   - No side effects or external dependencies\")\nprint(\"   - Easy to test and compose\")\n\n# ✅ DO: Use immutable validation pipelines\nprint(\"\\n✅ GOOD: Immutable validation pipeline composition\")\nbase_validator = DataFrameValidator([\n    create_validation_rule(validate_not_null, [\"employee_id\"])\n])\n\n# Create new validator without mutating the original\nextended_validator = base_validator.add_rule(\n    create_validation_rule(validate_range, \"age\", 0, 120)\n)\n\nprint(f\"   - Base validator has {len(base_validator.rules)} rules\")\nprint(f\"   - Extended validator has {len(extended_validator.rules)} rules\")\nprint(\"   - Original validator unchanged (immutability)\")\n\n# ✅ DO: Handle errors gracefully with Result types\nprint(\"\\n✅ GOOD: Graceful error handling with structured results\")\ntry:\n    result = validate_range(valid_df, \"nonexistent_column\", 0, 100)\n    print(f\"   - Error handled gracefully: {result.message}\")\n    print(f\"   - Error details preserved: {result.details}\")\nexcept Exception:\n    print(\"   - No exceptions thrown, errors captured in ValidationResult\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"=== ANTI-PATTERNS ===\\n\")\n\n# ❌ DON'T: Use functions with side effects\ndef bad_validation_pattern_with_side_effects(df: DataFrame, column: str) -> bool:\n    \"\"\"\n    ANTI-PATTERN: Validation function with side effects.\n    Prints directly, modifies global state, hard to test.\n    \"\"\"\n    null_count = df.filter(F.col(column).isNull()).count()\n    \n    # ❌ Side effect: Direct printing\n    if null_count > 0:\n        print(f\"ERROR: Column {column} has {null_count} nulls!\")\n        # ❌ Side effect: Modifying global state\n        global validation_errors\n        validation_errors = validation_errors.get(column, 0) + null_count\n    \n    return null_count == 0\n\nprint(\"❌ BAD: Validation with side effects\")\nprint(\"   - Direct printing makes testing difficult\")\nprint(\"   - Global state modification breaks composability\")\nprint(\"   - Returns only boolean, loses important information\")\n\n# ❌ DON'T: Use mutable validation configuration\nclass BadMutableValidator:\n    \"\"\"\n    ANTI-PATTERN: Mutable validator that can be modified after creation.\n    \"\"\"\n    def __init__(self):\n        self.rules = []  # ❌ Mutable list\n        self.errors = []  # ❌ Mutable state\n    \n    def add_rule(self, rule):\n        self.rules.append(rule)  # ❌ Modifies existing object\n    \n    def validate(self, df):\n        self.errors.clear()  # ❌ Side effect: clearing previous state\n        for rule in self.rules:\n            # Validation logic would go here\n            pass\n        return len(self.errors) == 0\n\nprint(\"\\n❌ BAD: Mutable validator\")\nprint(\"   - Mutable state can lead to unexpected behavior\")\nprint(\"   - Side effects in validation method\")\nprint(\"   - Hard to reason about state changes\")\n\n# ❌ DON'T: Swallow exceptions without proper handling\ndef bad_validation_with_silent_failures(df: DataFrame, column: str) -> bool:\n    \"\"\"\n    ANTI-PATTERN: Silently handling exceptions without reporting.\n    \"\"\"\n    try:\n        result = df.select(column).count()\n        return True\n    except Exception:\n        # ❌ Silent failure - no information about what went wrong\n        return False\n\nprint(\"\\n❌ BAD: Silent exception handling\")\nprint(\"   - Hides important error information\")\nprint(\"   - Makes debugging difficult\")\nprint(\"   - Fails without explanation\")\n\n# ✅ BETTER: Proper exception handling\ndef better_validation_with_error_capture(df: DataFrame, column: str) -> ValidationResult:\n    \"\"\"\n    BETTER: Capture exceptions and convert to structured results.\n    \"\"\"\n    try:\n        if column not in df.columns:\n            return ValidationResult(\n                is_valid=False,\n                level=ValidationLevel.ERROR,\n                message=f\"Column '{column}' not found in DataFrame\",\n                rule_name=\"column_existence\",\n                details={\"column\": column, \"available_columns\": df.columns}\n            )\n        \n        count = df.select(column).count()\n        return ValidationResult(\n            is_valid=True,\n            level=ValidationLevel.INFO,\n            message=f\"Column '{column}' accessible with {count} records\",\n            rule_name=\"column_existence\"\n        )\n        \n    except Exception as e:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Validation failed with exception: {str(e)}\",\n            rule_name=\"column_existence\",\n            details={\"exception\": str(e), \"exception_type\": type(e).__name__}\n        )\n\nprint(\"\\n✅ BETTER: Structured exception handling\")\ntest_result = better_validation_with_error_capture(valid_df, \"nonexistent_column\")\nprint(f\"   - Structured error: {test_result.message}\")\nprint(f\"   - Error details preserved: {test_result.details}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"=== KEY PRINCIPLES ===\")\nprint(\"1. ✅ Pure functions with no side effects\")\nprint(\"2. ✅ Immutable data structures and pipelines\")  \nprint(\"3. ✅ Structured error handling with Result types\")\nprint(\"4. ✅ Composable validation rules\")\nprint(\"5. ✅ Comprehensive error information\")\nprint(\"6. ❌ Avoid side effects in validation functions\")\nprint(\"7. ❌ Avoid mutable validation state\")\nprint(\"8. ❌ Avoid silent exception handling\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 6. Best Practices and Anti-Patterns\n# MAGIC \n# MAGIC Let's review functional programming best practices and anti-patterns in data validation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Great Expectations integration (conceptual - would require installation)\n# This cell demonstrates how you would integrate Great Expectations\n# In a real Databricks environment, you would install it first\n\ndef create_great_expectations_validator() -> ValidationRule:\n    \"\"\"\n    Conceptual integration with Great Expectations.\n    In practice, you would install great_expectations and configure it properly.\n    \"\"\"\n    def ge_validation_rule(df: DataFrame) -> ValidationResult:\n        \"\"\"\n        Wrapper function that adapts Great Expectations to our validation framework.\n        This is a conceptual example - actual implementation would require GE setup.\n        \"\"\"\n        try:\n            # This is pseudo-code demonstrating the integration pattern\n            # In reality, you would:\n            # 1. Convert PySpark DataFrame to Pandas or use Spark backend\n            # 2. Create GE DataContext and ExpectationSuite\n            # 3. Run validations and capture results\n            \n            # Simulated Great Expectations-style validation\n            issues_found = []\n            \n            # Simulate some GE-style expectations\n            if df.filter(F.col(\"age\") < 0).count() > 0:\n                issues_found.append(\"Found negative ages\")\n            \n            if df.filter(F.col(\"salary\") <= 0).count() > 0:\n                issues_found.append(\"Found non-positive salaries\")\n            \n            # Check for duplicates\n            total_count = df.count()\n            unique_count = df.select(\"employee_id\").distinct().count()\n            if total_count != unique_count:\n                issues_found.append(\"Found duplicate employee IDs\")\n            \n            if issues_found:\n                return ValidationResult(\n                    is_valid=False,\n                    level=ValidationLevel.ERROR,\n                    message=f\"Great Expectations validation failed: {'; '.join(issues_found)}\",\n                    rule_name=\"great_expectations\",\n                    details={\"issues\": issues_found}\n                )\n            \n            return ValidationResult(\n                is_valid=True,\n                level=ValidationLevel.INFO,\n                message=\"All Great Expectations validations passed\",\n                rule_name=\"great_expectations\"\n            )\n            \n        except Exception as e:\n            return ValidationResult(\n                is_valid=False,\n                level=ValidationLevel.ERROR,\n                message=f\"Great Expectations validation error: {str(e)}\",\n                rule_name=\"great_expectations\",\n                details={\"exception\": str(e)}\n            )\n    \n    return ge_validation_rule\n\n# Example of how to configure Great Expectations expectations\ndef create_employee_expectations_config() -> Dict[str, Any]:\n    \"\"\"\n    Configuration for Great Expectations expectations.\n    This would be used to set up a proper GE suite.\n    \"\"\"\n    return {\n        \"expectation_suite_name\": \"employee_data_suite\",\n        \"expectations\": [\n            {\n                \"expectation_type\": \"expect_table_columns_to_match_ordered_list\",\n                \"kwargs\": {\n                    \"column_list\": [\"employee_id\", \"name\", \"email\", \"age\", \"department\", \"salary\", \"hire_date\"]\n                }\n            },\n            {\n                \"expectation_type\": \"expect_column_values_to_not_be_null\",\n                \"kwargs\": {\"column\": \"employee_id\"}\n            },\n            {\n                \"expectation_type\": \"expect_column_values_to_be_between\",\n                \"kwargs\": {\"column\": \"age\", \"min_value\": 18, \"max_value\": 65}\n            },\n            {\n                \"expectation_type\": \"expect_column_values_to_be_between\",\n                \"kwargs\": {\"column\": \"salary\", \"min_value\": 0}\n            },\n            {\n                \"expectation_type\": \"expect_column_values_to_match_regex\",\n                \"kwargs\": {\n                    \"column\": \"email\",\n                    \"regex\": r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"\n                }\n            },\n            {\n                \"expectation_type\": \"expect_column_values_to_be_in_set\",\n                \"kwargs\": {\n                    \"column\": \"department\",\n                    \"value_set\": [\"Engineering\", \"Marketing\", \"Sales\", \"HR\", \"Finance\"]\n                }\n            }\n        ]\n    }\n\n# Test the conceptual GE integration\nprint(\"Testing conceptual Great Expectations integration...\")\nge_validator = create_great_expectations_validator()\n\n# Test with valid data\nge_result = ge_validator(valid_df)\nprint(f\"✅ GE validation (valid data): {ge_result.message}\")\n\n# Test with problematic data (create data with duplicates)\nduplicate_data = [\n    (1, \"Alice Johnson\", \"alice@company.com\", 28, \"Engineering\", 75000.0, \"2020-01-15\"),\n    (1, \"Bob Smith\", \"bob@company.com\", 32, \"Marketing\", 65000.0, \"2019-03-10\"),  # Duplicate ID\n]\n\nduplicate_df = spark.createDataFrame(duplicate_data, expected_schema)\nge_result_with_issues = ge_validator(duplicate_df)\nprint(f\"❌ GE validation (with duplicates): {ge_result_with_issues.message}\")\n\n# Show how to integrate GE with our validation pipeline\ncomplete_validator = enhanced_validator.add_rule(ge_validator)\nprint(f\"\\n📊 Complete validation pipeline now has {len(complete_validator.rules)} rules\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 5. Integration with Great Expectations\n# MAGIC \n# MAGIC Great Expectations is a powerful library for data validation. Let's see how to integrate it with our functional validation approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Test validation pipeline with problematic data\nprint(\"Creating test data with validation violations...\")\n\n# Create DataFrame with various validation issues\nproblematic_data = [\n    (None, \"Alice Johnson\", \"alice@company.com\", 25, \"Engineering\", 75000.0, \"2020-01-15\"),  # Null ID\n    (2, None, \"bob-invalid-email\", 17, \"Marketing\", -5000.0, \"2019-03-10\"),  # Multiple issues\n    (3, \"Carol Davis\", \"carol@company.com\", 70, \"Engineering\", 80000.0, \"2021-06-01\"),  # Age > 65\n    (4, \"David Wilson\", \"david@company.com\", 35, \"Sales\", 70000.0, \"2018-11-20\"),  # Valid record\n]\n\nproblematic_schema = StructType([\n    StructField(\"employee_id\", IntegerType(), True),  # Allow nulls for testing\n    StructField(\"name\", StringType(), True),\n    StructField(\"email\", StringType(), False),\n    StructField(\"age\", IntegerType(), False),\n    StructField(\"department\", StringType(), False),\n    StructField(\"salary\", DoubleType(), False),\n    StructField(\"hire_date\", StringType(), False)\n])\n\nproblematic_df = spark.createDataFrame(problematic_data, problematic_schema)\n\nprint(\"Problematic data created:\")\nproblematic_df.show(truncate=False)\n\n# Test validation with problematic data\nprint(\"\\n\" + \"=\"*60)\nprint(\"Testing validation pipeline with problematic data...\")\nprint(\"=\"*60)\n\nproblematic_report = validator.validate(problematic_df)\nproblematic_report.print_summary()\n\n# Show detailed failure analysis\nprint(f\"\\n🔍 Detailed Failure Analysis:\")\nfor result in problematic_report.results:\n    if not result.is_valid or result.level == ValidationLevel.WARNING:\n        print(f\"\\n❌ {result.rule_name}:\")\n        print(f\"   Message: {result.message}\")\n        if result.details:\n            for key, value in result.details.items():\n                print(f\"   {key}: {value}\")\n        if result.failed_records:\n            print(f\"   Failed Records: {result.failed_records}\")\n\n# Demonstrate validation rule composition - add custom business rules\nprint(f\"\\n🔧 Adding custom business validation rules...\")\n\ndef validate_department_codes(df: DataFrame, valid_departments: List[str]) -> ValidationResult:\n    \"\"\"Custom validation for department codes.\"\"\"\n    if \"department\" not in df.columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=\"Department column not found\",\n            rule_name=\"department_codes\"\n        )\n    \n    # Check for invalid department codes\n    invalid_count = df.filter(~F.col(\"department\").isin(valid_departments)).count()\n    total_count = df.count()\n    \n    if invalid_count > 0:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Found {invalid_count} records with invalid department codes\",\n            rule_name=\"department_codes\",\n            failed_records=invalid_count,\n            details={\"valid_departments\": valid_departments}\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=\"All department codes are valid\",\n        rule_name=\"department_codes\"\n    )\n\n# Create enhanced validator with custom business rules\nenhanced_validator = validator.add_rules([\n    create_validation_rule(validate_department_codes, [\"Engineering\", \"Marketing\", \"Sales\", \"HR\", \"Finance\"])\n])\n\nprint(\"Testing enhanced validator with business rules...\")\nenhanced_report = enhanced_validator.validate(valid_df)\nenhanced_report.print_summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Composable validation pipeline\nfrom typing import Protocol\n\nclass ValidationRule(Protocol):\n    \"\"\"\n    Protocol for validation rules that can be composed in a pipeline.\n    All validation rules should return ValidationResult or List[ValidationResult].\n    \"\"\"\n    def __call__(self, df: DataFrame) -> Union[ValidationResult, List[ValidationResult]]:\n        ...\n\ndef create_validation_rule(validation_func: Callable, *args, **kwargs) -> ValidationRule:\n    \"\"\"\n    Higher-order function that creates a validation rule from a validation function\n    by partially applying its arguments.\n    \"\"\"\n    return partial(validation_func, *args, **kwargs)\n\nclass DataFrameValidator:\n    \"\"\"\n    Functional validation pipeline that composes multiple validation rules.\n    Immutable and side-effect free.\n    \"\"\"\n    \n    def __init__(self, rules: List[ValidationRule]):\n        self.rules = rules\n    \n    def validate(self, df: DataFrame) -> ValidationReport:\n        \"\"\"\n        Execute all validation rules and return a comprehensive report.\n        Pure function with no side effects.\n        \"\"\"\n        all_results = []\n        \n        for rule in self.rules:\n            try:\n                result = rule(df)\n                \n                # Handle both single results and lists of results\n                if isinstance(result, list):\n                    all_results.extend(result)\n                else:\n                    all_results.append(result)\n                    \n            except Exception as e:\n                # Convert exceptions to validation results\n                error_result = ValidationResult(\n                    is_valid=False,\n                    level=ValidationLevel.ERROR,\n                    message=f\"Validation rule failed with exception: {str(e)}\",\n                    rule_name=\"exception_handler\",\n                    details={\"exception\": str(e), \"exception_type\": type(e).__name__}\n                )\n                all_results.append(error_result)\n        \n        return ValidationReport.from_results(all_results)\n    \n    def add_rule(self, rule: ValidationRule) -> 'DataFrameValidator':\n        \"\"\"Return a new validator with an additional rule (immutable).\"\"\"\n        return DataFrameValidator(self.rules + [rule])\n    \n    def add_rules(self, rules: List[ValidationRule]) -> 'DataFrameValidator':\n        \"\"\"Return a new validator with additional rules (immutable).\"\"\"\n        return DataFrameValidator(self.rules + rules)\n\n# Create a comprehensive validation pipeline for employee data\ndef create_employee_data_validator() -> DataFrameValidator:\n    \"\"\"\n    Factory function that creates a validation pipeline for employee data.\n    Demonstrates composition of multiple validation rules.\n    \"\"\"\n    rules = [\n        # Schema validation\n        create_validation_rule(\n            validate_columns_exist, \n            [\"employee_id\", \"name\", \"email\", \"age\", \"department\", \"salary\", \"hire_date\"]\n        ),\n        create_validation_rule(validate_schema_compatibility, expected_schema),\n        \n        # Not null constraints\n        create_validation_rule(validate_not_null, [\"employee_id\", \"name\", \"email\"]),\n        \n        # Range validations\n        create_validation_rule(validate_range, \"age\", 18, 65),\n        create_validation_rule(validate_range, \"salary\", 0, None),\n        \n        # Pattern validations\n        create_validation_rule(\n            validate_regex_pattern, \n            \"email\", \n            r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\",\n            \"valid email format\"\n        ),\n    ]\n    \n    return DataFrameValidator(rules)\n\n# Test the validation pipeline with valid data\nprint(\"Testing validation pipeline with valid data...\")\nvalidator = create_employee_data_validator()\nreport = validator.validate(valid_df)\n\nreport.print_summary()\n\n# Show individual results\nprint(f\"\\n📋 Individual Validation Results ({len(report.results)} total):\")\nfor i, result in enumerate(report.results, 1):\n    status = \"✅\" if result.is_valid else \"❌\"\n    print(f\"  {i}. {status} {result.rule_name}: {result.message}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 4. Composable Validation Pipeline\n# MAGIC \n# MAGIC Now let's create a functional validation pipeline that composes multiple validation rules and produces comprehensive reports.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Data constraint validation functions\ndef validate_not_null(df: DataFrame, columns: List[str]) -> List[ValidationResult]:\n    \"\"\"\n    Pure function to validate that specified columns don't contain null values.\n    Returns a list of validation results, one per column.\n    \"\"\"\n    results = []\n    \n    for col_name in columns:\n        if col_name not in df.columns:\n            results.append(ValidationResult(\n                is_valid=False,\n                level=ValidationLevel.ERROR,\n                message=f\"Column '{col_name}' does not exist\",\n                rule_name=\"not_null\",\n                details={\"column\": col_name}\n            ))\n            continue\n        \n        null_count = df.filter(F.col(col_name).isNull()).count()\n        total_count = df.count()\n        \n        if null_count > 0:\n            results.append(ValidationResult(\n                is_valid=False,\n                level=ValidationLevel.ERROR,\n                message=f\"Column '{col_name}' contains {null_count} null values out of {total_count} records\",\n                rule_name=\"not_null\",\n                failed_records=null_count,\n                details={\"column\": col_name, \"null_count\": null_count, \"total_count\": total_count}\n            ))\n        else:\n            results.append(ValidationResult(\n                is_valid=True,\n                level=ValidationLevel.INFO,\n                message=f\"Column '{col_name}' has no null values\",\n                rule_name=\"not_null\",\n                details={\"column\": col_name}\n            ))\n    \n    return results\n\ndef validate_range(df: DataFrame, column: str, min_val: Optional[float] = None, \n                  max_val: Optional[float] = None) -> ValidationResult:\n    \"\"\"\n    Pure function to validate that numeric column values fall within specified range.\n    \"\"\"\n    if column not in df.columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' does not exist\",\n            rule_name=\"range_validation\",\n            details={\"column\": column}\n        )\n    \n    conditions = []\n    range_description = []\n    \n    if min_val is not None:\n        conditions.append(F.col(column) < min_val)\n        range_description.append(f\"< {min_val}\")\n    \n    if max_val is not None:\n        conditions.append(F.col(column) > max_val)\n        range_description.append(f\"> {max_val}\")\n    \n    if not conditions:\n        return ValidationResult(\n            is_valid=True,\n            level=ValidationLevel.INFO,\n            message=f\"No range constraints specified for '{column}'\",\n            rule_name=\"range_validation\"\n        )\n    \n    # Find records outside the valid range\n    invalid_condition = reduce(lambda a, b: a | b, conditions)\n    invalid_count = df.filter(invalid_condition).count()\n    total_count = df.count()\n    \n    if invalid_count > 0:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' has {invalid_count} values outside valid range ({' or '.join(range_description)})\",\n            rule_name=\"range_validation\",\n            failed_records=invalid_count,\n            details={\n                \"column\": column, \n                \"invalid_count\": invalid_count, \n                \"total_count\": total_count,\n                \"min_val\": min_val,\n                \"max_val\": max_val\n            }\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=f\"All values in '{column}' are within valid range\",\n        rule_name=\"range_validation\",\n        details={\"column\": column, \"min_val\": min_val, \"max_val\": max_val}\n    )\n\ndef validate_regex_pattern(df: DataFrame, column: str, pattern: str, \n                          description: str = \"\") -> ValidationResult:\n    \"\"\"\n    Pure function to validate that string column values match a regex pattern.\n    \"\"\"\n    if column not in df.columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' does not exist\",\n            rule_name=\"regex_pattern\",\n            details={\"column\": column}\n        )\n    \n    # Count records that don't match the pattern\n    invalid_count = df.filter(~F.col(column).rlike(pattern)).count()\n    total_count = df.count()\n    \n    rule_desc = description or f\"pattern '{pattern}'\"\n    \n    if invalid_count > 0:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' has {invalid_count} values that don't match {rule_desc}\",\n            rule_name=\"regex_pattern\",\n            failed_records=invalid_count,\n            details={\n                \"column\": column, \n                \"pattern\": pattern,\n                \"description\": description,\n                \"invalid_count\": invalid_count, \n                \"total_count\": total_count\n            }\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=f\"All values in '{column}' match {rule_desc}\",\n        rule_name=\"regex_pattern\",\n        details={\"column\": column, \"pattern\": pattern, \"description\": description}\n    )\n\n# Test constraint validation functions\nprint(\"Testing constraint validation functions...\")\n\n# Test not null validation\nnull_results = validate_not_null(valid_df, [\"employee_id\", \"name\", \"email\"])\nfor result in null_results:\n    status = \"✅\" if result.is_valid else \"❌\"\n    print(f\"{status} Not null test for {result.details.get('column')}: {result.message}\")\n\n# Test range validation  \nage_range_result = validate_range(valid_df, \"age\", min_val=18, max_val=65)\nprint(f\"✅ Age range test: {age_range_result.message}\")\n\nsalary_range_result = validate_range(valid_df, \"salary\", min_val=0)\nprint(f\"✅ Salary range test: {salary_range_result.message}\")\n\n# Test regex pattern validation\nemail_pattern_result = validate_regex_pattern(\n    valid_df, \"email\", r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\", \n    \"valid email format\"\n)\nprint(f\"✅ Email pattern test: {email_pattern_result.message}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 3. Data Constraint Validation\n# MAGIC \n# MAGIC Beyond schema validation, we need to validate data content against business rules and constraints. Let's implement functional constraint validation patterns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Core schema validation functions\ndef validate_columns_exist(df: DataFrame, expected_columns: List[str]) -> ValidationResult:\n    \"\"\"\n    Pure function to validate that all expected columns exist in the DataFrame.\n    \"\"\"\n    actual_columns = set(df.columns)\n    expected_columns_set = set(expected_columns)\n    missing_columns = expected_columns_set - actual_columns\n    \n    if missing_columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Missing required columns: {sorted(missing_columns)}\",\n            rule_name=\"columns_exist\",\n            details={\"missing_columns\": sorted(missing_columns)}\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=f\"All {len(expected_columns)} required columns present\",\n        rule_name=\"columns_exist\"\n    )\n\ndef validate_schema_compatibility(df: DataFrame, expected_schema: StructType) -> ValidationResult:\n    \"\"\"\n    Pure function to validate DataFrame schema compatibility.\n    Checks both column presence and data type compatibility.\n    \"\"\"\n    actual_schema = df.schema\n    expected_fields = {field.name: field for field in expected_schema.fields}\n    actual_fields = {field.name: field for field in actual_schema.fields}\n    \n    issues = []\n    \n    # Check missing columns\n    missing_cols = set(expected_fields.keys()) - set(actual_fields.keys())\n    if missing_cols:\n        issues.append(f\"Missing columns: {sorted(missing_cols)}\")\n    \n    # Check type compatibility for common columns\n    common_cols = set(expected_fields.keys()) & set(actual_fields.keys())\n    type_mismatches = []\n    \n    for col_name in common_cols:\n        expected_type = expected_fields[col_name].dataType\n        actual_type = actual_fields[col_name].dataType\n        \n        if not _are_types_compatible(actual_type, expected_type):\n            type_mismatches.append(f\"{col_name}: expected {expected_type}, got {actual_type}\")\n    \n    if type_mismatches:\n        issues.append(f\"Type mismatches: {type_mismatches}\")\n    \n    if issues:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=\"; \".join(issues),\n            rule_name=\"schema_compatibility\",\n            details={\"issues\": issues}\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=\"Schema is compatible\",\n        rule_name=\"schema_compatibility\"\n    )\n\ndef _are_types_compatible(actual: DataType, expected: DataType) -> bool:\n    \"\"\"Helper function to check type compatibility with some flexibility.\"\"\"\n    if actual == expected:\n        return True\n    \n    # Allow some implicit conversions\n    compatible_conversions = {\n        (IntegerType, LongType),\n        (FloatType, DoubleType),\n        (StringType, VarcharType),\n    }\n    \n    return (type(actual), type(expected)) in compatible_conversions\n\n# Test the schema validation functions\nprint(\"Testing schema validation functions...\")\n\n# Test with valid DataFrame\nresult1 = validate_columns_exist(valid_df, [\"employee_id\", \"name\", \"email\", \"age\"])\nprint(f\"✅ Valid columns test: {result1.is_valid} - {result1.message}\")\n\n# Test with missing columns\nresult2 = validate_columns_exist(valid_df, [\"employee_id\", \"name\", \"missing_column\"])\nprint(f\"❌ Missing columns test: {result2.is_valid} - {result2.message}\")\n\n# Test schema compatibility\nresult3 = validate_schema_compatibility(valid_df, expected_schema)\nprint(f\"✅ Schema compatibility test: {result3.is_valid} - {result3.message}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Validation result classes for functional error handling\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import NamedTuple\n\nclass ValidationLevel(Enum):\n    ERROR = \"ERROR\"\n    WARNING = \"WARNING\" \n    INFO = \"INFO\"\n\n@dataclass\nclass ValidationResult:\n    \"\"\"\n    Immutable validation result that contains all validation information.\n    Enables functional composition and chaining of validation results.\n    \"\"\"\n    is_valid: bool\n    level: ValidationLevel\n    message: str\n    rule_name: str\n    failed_records: Optional[int] = None\n    details: Optional[Dict[str, Any]] = None\n    \n    def __post_init__(self):\n        if self.details is None:\n            self.details = {}\n\n@dataclass \nclass ValidationReport:\n    \"\"\"\n    Immutable collection of validation results with summary statistics.\n    \"\"\"\n    results: List[ValidationResult]\n    total_validations: int\n    passed_validations: int\n    failed_validations: int\n    warnings: int\n    \n    @classmethod\n    def from_results(cls, results: List[ValidationResult]) -> 'ValidationReport':\n        \"\"\"Pure constructor that calculates summary statistics from results.\"\"\"\n        total = len(results)\n        passed = sum(1 for r in results if r.is_valid)\n        failed = sum(1 for r in results if not r.is_valid and r.level == ValidationLevel.ERROR)\n        warnings = sum(1 for r in results if r.level == ValidationLevel.WARNING)\n        \n        return cls(\n            results=results,\n            total_validations=total,\n            passed_validations=passed,\n            failed_validations=failed,\n            warnings=warnings\n        )\n    \n    @property\n    def is_valid(self) -> bool:\n        \"\"\"Check if all validations passed (warnings don't fail validation).\"\"\"\n        return self.failed_validations == 0\n    \n    def print_summary(self) -> None:\n        \"\"\"Display validation summary.\"\"\"\n        status = \"✅ PASSED\" if self.is_valid else \"❌ FAILED\"\n        print(f\"\\n{status} - Validation Report Summary\")\n        print(f\"Total Validations: {self.total_validations}\")\n        print(f\"Passed: {self.passed_validations}\")\n        print(f\"Failed: {self.failed_validations}\")\n        print(f\"Warnings: {self.warnings}\")\n        \n        if not self.is_valid or self.warnings > 0:\n            print(\"\\nDetails:\")\n            for result in self.results:\n                if not result.is_valid or result.level == ValidationLevel.WARNING:\n                    icon = \"⚠️\" if result.level == ValidationLevel.WARNING else \"❌\"\n                    print(f\"  {icon} {result.rule_name}: {result.message}\")\n\nprint(\"✅ Validation result classes defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 2. Functional Schema Validation Patterns\n# MAGIC \n# MAGIC Let's implement pure functions for various types of schema validation. These functions return validation results without side effects.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Create sample data for validation demonstrations\ndef create_sample_data():\n    \"\"\"\n    Pure function to create consistent sample data for validation examples.\n    Returns multiple DataFrames with different validation scenarios.\n    \"\"\"\n    \n    # Valid employee data\n    valid_employee_data = [\n        (1, \"Alice Johnson\", \"alice@company.com\", 28, \"Engineering\", 75000.0, \"2020-01-15\"),\n        (2, \"Bob Smith\", \"bob@company.com\", 32, \"Marketing\", 65000.0, \"2019-03-10\"),\n        (3, \"Carol Davis\", \"carol@company.com\", 29, \"Engineering\", 80000.0, \"2021-06-01\"),\n        (4, \"David Wilson\", \"david@company.com\", 35, \"Sales\", 70000.0, \"2018-11-20\"),\n    ]\n    \n    valid_schema = StructType([\n        StructField(\"employee_id\", IntegerType(), False),\n        StructField(\"name\", StringType(), False),\n        StructField(\"email\", StringType(), False),\n        StructField(\"age\", IntegerType(), False),\n        StructField(\"department\", StringType(), False),\n        StructField(\"salary\", DoubleType(), False),\n        StructField(\"hire_date\", StringType(), False)\n    ])\n    \n    valid_df = spark.createDataFrame(valid_employee_data, valid_schema)\n    \n    # Invalid data scenarios for validation testing\n    invalid_data_scenarios = {\n        \"missing_columns\": [\n            (1, \"Alice Johnson\", \"alice@company.com\", 28, \"Engineering\"),  # Missing salary and hire_date\n            (2, \"Bob Smith\", \"bob@company.com\", 32, \"Marketing\")\n        ],\n        \n        \"wrong_types\": [\n            (\"1\", \"Alice Johnson\", \"alice@company.com\", \"28\", \"Engineering\", \"75000\", \"2020-01-15\"),  # String IDs and age\n            (\"2\", \"Bob Smith\", \"bob@company.com\", \"32\", \"Marketing\", \"65000\", \"2019-03-10\")\n        ],\n        \n        \"null_violations\": [\n            (1, None, \"alice@company.com\", 28, \"Engineering\", 75000.0, \"2020-01-15\"),  # Null name\n            (2, \"Bob Smith\", None, 32, \"Marketing\", 65000.0, \"2019-03-10\")  # Null email\n        ],\n        \n        \"constraint_violations\": [\n            (1, \"Alice Johnson\", \"invalid-email\", -5, \"InvalidDept\", -10000.0, \"invalid-date\"),  # Multiple violations\n            (2, \"Bob Smith\", \"bob@company.com\", 150, \"Marketing\", 1000000.0, \"2019-13-45\")\n        ]\n    }\n    \n    return valid_df, invalid_data_scenarios, valid_schema\n\n# Generate sample data\nvalid_df, invalid_scenarios, expected_schema = create_sample_data()\n\nprint(\"✅ Sample data created for validation demonstrations\")\nvalid_df.show(truncate=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 1. Schema Validation Fundamentals\n# MAGIC \n# MAGIC Schema validation ensures that DataFrames conform to expected structure, data types, and constraints. Let's start with functional approaches to schema validation.\n# MAGIC \n# MAGIC ### Core Validation Types\n# MAGIC \n# MAGIC 1. **Structural Validation**: Column presence, order, and naming\n# MAGIC 2. **Type Validation**: Data type compatibility and conversion\n# MAGIC 3. **Constraint Validation**: Null checks, value ranges, format patterns\n# MAGIC 4. **Relationship Validation**: Foreign key constraints and referential integrity",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Essential imports for DataFrame and schema validation\nfrom pyspark.sql import SparkSession, DataFrame\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.utils import AnalysisException\nfrom typing import Dict, List, Tuple, Optional, Callable, Any, Union\nfrom functools import reduce, partial\nimport json\nfrom datetime import datetime\nimport re\n\n# Initialize Spark session (if not already available)\nspark = SparkSession.builder.appName(\"DataFrameSchemaValidation\").getOrCreate()\n\nprint(\"✅ Setup complete - Ready for DataFrame and schema validation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Databricks notebook source\n# MAGIC %md\n# MAGIC # 3.2 Validating DataFrames and Schemas\n# MAGIC \n# MAGIC This notebook demonstrates functional approaches to DataFrame and schema validation in PySpark. We'll explore various validation strategies that maintain functional programming principles while ensuring data quality and integrity.\n# MAGIC \n# MAGIC ## Learning Objectives\n# MAGIC \n# MAGIC By the end of this notebook, you will understand how to:\n# MAGIC - Implement functional schema validation patterns\n# MAGIC - Create reusable validation functions\n# MAGIC - Handle validation errors gracefully\n# MAGIC - Build validation pipelines with pure functions\n# MAGIC - Use Great Expectations with PySpark\n# MAGIC - Create custom validation rules\n# MAGIC \n# MAGIC ## Prerequisites\n# MAGIC \n# MAGIC - Understanding of PySpark DataFrames\n# MAGIC - Knowledge of functional programming concepts\n# MAGIC - Familiarity with schema definitions",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}