{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Summary\n# MAGIC \n# MAGIC In this notebook, we've explored functional approaches to DataFrame and schema validation in PySpark:\n# MAGIC \n# MAGIC ### Key Concepts Covered\n# MAGIC \n# MAGIC 1. **Functional Validation Patterns**\n# MAGIC    - Pure validation functions with no side effects\n# MAGIC    - Structured error handling with ValidationResult types\n# MAGIC    - Immutable validation pipelines\n# MAGIC \n# MAGIC 2. **Schema Validation**\n# MAGIC    - Column existence checking\n# MAGIC    - Data type compatibility validation\n# MAGIC    - Schema evolution handling\n# MAGIC \n# MAGIC 3. **Data Constraint Validation**\n# MAGIC    - Null value checking\n# MAGIC    - Range validation for numeric data\n# MAGIC    - Pattern matching with regex\n# MAGIC    - Custom business rule validation\n# MAGIC \n# MAGIC 4. **Composable Validation Pipeline**\n# MAGIC    - Rule composition with higher-order functions\n# MAGIC    - Validation result aggregation\n# MAGIC    - Comprehensive error reporting\n# MAGIC \n# MAGIC 5. **Integration Patterns**\n# MAGIC    - Great Expectations integration\n# MAGIC    - External validation library adaptation\n# MAGIC    - Custom validation rule creation\n# MAGIC \n# MAGIC ### Best Practices Demonstrated\n# MAGIC \n# MAGIC - ‚úÖ **Pure Functions**: All validation functions are side-effect free\n# MAGIC - ‚úÖ **Immutable Results**: ValidationResult and ValidationReport are immutable\n# MAGIC - ‚úÖ **Composability**: Validation rules can be easily combined and reused\n# MAGIC - ‚úÖ **Error Handling**: Comprehensive error information with structured results\n# MAGIC - ‚úÖ **Testability**: All functions are easy to unit test\n# MAGIC \n# MAGIC ### Next Steps\n# MAGIC \n# MAGIC - Practice implementing the exercises provided\n# MAGIC - Integrate validation patterns into your data pipelines\n# MAGIC - Explore Great Expectations for advanced validation scenarios\n# MAGIC - Build reusable validation libraries for your organization\n# MAGIC - Implement validation in CI/CD pipelines for data quality assurance\n# MAGIC \n# MAGIC This functional approach to validation ensures reliable, maintainable, and composable data quality checking in your PySpark applications.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\nprint(\"=== EXERCISES: Practice DataFrame Validation ===\\n\")\n\n# Exercise 1: Create a custom validation rule\nprint(\"üìù EXERCISE 1: Create a unique constraint validator\")\nprint(\"   Task: Implement validate_unique_values() function\")\nprint(\"   Requirements:\")\nprint(\"   - Pure function that checks for duplicate values in a column\")\nprint(\"   - Return ValidationResult with appropriate details\")\nprint(\"   - Handle edge cases (missing column, empty DataFrame)\")\n\ndef validate_unique_values(df: DataFrame, column: str) -> ValidationResult:\n    \"\"\"\n    YOUR TASK: Implement this function to validate unique values.\n    Should return ValidationResult indicating if all values in column are unique.\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\nprint(\"   Hint: Use df.select(column).distinct().count() vs df.count()\")\n\n# Exercise 2: Create a composite validation rule\nprint(\"\\nüìù EXERCISE 2: Create a date format validator\")\nprint(\"   Task: Implement validate_date_format() function\")\nprint(\"   Requirements:\")\nprint(\"   - Validate that string dates match specific format (e.g., 'YYYY-MM-DD')\")\nprint(\"   - Use regex pattern matching\")\nprint(\"   - Return detailed information about invalid dates\")\n\ndef validate_date_format(df: DataFrame, column: str, date_pattern: str = r'^\\d{4}-\\d{2}-\\d{2}$') -> ValidationResult:\n    \"\"\"\n    YOUR TASK: Implement date format validation.\n    Should validate that all dates in column match the specified pattern.\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\nprint(\"   Hint: Use F.col(column).rlike(pattern) for regex matching\")\n\n# Exercise 3: Build a validation pipeline\nprint(\"\\nüìù EXERCISE 3: Build a customer data validator\")\nprint(\"   Task: Create validation pipeline for customer data\")\nprint(\"   Requirements:\")\nprint(\"   - Customer ID: unique, not null\")\nprint(\"   - Email: valid email format, not null\")\nprint(\"   - Age: between 18 and 120\")\nprint(\"   - Registration date: valid date format\")\n\n# Sample customer data for testing\ncustomer_test_data = [\n    (1, \"john@email.com\", 25, \"2023-01-15\"),\n    (2, \"jane@email.com\", 30, \"2023-02-20\"),\n    (3, \"bob@email.com\", 45, \"2023-03-10\"),\n    (4, \"alice@email.com\", 28, \"2023-04-05\"),\n]\n\ncustomer_schema = StructType([\n    StructField(\"customer_id\", IntegerType(), False),\n    StructField(\"email\", StringType(), False),\n    StructField(\"age\", IntegerType(), False),\n    StructField(\"registration_date\", StringType(), False)\n])\n\ncustomer_df = spark.createDataFrame(customer_test_data, customer_schema)\n\ndef create_customer_validator() -> DataFrameValidator:\n    \"\"\"\n    YOUR TASK: Create a complete validator for customer data.\n    Use the validation functions we've created and new ones from exercises 1 & 2.\n    \"\"\"\n    # TODO: Implement this function\n    # Should return DataFrameValidator with appropriate rules\n    pass\n\nprint(\"   Hint: Combine existing validation functions with your custom ones\")\n\n# Exercise 4: Error aggregation and reporting\nprint(\"\\nüìù EXERCISE 4: Create enhanced validation reporting\")\nprint(\"   Task: Implement detailed validation reporting\")\nprint(\"   Requirements:\")\nprint(\"   - Group validation results by error type\")\nprint(\"   - Calculate error percentages\")\nprint(\"   - Generate actionable recommendations\")\n\ndef create_detailed_validation_report(report: ValidationReport, df: DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    YOUR TASK: Create enhanced reporting from ValidationReport.\n    Should include error summaries, percentages, and recommendations.\n    \"\"\"\n    # TODO: Implement this function\n    # Should return dictionary with detailed analysis\n    pass\n\n# Solutions (uncomment to see reference implementations)\nprint(\"\\n\" + \"=\"*60)\nprint(\"üí° SOLUTIONS (Reference Implementations)\")\nprint(\"=\"*60)\n\n# Solution 1: Unique values validator\ndef validate_unique_values_solution(df: DataFrame, column: str) -> ValidationResult:\n    \"\"\"Reference implementation for unique values validation.\"\"\"\n    if column not in df.columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' does not exist\",\n            rule_name=\"unique_values\",\n            details={\"column\": column}\n        )\n    \n    total_count = df.count()\n    if total_count == 0:\n        return ValidationResult(\n            is_valid=True,\n            level=ValidationLevel.INFO,\n            message=f\"Column '{column}' is empty (no duplicates possible)\",\n            rule_name=\"unique_values\"\n        )\n    \n    unique_count = df.select(column).distinct().count()\n    duplicate_count = total_count - unique_count\n    \n    if duplicate_count > 0:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' has {duplicate_count} duplicate values\",\n            rule_name=\"unique_values\",\n            failed_records=duplicate_count,\n            details={\"total_count\": total_count, \"unique_count\": unique_count}\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=f\"Column '{column}' has all unique values\",\n        rule_name=\"unique_values\"\n    )\n\n# Test the solution\nprint(\"\\n‚úÖ Testing unique values validator:\")\nunique_result = validate_unique_values_solution(customer_df, \"customer_id\")\nprint(f\"   Result: {unique_result.message}\")\n\n# Solution 2: Date format validator\ndef validate_date_format_solution(df: DataFrame, column: str, date_pattern: str = r'^\\d{4}-\\d{2}-\\d{2}$') -> ValidationResult:\n    \"\"\"Reference implementation for date format validation.\"\"\"\n    if column not in df.columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' does not exist\",\n            rule_name=\"date_format\",\n            details={\"column\": column}\n        )\n    \n    invalid_count = df.filter(~F.col(column).rlike(date_pattern)).count()\n    total_count = df.count()\n    \n    if invalid_count > 0:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' has {invalid_count} invalid date formats\",\n            rule_name=\"date_format\",\n            failed_records=invalid_count,\n            details={\"pattern\": date_pattern, \"total_count\": total_count}\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=f\"All dates in '{column}' match expected format\",\n        rule_name=\"date_format\"\n    )\n\n# Test the solution\nprint(\"\\n‚úÖ Testing date format validator:\")\ndate_result = validate_date_format_solution(customer_df, \"registration_date\")\nprint(f\"   Result: {date_result.message}\")\n\nprint(f\"\\nüéØ Practice implementing these validators to master functional validation patterns!\")\nprint(f\"   Remember: Pure functions, immutable results, comprehensive error handling\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 7. Exercises and Practice\n# MAGIC \n# MAGIC Practice implementing functional validation patterns with these exercises.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# BEST PRACTICES for functional DataFrame validation\n\nprint(\"=== BEST PRACTICES ===\\n\")\n\n# ‚úÖ DO: Use pure functions that return validation results\ndef good_validation_pattern(df: DataFrame, column: str) -> ValidationResult:\n    \"\"\"\n    Pure function that validates data and returns structured results.\n    No side effects, easy to test, composable.\n    \"\"\"\n    null_count = df.filter(F.col(column).isNull()).count()\n    return ValidationResult(\n        is_valid=null_count == 0,\n        level=ValidationLevel.ERROR if null_count > 0 else ValidationLevel.INFO,\n        message=f\"Column '{column}' has {null_count} null values\",\n        rule_name=\"null_check\"\n    )\n\nprint(\"‚úÖ GOOD: Pure validation function with structured results\")\nprint(\"   - Returns ValidationResult with all necessary information\")\nprint(\"   - No side effects or external dependencies\")\nprint(\"   - Easy to test and compose\")\n\n# ‚úÖ DO: Use immutable validation pipelines\nprint(\"\\n‚úÖ GOOD: Immutable validation pipeline composition\")\nbase_validator = DataFrameValidator([\n    create_validation_rule(validate_not_null, [\"employee_id\"])\n])\n\n# Create new validator without mutating the original\nextended_validator = base_validator.add_rule(\n    create_validation_rule(validate_range, \"age\", 0, 120)\n)\n\nprint(f\"   - Base validator has {len(base_validator.rules)} rules\")\nprint(f\"   - Extended validator has {len(extended_validator.rules)} rules\")\nprint(\"   - Original validator unchanged (immutability)\")\n\n# ‚úÖ DO: Handle errors gracefully with Result types\nprint(\"\\n‚úÖ GOOD: Graceful error handling with structured results\")\ntry:\n    result = validate_range(valid_df, \"nonexistent_column\", 0, 100)\n    print(f\"   - Error handled gracefully: {result.message}\")\n    print(f\"   - Error details preserved: {result.details}\")\nexcept Exception:\n    print(\"   - No exceptions thrown, errors captured in ValidationResult\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"=== ANTI-PATTERNS ===\\n\")\n\n# ‚ùå DON'T: Use functions with side effects\ndef bad_validation_pattern_with_side_effects(df: DataFrame, column: str) -> bool:\n    \"\"\"\n    ANTI-PATTERN: Validation function with side effects.\n    Prints directly, modifies global state, hard to test.\n    \"\"\"\n    null_count = df.filter(F.col(column).isNull()).count()\n    \n    # ‚ùå Side effect: Direct printing\n    if null_count > 0:\n        print(f\"ERROR: Column {column} has {null_count} nulls!\")\n        # ‚ùå Side effect: Modifying global state\n        global validation_errors\n        validation_errors = validation_errors.get(column, 0) + null_count\n    \n    return null_count == 0\n\nprint(\"‚ùå BAD: Validation with side effects\")\nprint(\"   - Direct printing makes testing difficult\")\nprint(\"   - Global state modification breaks composability\")\nprint(\"   - Returns only boolean, loses important information\")\n\n# ‚ùå DON'T: Use mutable validation configuration\nclass BadMutableValidator:\n    \"\"\"\n    ANTI-PATTERN: Mutable validator that can be modified after creation.\n    \"\"\"\n    def __init__(self):\n        self.rules = []  # ‚ùå Mutable list\n        self.errors = []  # ‚ùå Mutable state\n    \n    def add_rule(self, rule):\n        self.rules.append(rule)  # ‚ùå Modifies existing object\n    \n    def validate(self, df):\n        self.errors.clear()  # ‚ùå Side effect: clearing previous state\n        for rule in self.rules:\n            # Validation logic would go here\n            pass\n        return len(self.errors) == 0\n\nprint(\"\\n‚ùå BAD: Mutable validator\")\nprint(\"   - Mutable state can lead to unexpected behavior\")\nprint(\"   - Side effects in validation method\")\nprint(\"   - Hard to reason about state changes\")\n\n# ‚ùå DON'T: Swallow exceptions without proper handling\ndef bad_validation_with_silent_failures(df: DataFrame, column: str) -> bool:\n    \"\"\"\n    ANTI-PATTERN: Silently handling exceptions without reporting.\n    \"\"\"\n    try:\n        result = df.select(column).count()\n        return True\n    except Exception:\n        # ‚ùå Silent failure - no information about what went wrong\n        return False\n\nprint(\"\\n‚ùå BAD: Silent exception handling\")\nprint(\"   - Hides important error information\")\nprint(\"   - Makes debugging difficult\")\nprint(\"   - Fails without explanation\")\n\n# ‚úÖ BETTER: Proper exception handling\ndef better_validation_with_error_capture(df: DataFrame, column: str) -> ValidationResult:\n    \"\"\"\n    BETTER: Capture exceptions and convert to structured results.\n    \"\"\"\n    try:\n        if column not in df.columns:\n            return ValidationResult(\n                is_valid=False,\n                level=ValidationLevel.ERROR,\n                message=f\"Column '{column}' not found in DataFrame\",\n                rule_name=\"column_existence\",\n                details={\"column\": column, \"available_columns\": df.columns}\n            )\n        \n        count = df.select(column).count()\n        return ValidationResult(\n            is_valid=True,\n            level=ValidationLevel.INFO,\n            message=f\"Column '{column}' accessible with {count} records\",\n            rule_name=\"column_existence\"\n        )\n        \n    except Exception as e:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Validation failed with exception: {str(e)}\",\n            rule_name=\"column_existence\",\n            details={\"exception\": str(e), \"exception_type\": type(e).__name__}\n        )\n\nprint(\"\\n‚úÖ BETTER: Structured exception handling\")\ntest_result = better_validation_with_error_capture(valid_df, \"nonexistent_column\")\nprint(f\"   - Structured error: {test_result.message}\")\nprint(f\"   - Error details preserved: {test_result.details}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"=== KEY PRINCIPLES ===\")\nprint(\"1. ‚úÖ Pure functions with no side effects\")\nprint(\"2. ‚úÖ Immutable data structures and pipelines\")  \nprint(\"3. ‚úÖ Structured error handling with Result types\")\nprint(\"4. ‚úÖ Composable validation rules\")\nprint(\"5. ‚úÖ Comprehensive error information\")\nprint(\"6. ‚ùå Avoid side effects in validation functions\")\nprint(\"7. ‚ùå Avoid mutable validation state\")\nprint(\"8. ‚ùå Avoid silent exception handling\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 6. Best Practices and Anti-Patterns\n# MAGIC \n# MAGIC Let's review functional programming best practices and anti-patterns in data validation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Great Expectations integration (conceptual - would require installation)\n# This cell demonstrates how you would integrate Great Expectations\n# In a real Databricks environment, you would install it first\n\ndef create_great_expectations_validator() -> ValidationRule:\n    \"\"\"\n    Conceptual integration with Great Expectations.\n    In practice, you would install great_expectations and configure it properly.\n    \"\"\"\n    def ge_validation_rule(df: DataFrame) -> ValidationResult:\n        \"\"\"\n        Wrapper function that adapts Great Expectations to our validation framework.\n        This is a conceptual example - actual implementation would require GE setup.\n        \"\"\"\n        try:\n            # This is pseudo-code demonstrating the integration pattern\n            # In reality, you would:\n            # 1. Convert PySpark DataFrame to Pandas or use Spark backend\n            # 2. Create GE DataContext and ExpectationSuite\n            # 3. Run validations and capture results\n            \n            # Simulated Great Expectations-style validation\n            issues_found = []\n            \n            # Simulate some GE-style expectations\n            if df.filter(F.col(\"age\") < 0).count() > 0:\n                issues_found.append(\"Found negative ages\")\n            \n            if df.filter(F.col(\"salary\") <= 0).count() > 0:\n                issues_found.append(\"Found non-positive salaries\")\n            \n            # Check for duplicates\n            total_count = df.count()\n            unique_count = df.select(\"employee_id\").distinct().count()\n            if total_count != unique_count:\n                issues_found.append(\"Found duplicate employee IDs\")\n            \n            if issues_found:\n                return ValidationResult(\n                    is_valid=False,\n                    level=ValidationLevel.ERROR,\n                    message=f\"Great Expectations validation failed: {'; '.join(issues_found)}\",\n                    rule_name=\"great_expectations\",\n                    details={\"issues\": issues_found}\n                )\n            \n            return ValidationResult(\n                is_valid=True,\n                level=ValidationLevel.INFO,\n                message=\"All Great Expectations validations passed\",\n                rule_name=\"great_expectations\"\n            )\n            \n        except Exception as e:\n            return ValidationResult(\n                is_valid=False,\n                level=ValidationLevel.ERROR,\n                message=f\"Great Expectations validation error: {str(e)}\",\n                rule_name=\"great_expectations\",\n                details={\"exception\": str(e)}\n            )\n    \n    return ge_validation_rule\n\n# Example of how to configure Great Expectations expectations\ndef create_employee_expectations_config() -> Dict[str, Any]:\n    \"\"\"\n    Configuration for Great Expectations expectations.\n    This would be used to set up a proper GE suite.\n    \"\"\"\n    return {\n        \"expectation_suite_name\": \"employee_data_suite\",\n        \"expectations\": [\n            {\n                \"expectation_type\": \"expect_table_columns_to_match_ordered_list\",\n                \"kwargs\": {\n                    \"column_list\": [\"employee_id\", \"name\", \"email\", \"age\", \"department\", \"salary\", \"hire_date\"]\n                }\n            },\n            {\n                \"expectation_type\": \"expect_column_values_to_not_be_null\",\n                \"kwargs\": {\"column\": \"employee_id\"}\n            },\n            {\n                \"expectation_type\": \"expect_column_values_to_be_between\",\n                \"kwargs\": {\"column\": \"age\", \"min_value\": 18, \"max_value\": 65}\n            },\n            {\n                \"expectation_type\": \"expect_column_values_to_be_between\",\n                \"kwargs\": {\"column\": \"salary\", \"min_value\": 0}\n            },\n            {\n                \"expectation_type\": \"expect_column_values_to_match_regex\",\n                \"kwargs\": {\n                    \"column\": \"email\",\n                    \"regex\": r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"\n                }\n            },\n            {\n                \"expectation_type\": \"expect_column_values_to_be_in_set\",\n                \"kwargs\": {\n                    \"column\": \"department\",\n                    \"value_set\": [\"Engineering\", \"Marketing\", \"Sales\", \"HR\", \"Finance\"]\n                }\n            }\n        ]\n    }\n\n# Test the conceptual GE integration\nprint(\"Testing conceptual Great Expectations integration...\")\nge_validator = create_great_expectations_validator()\n\n# Test with valid data\nge_result = ge_validator(valid_df)\nprint(f\"‚úÖ GE validation (valid data): {ge_result.message}\")\n\n# Test with problematic data (create data with duplicates)\nduplicate_data = [\n    (1, \"Alice Johnson\", \"alice@company.com\", 28, \"Engineering\", 75000.0, \"2020-01-15\"),\n    (1, \"Bob Smith\", \"bob@company.com\", 32, \"Marketing\", 65000.0, \"2019-03-10\"),  # Duplicate ID\n]\n\nduplicate_df = spark.createDataFrame(duplicate_data, expected_schema)\nge_result_with_issues = ge_validator(duplicate_df)\nprint(f\"‚ùå GE validation (with duplicates): {ge_result_with_issues.message}\")\n\n# Show how to integrate GE with our validation pipeline\ncomplete_validator = enhanced_validator.add_rule(ge_validator)\nprint(f\"\\nüìä Complete validation pipeline now has {len(complete_validator.rules)} rules\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 5. Integration with Great Expectations\n# MAGIC \n# MAGIC Great Expectations is a powerful library for data validation. Let's see how to integrate it with our functional validation approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Test validation pipeline with problematic data\nprint(\"Creating test data with validation violations...\")\n\n# Create DataFrame with various validation issues\nproblematic_data = [\n    (None, \"Alice Johnson\", \"alice@company.com\", 25, \"Engineering\", 75000.0, \"2020-01-15\"),  # Null ID\n    (2, None, \"bob-invalid-email\", 17, \"Marketing\", -5000.0, \"2019-03-10\"),  # Multiple issues\n    (3, \"Carol Davis\", \"carol@company.com\", 70, \"Engineering\", 80000.0, \"2021-06-01\"),  # Age > 65\n    (4, \"David Wilson\", \"david@company.com\", 35, \"Sales\", 70000.0, \"2018-11-20\"),  # Valid record\n]\n\nproblematic_schema = StructType([\n    StructField(\"employee_id\", IntegerType(), True),  # Allow nulls for testing\n    StructField(\"name\", StringType(), True),\n    StructField(\"email\", StringType(), False),\n    StructField(\"age\", IntegerType(), False),\n    StructField(\"department\", StringType(), False),\n    StructField(\"salary\", DoubleType(), False),\n    StructField(\"hire_date\", StringType(), False)\n])\n\nproblematic_df = spark.createDataFrame(problematic_data, problematic_schema)\n\nprint(\"Problematic data created:\")\nproblematic_df.show(truncate=False)\n\n# Test validation with problematic data\nprint(\"\\n\" + \"=\"*60)\nprint(\"Testing validation pipeline with problematic data...\")\nprint(\"=\"*60)\n\nproblematic_report = validator.validate(problematic_df)\nproblematic_report.print_summary()\n\n# Show detailed failure analysis\nprint(f\"\\nüîç Detailed Failure Analysis:\")\nfor result in problematic_report.results:\n    if not result.is_valid or result.level == ValidationLevel.WARNING:\n        print(f\"\\n‚ùå {result.rule_name}:\")\n        print(f\"   Message: {result.message}\")\n        if result.details:\n            for key, value in result.details.items():\n                print(f\"   {key}: {value}\")\n        if result.failed_records:\n            print(f\"   Failed Records: {result.failed_records}\")\n\n# Demonstrate validation rule composition - add custom business rules\nprint(f\"\\nüîß Adding custom business validation rules...\")\n\ndef validate_department_codes(df: DataFrame, valid_departments: List[str]) -> ValidationResult:\n    \"\"\"Custom validation for department codes.\"\"\"\n    if \"department\" not in df.columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=\"Department column not found\",\n            rule_name=\"department_codes\"\n        )\n    \n    # Check for invalid department codes\n    invalid_count = df.filter(~F.col(\"department\").isin(valid_departments)).count()\n    total_count = df.count()\n    \n    if invalid_count > 0:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Found {invalid_count} records with invalid department codes\",\n            rule_name=\"department_codes\",\n            failed_records=invalid_count,\n            details={\"valid_departments\": valid_departments}\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=\"All department codes are valid\",\n        rule_name=\"department_codes\"\n    )\n\n# Create enhanced validator with custom business rules\nenhanced_validator = validator.add_rules([\n    create_validation_rule(validate_department_codes, [\"Engineering\", \"Marketing\", \"Sales\", \"HR\", \"Finance\"])\n])\n\nprint(\"Testing enhanced validator with business rules...\")\nenhanced_report = enhanced_validator.validate(valid_df)\nenhanced_report.print_summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Composable validation pipeline\nfrom typing import Protocol\n\nclass ValidationRule(Protocol):\n    \"\"\"\n    Protocol for validation rules that can be composed in a pipeline.\n    All validation rules should return ValidationResult or List[ValidationResult].\n    \"\"\"\n    def __call__(self, df: DataFrame) -> Union[ValidationResult, List[ValidationResult]]:\n        ...\n\ndef create_validation_rule(validation_func: Callable, *args, **kwargs) -> ValidationRule:\n    \"\"\"\n    Higher-order function that creates a validation rule from a validation function\n    by partially applying its arguments.\n    \"\"\"\n    return partial(validation_func, *args, **kwargs)\n\nclass DataFrameValidator:\n    \"\"\"\n    Functional validation pipeline that composes multiple validation rules.\n    Immutable and side-effect free.\n    \"\"\"\n    \n    def __init__(self, rules: List[ValidationRule]):\n        self.rules = rules\n    \n    def validate(self, df: DataFrame) -> ValidationReport:\n        \"\"\"\n        Execute all validation rules and return a comprehensive report.\n        Pure function with no side effects.\n        \"\"\"\n        all_results = []\n        \n        for rule in self.rules:\n            try:\n                result = rule(df)\n                \n                # Handle both single results and lists of results\n                if isinstance(result, list):\n                    all_results.extend(result)\n                else:\n                    all_results.append(result)\n                    \n            except Exception as e:\n                # Convert exceptions to validation results\n                error_result = ValidationResult(\n                    is_valid=False,\n                    level=ValidationLevel.ERROR,\n                    message=f\"Validation rule failed with exception: {str(e)}\",\n                    rule_name=\"exception_handler\",\n                    details={\"exception\": str(e), \"exception_type\": type(e).__name__}\n                )\n                all_results.append(error_result)\n        \n        return ValidationReport.from_results(all_results)\n    \n    def add_rule(self, rule: ValidationRule) -> 'DataFrameValidator':\n        \"\"\"Return a new validator with an additional rule (immutable).\"\"\"\n        return DataFrameValidator(self.rules + [rule])\n    \n    def add_rules(self, rules: List[ValidationRule]) -> 'DataFrameValidator':\n        \"\"\"Return a new validator with additional rules (immutable).\"\"\"\n        return DataFrameValidator(self.rules + rules)\n\n# Create a comprehensive validation pipeline for employee data\ndef create_employee_data_validator() -> DataFrameValidator:\n    \"\"\"\n    Factory function that creates a validation pipeline for employee data.\n    Demonstrates composition of multiple validation rules.\n    \"\"\"\n    rules = [\n        # Schema validation\n        create_validation_rule(\n            validate_columns_exist, \n            [\"employee_id\", \"name\", \"email\", \"age\", \"department\", \"salary\", \"hire_date\"]\n        ),\n        create_validation_rule(validate_schema_compatibility, expected_schema),\n        \n        # Not null constraints\n        create_validation_rule(validate_not_null, [\"employee_id\", \"name\", \"email\"]),\n        \n        # Range validations\n        create_validation_rule(validate_range, \"age\", 18, 65),\n        create_validation_rule(validate_range, \"salary\", 0, None),\n        \n        # Pattern validations\n        create_validation_rule(\n            validate_regex_pattern, \n            \"email\", \n            r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\",\n            \"valid email format\"\n        ),\n    ]\n    \n    return DataFrameValidator(rules)\n\n# Test the validation pipeline with valid data\nprint(\"Testing validation pipeline with valid data...\")\nvalidator = create_employee_data_validator()\nreport = validator.validate(valid_df)\n\nreport.print_summary()\n\n# Show individual results\nprint(f\"\\nüìã Individual Validation Results ({len(report.results)} total):\")\nfor i, result in enumerate(report.results, 1):\n    status = \"‚úÖ\" if result.is_valid else \"‚ùå\"\n    print(f\"  {i}. {status} {result.rule_name}: {result.message}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 4. Composable Validation Pipeline\n# MAGIC \n# MAGIC Now let's create a functional validation pipeline that composes multiple validation rules and produces comprehensive reports.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Data constraint validation functions\ndef validate_not_null(df: DataFrame, columns: List[str]) -> List[ValidationResult]:\n    \"\"\"\n    Pure function to validate that specified columns don't contain null values.\n    Returns a list of validation results, one per column.\n    \"\"\"\n    results = []\n    \n    for col_name in columns:\n        if col_name not in df.columns:\n            results.append(ValidationResult(\n                is_valid=False,\n                level=ValidationLevel.ERROR,\n                message=f\"Column '{col_name}' does not exist\",\n                rule_name=\"not_null\",\n                details={\"column\": col_name}\n            ))\n            continue\n        \n        null_count = df.filter(F.col(col_name).isNull()).count()\n        total_count = df.count()\n        \n        if null_count > 0:\n            results.append(ValidationResult(\n                is_valid=False,\n                level=ValidationLevel.ERROR,\n                message=f\"Column '{col_name}' contains {null_count} null values out of {total_count} records\",\n                rule_name=\"not_null\",\n                failed_records=null_count,\n                details={\"column\": col_name, \"null_count\": null_count, \"total_count\": total_count}\n            ))\n        else:\n            results.append(ValidationResult(\n                is_valid=True,\n                level=ValidationLevel.INFO,\n                message=f\"Column '{col_name}' has no null values\",\n                rule_name=\"not_null\",\n                details={\"column\": col_name}\n            ))\n    \n    return results\n\ndef validate_range(df: DataFrame, column: str, min_val: Optional[float] = None, \n                  max_val: Optional[float] = None) -> ValidationResult:\n    \"\"\"\n    Pure function to validate that numeric column values fall within specified range.\n    \"\"\"\n    if column not in df.columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' does not exist\",\n            rule_name=\"range_validation\",\n            details={\"column\": column}\n        )\n    \n    conditions = []\n    range_description = []\n    \n    if min_val is not None:\n        conditions.append(F.col(column) < min_val)\n        range_description.append(f\"< {min_val}\")\n    \n    if max_val is not None:\n        conditions.append(F.col(column) > max_val)\n        range_description.append(f\"> {max_val}\")\n    \n    if not conditions:\n        return ValidationResult(\n            is_valid=True,\n            level=ValidationLevel.INFO,\n            message=f\"No range constraints specified for '{column}'\",\n            rule_name=\"range_validation\"\n        )\n    \n    # Find records outside the valid range\n    invalid_condition = reduce(lambda a, b: a | b, conditions)\n    invalid_count = df.filter(invalid_condition).count()\n    total_count = df.count()\n    \n    if invalid_count > 0:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' has {invalid_count} values outside valid range ({' or '.join(range_description)})\",\n            rule_name=\"range_validation\",\n            failed_records=invalid_count,\n            details={\n                \"column\": column, \n                \"invalid_count\": invalid_count, \n                \"total_count\": total_count,\n                \"min_val\": min_val,\n                \"max_val\": max_val\n            }\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=f\"All values in '{column}' are within valid range\",\n        rule_name=\"range_validation\",\n        details={\"column\": column, \"min_val\": min_val, \"max_val\": max_val}\n    )\n\ndef validate_regex_pattern(df: DataFrame, column: str, pattern: str, \n                          description: str = \"\") -> ValidationResult:\n    \"\"\"\n    Pure function to validate that string column values match a regex pattern.\n    \"\"\"\n    if column not in df.columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' does not exist\",\n            rule_name=\"regex_pattern\",\n            details={\"column\": column}\n        )\n    \n    # Count records that don't match the pattern\n    invalid_count = df.filter(~F.col(column).rlike(pattern)).count()\n    total_count = df.count()\n    \n    rule_desc = description or f\"pattern '{pattern}'\"\n    \n    if invalid_count > 0:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Column '{column}' has {invalid_count} values that don't match {rule_desc}\",\n            rule_name=\"regex_pattern\",\n            failed_records=invalid_count,\n            details={\n                \"column\": column, \n                \"pattern\": pattern,\n                \"description\": description,\n                \"invalid_count\": invalid_count, \n                \"total_count\": total_count\n            }\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=f\"All values in '{column}' match {rule_desc}\",\n        rule_name=\"regex_pattern\",\n        details={\"column\": column, \"pattern\": pattern, \"description\": description}\n    )\n\n# Test constraint validation functions\nprint(\"Testing constraint validation functions...\")\n\n# Test not null validation\nnull_results = validate_not_null(valid_df, [\"employee_id\", \"name\", \"email\"])\nfor result in null_results:\n    status = \"‚úÖ\" if result.is_valid else \"‚ùå\"\n    print(f\"{status} Not null test for {result.details.get('column')}: {result.message}\")\n\n# Test range validation  \nage_range_result = validate_range(valid_df, \"age\", min_val=18, max_val=65)\nprint(f\"‚úÖ Age range test: {age_range_result.message}\")\n\nsalary_range_result = validate_range(valid_df, \"salary\", min_val=0)\nprint(f\"‚úÖ Salary range test: {salary_range_result.message}\")\n\n# Test regex pattern validation\nemail_pattern_result = validate_regex_pattern(\n    valid_df, \"email\", r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\", \n    \"valid email format\"\n)\nprint(f\"‚úÖ Email pattern test: {email_pattern_result.message}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 3. Data Constraint Validation\n# MAGIC \n# MAGIC Beyond schema validation, we need to validate data content against business rules and constraints. Let's implement functional constraint validation patterns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Core schema validation functions\ndef validate_columns_exist(df: DataFrame, expected_columns: List[str]) -> ValidationResult:\n    \"\"\"\n    Pure function to validate that all expected columns exist in the DataFrame.\n    \"\"\"\n    actual_columns = set(df.columns)\n    expected_columns_set = set(expected_columns)\n    missing_columns = expected_columns_set - actual_columns\n    \n    if missing_columns:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=f\"Missing required columns: {sorted(missing_columns)}\",\n            rule_name=\"columns_exist\",\n            details={\"missing_columns\": sorted(missing_columns)}\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=f\"All {len(expected_columns)} required columns present\",\n        rule_name=\"columns_exist\"\n    )\n\ndef validate_schema_compatibility(df: DataFrame, expected_schema: StructType) -> ValidationResult:\n    \"\"\"\n    Pure function to validate DataFrame schema compatibility.\n    Checks both column presence and data type compatibility.\n    \"\"\"\n    actual_schema = df.schema\n    expected_fields = {field.name: field for field in expected_schema.fields}\n    actual_fields = {field.name: field for field in actual_schema.fields}\n    \n    issues = []\n    \n    # Check missing columns\n    missing_cols = set(expected_fields.keys()) - set(actual_fields.keys())\n    if missing_cols:\n        issues.append(f\"Missing columns: {sorted(missing_cols)}\")\n    \n    # Check type compatibility for common columns\n    common_cols = set(expected_fields.keys()) & set(actual_fields.keys())\n    type_mismatches = []\n    \n    for col_name in common_cols:\n        expected_type = expected_fields[col_name].dataType\n        actual_type = actual_fields[col_name].dataType\n        \n        if not _are_types_compatible(actual_type, expected_type):\n            type_mismatches.append(f\"{col_name}: expected {expected_type}, got {actual_type}\")\n    \n    if type_mismatches:\n        issues.append(f\"Type mismatches: {type_mismatches}\")\n    \n    if issues:\n        return ValidationResult(\n            is_valid=False,\n            level=ValidationLevel.ERROR,\n            message=\"; \".join(issues),\n            rule_name=\"schema_compatibility\",\n            details={\"issues\": issues}\n        )\n    \n    return ValidationResult(\n        is_valid=True,\n        level=ValidationLevel.INFO,\n        message=\"Schema is compatible\",\n        rule_name=\"schema_compatibility\"\n    )\n\ndef _are_types_compatible(actual: DataType, expected: DataType) -> bool:\n    \"\"\"Helper function to check type compatibility with some flexibility.\"\"\"\n    if actual == expected:\n        return True\n    \n    # Allow some implicit conversions\n    compatible_conversions = {\n        (IntegerType, LongType),\n        (FloatType, DoubleType),\n        (StringType, VarcharType),\n    }\n    \n    return (type(actual), type(expected)) in compatible_conversions\n\n# Test the schema validation functions\nprint(\"Testing schema validation functions...\")\n\n# Test with valid DataFrame\nresult1 = validate_columns_exist(valid_df, [\"employee_id\", \"name\", \"email\", \"age\"])\nprint(f\"‚úÖ Valid columns test: {result1.is_valid} - {result1.message}\")\n\n# Test with missing columns\nresult2 = validate_columns_exist(valid_df, [\"employee_id\", \"name\", \"missing_column\"])\nprint(f\"‚ùå Missing columns test: {result2.is_valid} - {result2.message}\")\n\n# Test schema compatibility\nresult3 = validate_schema_compatibility(valid_df, expected_schema)\nprint(f\"‚úÖ Schema compatibility test: {result3.is_valid} - {result3.message}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Validation result classes for functional error handling\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import NamedTuple\n\nclass ValidationLevel(Enum):\n    ERROR = \"ERROR\"\n    WARNING = \"WARNING\" \n    INFO = \"INFO\"\n\n@dataclass\nclass ValidationResult:\n    \"\"\"\n    Immutable validation result that contains all validation information.\n    Enables functional composition and chaining of validation results.\n    \"\"\"\n    is_valid: bool\n    level: ValidationLevel\n    message: str\n    rule_name: str\n    failed_records: Optional[int] = None\n    details: Optional[Dict[str, Any]] = None\n    \n    def __post_init__(self):\n        if self.details is None:\n            self.details = {}\n\n@dataclass \nclass ValidationReport:\n    \"\"\"\n    Immutable collection of validation results with summary statistics.\n    \"\"\"\n    results: List[ValidationResult]\n    total_validations: int\n    passed_validations: int\n    failed_validations: int\n    warnings: int\n    \n    @classmethod\n    def from_results(cls, results: List[ValidationResult]) -> 'ValidationReport':\n        \"\"\"Pure constructor that calculates summary statistics from results.\"\"\"\n        total = len(results)\n        passed = sum(1 for r in results if r.is_valid)\n        failed = sum(1 for r in results if not r.is_valid and r.level == ValidationLevel.ERROR)\n        warnings = sum(1 for r in results if r.level == ValidationLevel.WARNING)\n        \n        return cls(\n            results=results,\n            total_validations=total,\n            passed_validations=passed,\n            failed_validations=failed,\n            warnings=warnings\n        )\n    \n    @property\n    def is_valid(self) -> bool:\n        \"\"\"Check if all validations passed (warnings don't fail validation).\"\"\"\n        return self.failed_validations == 0\n    \n    def print_summary(self) -> None:\n        \"\"\"Display validation summary.\"\"\"\n        status = \"‚úÖ PASSED\" if self.is_valid else \"‚ùå FAILED\"\n        print(f\"\\n{status} - Validation Report Summary\")\n        print(f\"Total Validations: {self.total_validations}\")\n        print(f\"Passed: {self.passed_validations}\")\n        print(f\"Failed: {self.failed_validations}\")\n        print(f\"Warnings: {self.warnings}\")\n        \n        if not self.is_valid or self.warnings > 0:\n            print(\"\\nDetails:\")\n            for result in self.results:\n                if not result.is_valid or result.level == ValidationLevel.WARNING:\n                    icon = \"‚ö†Ô∏è\" if result.level == ValidationLevel.WARNING else \"‚ùå\"\n                    print(f\"  {icon} {result.rule_name}: {result.message}\")\n\nprint(\"‚úÖ Validation result classes defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 2. Functional Schema Validation Patterns\n# MAGIC \n# MAGIC Let's implement pure functions for various types of schema validation. These functions return validation results without side effects.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Create sample data for validation demonstrations\ndef create_sample_data():\n    \"\"\"\n    Pure function to create consistent sample data for validation examples.\n    Returns multiple DataFrames with different validation scenarios.\n    \"\"\"\n    \n    # Valid employee data\n    valid_employee_data = [\n        (1, \"Alice Johnson\", \"alice@company.com\", 28, \"Engineering\", 75000.0, \"2020-01-15\"),\n        (2, \"Bob Smith\", \"bob@company.com\", 32, \"Marketing\", 65000.0, \"2019-03-10\"),\n        (3, \"Carol Davis\", \"carol@company.com\", 29, \"Engineering\", 80000.0, \"2021-06-01\"),\n        (4, \"David Wilson\", \"david@company.com\", 35, \"Sales\", 70000.0, \"2018-11-20\"),\n    ]\n    \n    valid_schema = StructType([\n        StructField(\"employee_id\", IntegerType(), False),\n        StructField(\"name\", StringType(), False),\n        StructField(\"email\", StringType(), False),\n        StructField(\"age\", IntegerType(), False),\n        StructField(\"department\", StringType(), False),\n        StructField(\"salary\", DoubleType(), False),\n        StructField(\"hire_date\", StringType(), False)\n    ])\n    \n    valid_df = spark.createDataFrame(valid_employee_data, valid_schema)\n    \n    # Invalid data scenarios for validation testing\n    invalid_data_scenarios = {\n        \"missing_columns\": [\n            (1, \"Alice Johnson\", \"alice@company.com\", 28, \"Engineering\"),  # Missing salary and hire_date\n            (2, \"Bob Smith\", \"bob@company.com\", 32, \"Marketing\")\n        ],\n        \n        \"wrong_types\": [\n            (\"1\", \"Alice Johnson\", \"alice@company.com\", \"28\", \"Engineering\", \"75000\", \"2020-01-15\"),  # String IDs and age\n            (\"2\", \"Bob Smith\", \"bob@company.com\", \"32\", \"Marketing\", \"65000\", \"2019-03-10\")\n        ],\n        \n        \"null_violations\": [\n            (1, None, \"alice@company.com\", 28, \"Engineering\", 75000.0, \"2020-01-15\"),  # Null name\n            (2, \"Bob Smith\", None, 32, \"Marketing\", 65000.0, \"2019-03-10\")  # Null email\n        ],\n        \n        \"constraint_violations\": [\n            (1, \"Alice Johnson\", \"invalid-email\", -5, \"InvalidDept\", -10000.0, \"invalid-date\"),  # Multiple violations\n            (2, \"Bob Smith\", \"bob@company.com\", 150, \"Marketing\", 1000000.0, \"2019-13-45\")\n        ]\n    }\n    \n    return valid_df, invalid_data_scenarios, valid_schema\n\n# Generate sample data\nvalid_df, invalid_scenarios, expected_schema = create_sample_data()\n\nprint(\"‚úÖ Sample data created for validation demonstrations\")\nvalid_df.show(truncate=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 1. Schema Validation Fundamentals\n# MAGIC \n# MAGIC Schema validation ensures that DataFrames conform to expected structure, data types, and constraints. Let's start with functional approaches to schema validation.\n# MAGIC \n# MAGIC ### Core Validation Types\n# MAGIC \n# MAGIC 1. **Structural Validation**: Column presence, order, and naming\n# MAGIC 2. **Type Validation**: Data type compatibility and conversion\n# MAGIC 3. **Constraint Validation**: Null checks, value ranges, format patterns\n# MAGIC 4. **Relationship Validation**: Foreign key constraints and referential integrity",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Essential imports for DataFrame and schema validation\nfrom pyspark.sql import SparkSession, DataFrame\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.utils import AnalysisException\nfrom typing import Dict, List, Tuple, Optional, Callable, Any, Union\nfrom functools import reduce, partial\nimport json\nfrom datetime import datetime\nimport re\n\n# Initialize Spark session (if not already available)\nspark = SparkSession.builder.appName(\"DataFrameSchemaValidation\").getOrCreate()\n\nprint(\"‚úÖ Setup complete - Ready for DataFrame and schema validation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Databricks notebook source\n# MAGIC %md\n# MAGIC # 3.2 Validating DataFrames and Schemas\n# MAGIC \n# MAGIC This notebook demonstrates functional approaches to DataFrame and schema validation in PySpark. We'll explore various validation strategies that maintain functional programming principles while ensuring data quality and integrity.\n# MAGIC \n# MAGIC ## Learning Objectives\n# MAGIC \n# MAGIC By the end of this notebook, you will understand how to:\n# MAGIC - Implement functional schema validation patterns\n# MAGIC - Create reusable validation functions\n# MAGIC - Handle validation errors gracefully\n# MAGIC - Build validation pipelines with pure functions\n# MAGIC - Use Great Expectations with PySpark\n# MAGIC - Create custom validation rules\n# MAGIC \n# MAGIC ## Prerequisites\n# MAGIC \n# MAGIC - Understanding of PySpark DataFrames\n# MAGIC - Knowledge of functional programming concepts\n# MAGIC - Familiarity with schema definitions",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}