{
 "cells": [
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Databricks job configuration for automated testing\ndef create_databricks_test_job_config() -> Dict[str, Any]:\n    \"\"\"\n    Pure function that creates Databricks job configuration for CI/CD testing.\n    This configuration can be used with Databricks CLI or REST API.\n    \"\"\"\n    job_config = {\n        \"name\": \"PySpark-Functional-Tests-CI\",\n        \"new_cluster\": {\n            \"spark_version\": \"12.2.x-scala2.12\",\n            \"node_type_id\": \"i3.xlarge\",\n            \"num_workers\": 2,\n            \"spark_conf\": {\n                \"spark.sql.adaptive.enabled\": \"true\",\n                \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n                \"spark.databricks.cluster.profile\": \"singleNode\",\n                \"spark.master\": \"local[*]\"\n            },\n            \"custom_tags\": {\n                \"ResourceClass\": \"SingleNode\",\n                \"Environment\": \"CI\",\n                \"Project\": \"PySpark-Functional-Pipeline\"\n            }\n        },\n        \"libraries\": [\n            {\"pypi\": {\"package\": \"pytest==7.2.0\"}},\n            {\"pypi\": {\"package\": \"pytest-cov==4.0.0\"}},\n            {\"pypi\": {\"package\": \"pyyaml==6.0\"}},\n            {\"pypi\": {\"package\": \"great-expectations==0.15.50\"}}\n        ],\n        \"notebook_task\": {\n            \"notebook_path\": \"/Repos/shared/pyspark-pipeline/notebooks/test_runner\",\n            \"base_parameters\": {\n                \"environment\": \"ci\",\n                \"test_suite\": \"full\",\n                \"coverage_threshold\": \"80\"\n            }\n        },\n        \"timeout_seconds\": 3600,\n        \"max_retries\": 1,\n        \"email_notifications\": {\n            \"on_failure\": [\"team@company.com\"],\n            \"on_success\": [\"team@company.com\"]\n        },\n        \"webhook_notifications\": {\n            \"on_failure\": [\n                {\n                    \"id\": \"slack-webhook\",\n                    \"url\": \"https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK\"\n                }\n            ]\n        }\n    }\n    \n    return job_config\n\n# Quality gate job configuration\ndef create_quality_gate_job_config() -> Dict[str, Any]:\n    \"\"\"\n    Pure function for quality gate validation job in Databricks.\n    \"\"\"\n    quality_config = {\n        \"name\": \"Data-Quality-Gates\",\n        \"new_cluster\": {\n            \"spark_version\": \"12.2.x-scala2.12\",\n            \"node_type_id\": \"i3.xlarge\",\n            \"num_workers\": 1,\n            \"spark_conf\": {\n                \"spark.sql.adaptive.enabled\": \"true\"\n            }\n        },\n        \"libraries\": [\n            {\"pypi\": {\"package\": \"great-expectations==0.15.50\"}},\n            {\"pypi\": {\"package\": \"delta-lake==2.2.0\"}}\n        ],\n        \"python_wheel_task\": {\n            \"package_name\": \"pyspark_functional_pipeline\",\n            \"entry_point\": \"quality_check\",\n            \"parameters\": [\"--environment\", \"staging\", \"--strict-mode\"]\n        },\n        \"timeout_seconds\": 1800\n    }\n    \n    return quality_config\n\n# Test execution notebook template\ntest_runner_notebook = '''\n# Databricks notebook source\n# MAGIC %md\n# MAGIC # Test Runner Notebook\n# MAGIC \n# MAGIC This notebook is executed by CI/CD pipelines to run tests in Databricks environment.\n\n# COMMAND ----------\n\n# MAGIC %pip install pytest pytest-cov\n\n# COMMAND ----------\n\nimport pytest\nimport sys\nimport os\nfrom pyspark.sql import SparkSession\n\n# Get parameters from job\nenvironment = dbutils.widgets.get(\"environment\")\ntest_suite = dbutils.widgets.get(\"test_suite\")\ncoverage_threshold = float(dbutils.widgets.get(\"coverage_threshold\"))\n\nprint(f\"Running tests with environment: {environment}\")\nprint(f\"Test suite: {test_suite}\")\nprint(f\"Coverage threshold: {coverage_threshold}%\")\n\n# COMMAND ----------\n\n# Set up test environment\nspark = SparkSession.builder.getOrCreate()\n\n# Add project modules to path\nsys.path.append(\"/Workspace/Repos/shared/pyspark-pipeline/src\")\n\n# COMMAND ----------\n\n# Run tests based on suite parameter\nif test_suite == \"unit\":\n    test_path = \"/Workspace/Repos/shared/pyspark-pipeline/tests/unit\"\nelif test_suite == \"integration\":\n    test_path = \"/Workspace/Repos/shared/pyspark-pipeline/tests/integration\"\nelse:  # full\n    test_path = \"/Workspace/Repos/shared/pyspark-pipeline/tests\"\n\n# Execute tests\nexit_code = pytest.main([\n    test_path,\n    \"-v\",\n    \"--tb=short\",\n    f\"--cov=/Workspace/Repos/shared/pyspark-pipeline/src\",\n    f\"--cov-fail-under={coverage_threshold}\",\n    \"--cov-report=term-missing\",\n    \"--junitxml=test-results.xml\"\n])\n\n# COMMAND ----------\n\n# Handle test results\nif exit_code == 0:\n    print(\"✅ All tests passed!\")\n    dbutils.notebook.exit(\"SUCCESS\")\nelse:\n    print(\"❌ Tests failed!\")\n    dbutils.notebook.exit(\"FAILURE\")\n'''\n\nprint(\"📋 Databricks Test Job Configuration:\")\ntest_config = create_databricks_test_job_config()\nprint(json.dumps(test_config, indent=2))\n\nprint(\"\\n📋 Quality Gate Job Configuration:\")\nquality_config = create_quality_gate_job_config()\nprint(json.dumps(quality_config, indent=2))\n\nprint(f\"\\n📝 Test Runner Notebook Template:\")\nprint(\"=\" * 60)\nprint(test_runner_notebook)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 3. Databricks Job Configuration for Testing\n# MAGIC \n# MAGIC Let's create Databricks job configurations that can be used in CI/CD pipelines for running tests in the actual Databricks environment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# GitHub Actions CI pipeline configuration\ngithub_actions_ci = \"\"\"\nname: PySpark Functional Tests CI\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main, develop ]\n\nenv:\n  PYTHON_VERSION: '3.8'\n  SPARK_VERSION: '3.3.0'\n  JAVA_VERSION: '11'\n\njobs:\n  quality-checks:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n    \n    - name: Set up Java\n      uses: actions/setup-java@v3\n      with:\n        java-version: ${{ env.JAVA_VERSION }}\n        distribution: 'temurin'\n    \n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install pytest pytest-cov black flake8 mypy\n    \n    - name: Code formatting check\n      run: black --check src/ tests/\n    \n    - name: Linting\n      run: flake8 src/ tests/\n    \n    - name: Type checking\n      run: mypy src/\n    \n    - name: Security scan\n      run: |\n        pip install bandit\n        bandit -r src/\n\n  unit-tests:\n    runs-on: ubuntu-latest\n    needs: quality-checks\n    strategy:\n      matrix:\n        python-version: ['3.8', '3.9', '3.10']\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n    \n    - name: Set up Java\n      uses: actions/setup-java@v3\n      with:\n        java-version: ${{ env.JAVA_VERSION }}\n        distribution: 'temurin'\n    \n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install pytest pytest-cov pytest-xdist\n    \n    - name: Run unit tests\n      run: |\n        pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=term-missing\n    \n    - name: Upload coverage to Codecov\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n        flags: unittests\n\n  integration-tests:\n    runs-on: ubuntu-latest\n    needs: unit-tests\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n    \n    - name: Set up Java\n      uses: actions/setup-java@v3\n      with:\n        java-version: ${{ env.JAVA_VERSION }}\n        distribution: 'temurin'\n    \n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install pytest\n    \n    - name: Run integration tests\n      run: |\n        pytest tests/integration/ -v --tb=short\n    \n    - name: Generate test report\n      run: |\n        pytest tests/ --junitxml=test-results.xml\n    \n    - name: Upload test results\n      uses: actions/upload-artifact@v3\n      with:\n        name: test-results\n        path: test-results.xml\n\n  databricks-validation:\n    runs-on: ubuntu-latest\n    needs: integration-tests\n    if: github.ref == 'refs/heads/main'\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Install Databricks CLI\n      run: |\n        pip install databricks-cli\n    \n    - name: Configure Databricks CLI\n      run: |\n        echo \"[DEFAULT]\" > ~/.databrickscfg\n        echo \"host = ${{ secrets.DATABRICKS_HOST }}\" >> ~/.databrickscfg\n        echo \"token = ${{ secrets.DATABRICKS_TOKEN }}\" >> ~/.databrickscfg\n    \n    - name: Upload notebooks to Databricks\n      run: |\n        databricks workspace import_dir notebooks /Repos/shared/pyspark-pipeline/notebooks --overwrite\n    \n    - name: Run Databricks tests\n      run: |\n        databricks runs submit --json-file .github/workflows/databricks-test-job.json\n\"\"\"\n\nprint(\"📋 GitHub Actions CI Pipeline Configuration:\")\nprint(\"=\" * 60)\nprint(github_actions_ci)\n\n# Azure DevOps pipeline configuration\nazure_devops_pipeline = \"\"\"\n# Azure DevOps Pipeline for PySpark Functional Tests\ntrigger:\n  branches:\n    include:\n      - main\n      - develop\n\npr:\n  branches:\n    include:\n      - main\n      - develop\n\nvariables:\n  pythonVersion: '3.8'\n  sparkVersion: '3.3.0'\n\nstages:\n- stage: QualityGates\n  displayName: 'Code Quality Gates'\n  jobs:\n  - job: CodeQuality\n    displayName: 'Code Quality Checks'\n    pool:\n      vmImage: 'ubuntu-latest'\n    \n    steps:\n    - task: UsePythonVersion@0\n      inputs:\n        versionSpec: $(pythonVersion)\n        displayName: 'Use Python $(pythonVersion)'\n    \n    - script: |\n        pip install -r requirements.txt\n        pip install black flake8 mypy bandit pytest\n      displayName: 'Install dependencies'\n    \n    - script: |\n        black --check src/ tests/\n      displayName: 'Code formatting check'\n    \n    - script: |\n        flake8 src/ tests/\n      displayName: 'Linting'\n    \n    - script: |\n        mypy src/\n      displayName: 'Type checking'\n    \n    - script: |\n        bandit -r src/\n      displayName: 'Security scan'\n\n- stage: Testing\n  displayName: 'Testing'\n  dependsOn: QualityGates\n  jobs:\n  - job: UnitTests\n    displayName: 'Unit Tests'\n    pool:\n      vmImage: 'ubuntu-latest'\n    \n    steps:\n    - task: UsePythonVersion@0\n      inputs:\n        versionSpec: $(pythonVersion)\n    \n    - script: |\n        pip install -r requirements.txt\n        pip install pytest pytest-cov pytest-azurepipelines\n      displayName: 'Install dependencies'\n    \n    - script: |\n        pytest tests/unit/ --cov=src --cov-report=xml --cov-report=html --junitxml=test-results.xml\n      displayName: 'Run unit tests'\n    \n    - task: PublishTestResults@2\n      inputs:\n        testResultsFiles: 'test-results.xml'\n        testRunTitle: 'Unit Tests'\n    \n    - task: PublishCodeCoverageResults@1\n      inputs:\n        codeCoverageTool: 'Cobertura'\n        summaryFileLocation: 'coverage.xml'\n        reportDirectory: 'htmlcov'\n\n  - job: IntegrationTests\n    displayName: 'Integration Tests'\n    dependsOn: UnitTests\n    pool:\n      vmImage: 'ubuntu-latest'\n    \n    steps:\n    - task: UsePythonVersion@0\n      inputs:\n        versionSpec: $(pythonVersion)\n    \n    - script: |\n        pip install -r requirements.txt\n        pip install pytest pytest-azurepipelines\n      displayName: 'Install dependencies'\n    \n    - script: |\n        pytest tests/integration/ --junitxml=integration-results.xml\n      displayName: 'Run integration tests'\n    \n    - task: PublishTestResults@2\n      inputs:\n        testResultsFiles: 'integration-results.xml'\n        testRunTitle: 'Integration Tests'\n\n- stage: DatabricksValidation\n  displayName: 'Databricks Validation'\n  dependsOn: Testing\n  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))\n  jobs:\n  - job: DatabricksTests\n    displayName: 'Databricks Environment Tests'\n    pool:\n      vmImage: 'ubuntu-latest'\n    \n    steps:\n    - script: |\n        pip install databricks-cli\n      displayName: 'Install Databricks CLI'\n    \n    - script: |\n        echo \"[DEFAULT]\" > ~/.databrickscfg\n        echo \"host = $(DATABRICKS_HOST)\" >> ~/.databrickscfg\n        echo \"token = $(DATABRICKS_TOKEN)\" >> ~/.databrickscfg\n      displayName: 'Configure Databricks CLI'\n    \n    - script: |\n        databricks workspace import_dir notebooks /Repos/shared/pyspark-pipeline/notebooks --overwrite\n      displayName: 'Upload notebooks'\n    \n    - script: |\n        databricks runs submit --json-file deployment/databricks-test-job.json\n      displayName: 'Run Databricks tests'\n\"\"\"\n\nprint(\"\\n📋 Azure DevOps Pipeline Configuration:\")\nprint(\"=\" * 60)\nprint(azure_devops_pipeline)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 2. CI/CD Pipeline Configuration\n# MAGIC \n# MAGIC Let's create CI/CD pipeline configurations that integrate functional testing with Databricks. We'll cover GitHub Actions, Azure DevOps, and other popular CI/CD platforms.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Recommended project structure for PySpark functional programming with CI/CD\nproject_structure = \"\"\"\npyspark-data-project/\n├── .github/\n│   └── workflows/\n│       ├── ci.yml              # CI pipeline configuration\n│       ├── cd.yml              # CD pipeline configuration\n│       └── quality-gates.yml   # Quality gate automation\n├── src/\n│   ├── __init__.py\n│   ├── data_processing/\n│   │   ├── __init__.py\n│   │   ├── transformations.py  # Pure transformation functions\n│   │   ├── validations.py      # Data validation functions\n│   │   └── aggregations.py     # Aggregation functions\n│   ├── utils/\n│   │   ├── __init__.py\n│   │   ├── spark_utils.py      # Spark utility functions\n│   │   ├── config.py           # Configuration management\n│   │   └── logging_utils.py    # Logging utilities\n│   └── jobs/\n│       ├── __init__.py\n│       ├── etl_pipeline.py     # Main ETL pipeline\n│       └── data_quality_check.py\n├── tests/\n│   ├── __init__.py\n│   ├── unit/\n│   │   ├── __init__.py\n│   │   ├── test_transformations.py\n│   │   ├── test_validations.py\n│   │   └── test_aggregations.py\n│   ├── integration/\n│   │   ├── __init__.py\n│   │   ├── test_etl_pipeline.py\n│   │   └── test_data_quality.py\n│   ├── fixtures/\n│   │   ├── __init__.py\n│   │   └── test_data.py        # Test data generators\n│   └── conftest.py            # Pytest configuration\n├── config/\n│   ├── dev.yml                # Development environment config\n│   ├── staging.yml            # Staging environment config\n│   └── prod.yml               # Production environment config\n├── notebooks/\n│   ├── exploratory/           # Data exploration notebooks\n│   ├── experiments/           # ML experiment notebooks\n│   └── demo/                  # Demo and documentation notebooks\n├── scripts/\n│   ├── run_tests.sh          # Test execution script\n│   ├── deploy.sh             # Deployment script\n│   └── quality_check.py      # Quality gate validation\n├── requirements.txt          # Python dependencies\n├── setup.py                  # Package setup configuration\n├── pytest.ini               # Pytest configuration\n├── .gitignore               # Git ignore patterns\n└── README.md                # Project documentation\n\"\"\"\n\nprint(\"📁 Recommended Project Structure:\")\nprint(project_structure)\n\n# Core principles for CI/CD-friendly structure\nprint(\"\\n🎯 Key Structural Principles:\")\nprint(\"1. ✅ Separation of Concerns - Clear module boundaries\")\nprint(\"2. ✅ Testable Architecture - Easy unit and integration testing\")\nprint(\"3. ✅ Environment Agnostic - Configuration externalized\")\nprint(\"4. ✅ Functional Organization - Pure functions grouped logically\")\nprint(\"5. ✅ CI/CD Ready - Standard structure for automation\")\n\n# Configuration management for different environments\ndef create_environment_config() -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Pure function that creates environment-specific configurations.\n    Demonstrates functional approach to configuration management.\n    \"\"\"\n    base_config = {\n        \"spark\": {\n            \"app_name\": \"PySpark-Functional-Pipeline\",\n            \"executor_memory\": \"2g\",\n            \"driver_memory\": \"1g\"\n        },\n        \"data\": {\n            \"input_format\": \"delta\",\n            \"output_format\": \"delta\"\n        },\n        \"quality\": {\n            \"enable_validation\": True,\n            \"failure_threshold\": 0.05\n        }\n    }\n    \n    environments = {\n        \"dev\": {\n            **base_config,\n            \"data\": {\n                **base_config[\"data\"],\n                \"input_path\": \"/tmp/dev/input\",\n                \"output_path\": \"/tmp/dev/output\"\n            },\n            \"spark\": {\n                **base_config[\"spark\"],\n                \"executor_instances\": 1\n            }\n        },\n        \"staging\": {\n            **base_config,\n            \"data\": {\n                **base_config[\"data\"],\n                \"input_path\": \"/mnt/staging/input\",\n                \"output_path\": \"/mnt/staging/output\"\n            },\n            \"spark\": {\n                **base_config[\"spark\"],\n                \"executor_instances\": 3\n            }\n        },\n        \"prod\": {\n            **base_config,\n            \"data\": {\n                **base_config[\"data\"],\n                \"input_path\": \"/mnt/prod/input\",\n                \"output_path\": \"/mnt/prod/output\"\n            },\n            \"spark\": {\n                **base_config[\"spark\"],\n                \"executor_instances\": 10,\n                \"executor_memory\": \"4g\"\n            },\n            \"quality\": {\n                **base_config[\"quality\"],\n                \"failure_threshold\": 0.01  # Stricter in production\n            }\n        }\n    }\n    \n    return environments\n\n# Test the configuration function\nconfigs = create_environment_config()\nprint(f\"\\n📋 Environment Configurations Created:\")\nfor env, config in configs.items():\n    print(f\"   {env}: {config['spark']['executor_instances']} executors, {config['quality']['failure_threshold']} failure threshold\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 1. Project Structure for CI/CD\n# MAGIC \n# MAGIC A well-organized project structure is crucial for effective CI/CD integration. Let's explore functional project organization patterns that work well with Databricks Repos and automated testing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# COMMAND ----------\n\n# Essential imports for CI/CD integration\nfrom pyspark.sql import SparkSession, DataFrame\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom typing import Dict, List, Tuple, Optional, Callable, Any, Union\nfrom functools import reduce, partial\nimport json\nimport os\nimport yaml\nfrom datetime import datetime\nimport subprocess\nimport sys\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"CICDIntegration\").getOrCreate()\n\nprint(\"✅ Setup complete - Ready for CI/CD integration patterns!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Databricks notebook source\n# MAGIC %md\n# MAGIC # 3.3 Integrating Tests with Databricks Repos and CI/CD\n# MAGIC \n# MAGIC This notebook demonstrates how to integrate functional PySpark tests with Databricks Repos and CI/CD pipelines. We'll explore version control, automated testing, and deployment strategies that maintain functional programming principles.\n# MAGIC \n# MAGIC ## Learning Objectives\n# MAGIC \n# MAGIC By the end of this notebook, you will understand how to:\n# MAGIC - Structure PySpark projects for version control and CI/CD\n# MAGIC - Configure Databricks Repos for collaborative development\n# MAGIC - Create CI/CD pipelines with functional tests\n# MAGIC - Implement deployment strategies with quality gates\n# MAGIC - Handle environment-specific configurations functionally\n# MAGIC - Monitor test execution and quality metrics\n# MAGIC \n# MAGIC ## Prerequisites\n# MAGIC \n# MAGIC - Understanding of Git version control\n# MAGIC - Knowledge of CI/CD concepts\n# MAGIC - Familiarity with Databricks Repos\n# MAGIC - Experience with functional testing patterns",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}