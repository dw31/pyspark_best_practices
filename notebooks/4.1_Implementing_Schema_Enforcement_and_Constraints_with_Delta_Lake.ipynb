{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Implementing Schema Enforcement and Constraints with Delta Lake\n",
    "\n",
    "This notebook demonstrates how to implement robust data quality controls using Delta Lake's schema enforcement and constraint capabilities.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Delta Lake's schema enforcement mechanisms\n",
    "- Implement CHECK constraints for data validation\n",
    "- Handle schema evolution safely\n",
    "- Use Delta Lake features for data integrity\n",
    "- Build declarative data quality pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Delta Lake Schema Enforcement\n",
    "\n",
    "Delta Lake provides strong schema enforcement that prevents data corruption by ensuring all writes conform to the table's schema. This is a cornerstone of data reliability in the lakehouse architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from delta import *\n",
    "\n",
    "# Configure Spark for Delta Lake\n",
    "# Note: In Databricks, this is pre-configured\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DeltaLakeSchemaEnforcement\")\n",
    "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Sample customer data with various data quality issues\n",
    "customer_data = [\n",
    "    (1, \"John Doe\", \"john.doe@email.com\", 28, \"Premium\", 1200.50, \"2023-01-15\"),\n",
    "    (2, \"Jane Smith\", \"jane.smith@email.com\", 35, \"Standard\", 850.75, \"2023-01-16\"),\n",
    "    (3, \"Bob Johnson\", \"bob@company.com\", 42, \"Premium\", 2100.00, \"2023-01-17\"),\n",
    "    (4, \"Alice Brown\", \"alice.brown@service.org\", 29, \"Gold\", 1500.25, \"2023-01-18\"),\n",
    "    (5, \"Charlie Wilson\", \"charlie@email.com\", 33, \"Standard\", 750.00, \"2023-01-19\")\n",
    "]\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),  # NOT NULL\n",
    "    StructField(\"name\", StringType(), False),          # NOT NULL\n",
    "    StructField(\"email\", StringType(), False),         # NOT NULL\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"account_balance\", DoubleType(), True),\n",
    "    StructField(\"created_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "customers_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "\n",
    "print(\"Customer data created:\")\n",
    "customers_df.show()\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Delta Tables with Schema Enforcement\n",
    "\n",
    "Let's create a Delta table and demonstrate basic schema enforcement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delta table with initial schema\n",
    "delta_table_path = \"/tmp/delta_customers\"\n",
    "\n",
    "# Clear existing data for demo\n",
    "try:\n",
    "    dbutils.fs.rm(delta_table_path, True)\n",
    "except:\n",
    "    pass  # Path doesn't exist yet\n",
    "\n",
    "print(\"=== Creating Initial Delta Table ===\")\n",
    "\n",
    "# Write initial data to create the Delta table\n",
    "(customers_df\n",
    " .withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .save(delta_table_path))\n",
    "\n",
    "print(f\"Delta table created at: {delta_table_path}\")\n",
    "\n",
    "# Read the table back to verify\n",
    "delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "print(\"\\nTable contents:\")\n",
    "delta_df.show()\n",
    "print(\"\\nTable schema:\")\n",
    "delta_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Schema Enforcement\n",
    "\n",
    "Now let's see how Delta Lake prevents schema violations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Schema Enforcement Demonstrations ===\")\n",
    "\n",
    "# 1. Adding extra columns (will fail without schema evolution)\n",
    "print(\"\\n1. Testing extra column addition:\")\n",
    "\n",
    "bad_data_extra_col = [\n",
    "    (6, \"David Lee\", \"david@email.com\", 31, \"Premium\", 900.00, \"2023-01-20\", \"New York\")  # Extra column\n",
    "]\n",
    "\n",
    "bad_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"account_balance\", DoubleType(), True),\n",
    "    StructField(\"created_date\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True)  # Extra column!\n",
    "])\n",
    "\n",
    "bad_df = spark.createDataFrame(bad_data_extra_col, bad_schema)\n",
    "bad_df = bad_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (bad_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(delta_table_path))\n",
    "    print(\"❌ Unexpected: Write succeeded\")\n",
    "except Exception as e:\n",
    "    print(f\"✅ Expected: Schema enforcement prevented write - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Wrong data types (will fail)\n",
    "print(\"\\n2. Testing wrong data types:\")\n",
    "\n",
    "bad_data_types = [\n",
    "    (\"seven\", \"Eve Green\", \"eve@email.com\", 25, \"Standard\", 600.00, \"2023-01-21\")  # String instead of Int\n",
    "]\n",
    "\n",
    "bad_type_df = spark.createDataFrame(bad_data_types, \n",
    "                                   [\"customer_id\", \"name\", \"email\", \"age\", \"tier\", \"account_balance\", \"created_date\"])\n",
    "bad_type_df = bad_type_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (bad_type_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(delta_table_path))\n",
    "    print(\"❌ Unexpected: Write succeeded\")\n",
    "except Exception as e:\n",
    "    print(f\"✅ Expected: Type mismatch prevented write - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Missing required columns (will fail)\n",
    "print(\"\\n3. Testing missing required columns:\")\n",
    "\n",
    "incomplete_data = [\n",
    "    (7, \"Frank Miller\", 40, \"Premium\", 1100.00)  # Missing email column\n",
    "]\n",
    "\n",
    "incomplete_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    # Missing email field!\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"account_balance\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "incomplete_df = spark.createDataFrame(incomplete_data, incomplete_schema)\n",
    "\n",
    "try:\n",
    "    (incomplete_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(delta_table_path))\n",
    "    print(\"❌ Unexpected: Write succeeded\")\n",
    "except Exception as e:\n",
    "    print(f\"✅ Expected: Missing column prevented write - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe Schema Evolution\n",
    "\n",
    "When you need to evolve schemas, Delta Lake provides safe mechanisms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Safe Schema Evolution ===\")\n",
    "\n",
    "# 1. Adding optional columns with mergeSchema option\n",
    "print(\"\\n1. Adding new optional columns:\")\n",
    "\n",
    "new_customer_data = [\n",
    "    (6, \"David Lee\", \"david@email.com\", 31, \"Premium\", 900.00, \"2023-01-20\", \"New York\", \"Engineering\")\n",
    "]\n",
    "\n",
    "evolved_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"account_balance\", DoubleType(), True),\n",
    "    StructField(\"created_date\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),        # New optional column\n",
    "    StructField(\"department\", StringType(), True)   # New optional column\n",
    "])\n",
    "\n",
    "evolved_df = spark.createDataFrame(new_customer_data, evolved_schema)\n",
    "evolved_df = evolved_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Enable schema merging\n",
    "(evolved_df\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"append\")\n",
    " .option(\"mergeSchema\", \"true\")  # This allows schema evolution\n",
    " .save(delta_table_path))\n",
    "\n",
    "print(\"✅ Schema evolution successful!\")\n",
    "\n",
    "# Verify the evolved schema\n",
    "evolved_table = spark.read.format(\"delta\").load(delta_table_path)\n",
    "print(\"\\nEvolved table schema:\")\n",
    "evolved_table.printSchema()\n",
    "\n",
    "print(\"\\nTable contents after evolution:\")\n",
    "evolved_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing CHECK Constraints\n",
    "\n",
    "Delta Lake supports CHECK constraints to enforce business rules at the table level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Implementing CHECK Constraints ===\")\n",
    "\n",
    "# Create a new table with constraints for demonstration\n",
    "constrained_table_path = \"/tmp/delta_customers_constrained\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.rm(constrained_table_path, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create table using SQL for easier constraint definition\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE delta.`{constrained_table_path}` (\n",
    "    customer_id INT NOT NULL,\n",
    "    name STRING NOT NULL,\n",
    "    email STRING NOT NULL,\n",
    "    age INT,\n",
    "    tier STRING,\n",
    "    account_balance DOUBLE,\n",
    "    created_date DATE\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table created with basic schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CHECK constraints\n",
    "print(\"\\nAdding CHECK constraints:\")\n",
    "\n",
    "# 1. Age constraint\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{constrained_table_path}`\n",
    "ADD CONSTRAINT age_check CHECK (age >= 18 AND age <= 120)\n",
    "\"\"\")\n",
    "print(\"✅ Age constraint added: age >= 18 AND age <= 120\")\n",
    "\n",
    "# 2. Account balance constraint\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{constrained_table_path}`\n",
    "ADD CONSTRAINT balance_check CHECK (account_balance >= 0)\n",
    "\"\"\")\n",
    "print(\"✅ Balance constraint added: account_balance >= 0\")\n",
    "\n",
    "# 3. Tier validation constraint\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{constrained_table_path}`\n",
    "ADD CONSTRAINT tier_check CHECK (tier IN ('Standard', 'Premium', 'Gold'))\n",
    "\"\"\")\n",
    "print(\"✅ Tier constraint added: tier IN ('Standard', 'Premium', 'Gold')\")\n",
    "\n",
    "# 4. Email format constraint (basic)\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{constrained_table_path}`\n",
    "ADD CONSTRAINT email_format_check CHECK (email LIKE '%@%')\n",
    "\"\"\")\n",
    "print(\"✅ Email format constraint added: email LIKE '%@%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table constraints\n",
    "print(\"\\n=== Current Table Constraints ===\")\n",
    "constraints_df = spark.sql(f\"DESCRIBE TABLE EXTENDED delta.`{constrained_table_path}`\")\n",
    "constraints_df.filter(F.col(\"col_name\").contains(\"Constraint\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Constraint Enforcement\n",
    "\n",
    "Let's test how constraints prevent invalid data from entering the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing Constraint Enforcement ===\")\n",
    "\n",
    "# 1. Valid data (should succeed)\n",
    "print(\"\\n1. Inserting valid data:\")\n",
    "\n",
    "valid_data = [\n",
    "    (1, \"John Doe\", \"john@email.com\", 25, \"Standard\", 500.00, \"2023-01-15\"),\n",
    "    (2, \"Jane Smith\", \"jane@email.com\", 35, \"Premium\", 1500.00, \"2023-01-16\")\n",
    "]\n",
    "\n",
    "valid_df = spark.createDataFrame(valid_data, customer_schema)\n",
    "valid_df = valid_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (valid_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(constrained_table_path))\n",
    "    print(\"✅ Valid data inserted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected failure: {e}\")\n",
    "\n",
    "# Verify insertion\n",
    "result_df = spark.read.format(\"delta\").load(constrained_table_path)\n",
    "print(f\"Current record count: {result_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Invalid age (should fail)\n",
    "print(\"\\n2. Testing age constraint violation:\")\n",
    "\n",
    "invalid_age_data = [\n",
    "    (3, \"Too Young\", \"young@email.com\", 15, \"Standard\", 100.00, \"2023-01-17\")  # Age < 18\n",
    "]\n",
    "\n",
    "invalid_age_df = spark.createDataFrame(invalid_age_data, customer_schema)\n",
    "invalid_age_df = invalid_age_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (invalid_age_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(constrained_table_path))\n",
    "    print(\"❌ Unexpected: Invalid age data was inserted\")\n",
    "except Exception as e:\n",
    "    print(f\"✅ Expected: Age constraint prevented insertion - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Invalid balance (should fail)\n",
    "print(\"\\n3. Testing balance constraint violation:\")\n",
    "\n",
    "invalid_balance_data = [\n",
    "    (4, \"Negative Balance\", \"negative@email.com\", 30, \"Standard\", -100.00, \"2023-01-18\")\n",
    "]\n",
    "\n",
    "invalid_balance_df = spark.createDataFrame(invalid_balance_data, customer_schema)\n",
    "invalid_balance_df = invalid_balance_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (invalid_balance_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(constrained_table_path))\n",
    "    print(\"❌ Unexpected: Negative balance data was inserted\")\n",
    "except Exception as e:\n",
    "    print(f\"✅ Expected: Balance constraint prevented insertion - {str(e)[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Invalid tier (should fail)\n",
    "print(\"\\n4. Testing tier constraint violation:\")\n",
    "\n",
    "invalid_tier_data = [\n",
    "    (5, \"Invalid Tier\", \"invalid@email.com\", 25, \"Platinum\", 2000.00, \"2023-01-19\")  # Invalid tier\n",
    "]\n",
    "\n",
    "invalid_tier_df = spark.createDataFrame(invalid_tier_data, customer_schema)\n",
    "invalid_tier_df = invalid_tier_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (invalid_tier_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(constrained_table_path))\n",
    "    print(\"❌ Unexpected: Invalid tier data was inserted\")\n",
    "except Exception as e:\n",
    "    print(f\"✅ Expected: Tier constraint prevented insertion - {str(e)[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Constraint Patterns\n",
    "\n",
    "Let's explore more sophisticated constraint patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Constraint Patterns ===\")\n",
    "\n",
    "# Create a more complex table for advanced constraints\n",
    "advanced_table_path = \"/tmp/delta_orders_advanced\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.rm(advanced_table_path, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create orders table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE delta.`{advanced_table_path}` (\n",
    "    order_id STRING NOT NULL,\n",
    "    customer_id INT NOT NULL,\n",
    "    order_date DATE NOT NULL,\n",
    "    ship_date DATE,\n",
    "    order_amount DECIMAL(10,2) NOT NULL,\n",
    "    discount_percent DECIMAL(5,2),\n",
    "    status STRING NOT NULL,\n",
    "    priority STRING,\n",
    "    created_timestamp TIMESTAMP NOT NULL\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"Advanced orders table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sophisticated constraints\n",
    "print(\"\\nAdding advanced constraints:\")\n",
    "\n",
    "# 1. Date logic constraint\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{advanced_table_path}`\n",
    "ADD CONSTRAINT date_logic_check \n",
    "CHECK (ship_date IS NULL OR ship_date >= order_date)\n",
    "\"\"\")\n",
    "print(\"✅ Date logic constraint: ship_date >= order_date\")\n",
    "\n",
    "# 2. Amount and discount relationship\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{advanced_table_path}`\n",
    "ADD CONSTRAINT discount_logic_check \n",
    "CHECK (discount_percent IS NULL OR (discount_percent >= 0 AND discount_percent <= 50))\n",
    "\"\"\")\n",
    "print(\"✅ Discount constraint: 0 <= discount_percent <= 50\")\n",
    "\n",
    "# 3. Status and priority relationship\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{advanced_table_path}`\n",
    "ADD CONSTRAINT status_check \n",
    "CHECK (status IN ('Pending', 'Processing', 'Shipped', 'Delivered', 'Cancelled'))\n",
    "\"\"\")\n",
    "print(\"✅ Status constraint: Valid status values\")\n",
    "\n",
    "# 4. Order ID format constraint\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{advanced_table_path}`\n",
    "ADD CONSTRAINT order_id_format_check \n",
    "CHECK (order_id RLIKE '^ORD-[0-9]{{8}}$')\n",
    "\"\"\")\n",
    "print(\"✅ Order ID format constraint: ORD-########\")\n",
    "\n",
    "# 5. Minimum order amount\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{advanced_table_path}`\n",
    "ADD CONSTRAINT minimum_order_check \n",
    "CHECK (order_amount >= 0.01)\n",
    "\"\"\")\n",
    "print(\"✅ Minimum order amount constraint: >= $0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced constraints\n",
    "print(\"\\n=== Testing Advanced Constraints ===\")\n",
    "\n",
    "# Valid order data\n",
    "valid_orders = [\n",
    "    (\"ORD-00000001\", 1, \"2023-01-15\", \"2023-01-18\", 125.99, 5.0, \"Shipped\", \"High\", \"2023-01-15 10:30:00\"),\n",
    "    (\"ORD-00000002\", 2, \"2023-01-16\", None, 89.50, None, \"Processing\", \"Normal\", \"2023-01-16 14:20:00\")\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", StringType(), False),\n",
    "    StructField(\"ship_date\", StringType(), True),\n",
    "    StructField(\"order_amount\", DoubleType(), False),\n",
    "    StructField(\"discount_percent\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"priority\", StringType(), True),\n",
    "    StructField(\"created_timestamp\", StringType(), False)\n",
    "])\n",
    "\n",
    "valid_orders_df = spark.createDataFrame(valid_orders, orders_schema)\n",
    "valid_orders_df = (valid_orders_df\n",
    "                  .withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "                  .withColumn(\"ship_date\", F.to_date(\"ship_date\"))\n",
    "                  .withColumn(\"created_timestamp\", F.to_timestamp(\"created_timestamp\")))\n",
    "\n",
    "# Insert valid data\n",
    "try:\n",
    "    (valid_orders_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(advanced_table_path))\n",
    "    print(\"✅ Valid orders inserted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected failure: {e}\")\n",
    "\n",
    "# Show current data\n",
    "current_orders = spark.read.format(\"delta\").load(advanced_table_path)\n",
    "print(\"\\nCurrent orders:\")\n",
    "current_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test constraint violations\n",
    "print(\"\\n=== Testing Constraint Violations ===\")\n",
    "\n",
    "# 1. Invalid order ID format\n",
    "print(\"\\n1. Testing invalid order ID format:\")\n",
    "invalid_id_orders = [\n",
    "    (\"INVALID-ID\", 3, \"2023-01-17\", None, 50.00, None, \"Pending\", \"Low\", \"2023-01-17 09:00:00\")\n",
    "]\n",
    "\n",
    "invalid_id_df = spark.createDataFrame(invalid_id_orders, orders_schema)\n",
    "invalid_id_df = (invalid_id_df\n",
    "                .withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "                .withColumn(\"ship_date\", F.to_date(\"ship_date\"))\n",
    "                .withColumn(\"created_timestamp\", F.to_timestamp(\"created_timestamp\")))\n",
    "\n",
    "try:\n",
    "    (invalid_id_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(advanced_table_path))\n",
    "    print(\"❌ Unexpected: Invalid order ID was accepted\")\n",
    "except Exception as e:\n",
    "    print(\"✅ Expected: Order ID format constraint prevented insertion\")\n",
    "\n",
    "# 2. Invalid date logic (ship_date < order_date)\n",
    "print(\"\\n2. Testing invalid date logic:\")\n",
    "invalid_date_orders = [\n",
    "    (\"ORD-00000003\", 4, \"2023-01-20\", \"2023-01-18\", 75.00, None, \"Shipped\", \"Normal\", \"2023-01-20 11:00:00\")\n",
    "]\n",
    "\n",
    "invalid_date_df = spark.createDataFrame(invalid_date_orders, orders_schema)\n",
    "invalid_date_df = (invalid_date_df\n",
    "                  .withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "                  .withColumn(\"ship_date\", F.to_date(\"ship_date\"))\n",
    "                  .withColumn(\"created_timestamp\", F.to_timestamp(\"created_timestamp\")))\n",
    "\n",
    "try:\n",
    "    (invalid_date_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(advanced_table_path))\n",
    "    print(\"❌ Unexpected: Invalid date logic was accepted\")\n",
    "except Exception as e:\n",
    "    print(\"✅ Expected: Date logic constraint prevented insertion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Declarative Data Quality Functions\n",
    "\n",
    "Let's create reusable functions that leverage Delta Lake's constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Declarative Data Quality Functions ===\")\n",
    "\n",
    "class DeltaTableManager:\n",
    "    \"\"\"\n",
    "    Utility class for managing Delta tables with constraints\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_customer_table(table_path, constraints=True):\n",
    "        \"\"\"\n",
    "        Create a customer table with optional constraints\n",
    "        \"\"\"\n",
    "        # Create base table\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS delta.`{table_path}` (\n",
    "            customer_id INT NOT NULL,\n",
    "            name STRING NOT NULL,\n",
    "            email STRING NOT NULL,\n",
    "            age INT,\n",
    "            tier STRING,\n",
    "            account_balance DOUBLE,\n",
    "            created_date DATE,\n",
    "            city STRING,\n",
    "            department STRING\n",
    "        ) USING DELTA\n",
    "        \"\"\")\n",
    "        \n",
    "        if constraints:\n",
    "            DeltaTableManager.add_customer_constraints(table_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_customer_constraints(table_path):\n",
    "        \"\"\"\n",
    "        Add standard customer constraints to a Delta table\n",
    "        \"\"\"\n",
    "        constraints = [\n",
    "            (\"age_check\", \"age >= 18 AND age <= 120\"),\n",
    "            (\"balance_check\", \"account_balance >= 0\"),\n",
    "            (\"tier_check\", \"tier IN ('Standard', 'Premium', 'Gold')\"),\n",
    "            (\"email_format_check\", \"email LIKE '%@%'\")\n",
    "        ]\n",
    "        \n",
    "        for constraint_name, constraint_expr in constraints:\n",
    "            try:\n",
    "                spark.sql(f\"\"\"\n",
    "                ALTER TABLE delta.`{table_path}`\n",
    "                ADD CONSTRAINT {constraint_name} CHECK ({constraint_expr})\n",
    "                \"\"\")\n",
    "                print(f\"✅ Added constraint: {constraint_name}\")\n",
    "            except Exception as e:\n",
    "                if \"already exists\" in str(e).lower():\n",
    "                    print(f\"⚠️  Constraint {constraint_name} already exists\")\n",
    "                else:\n",
    "                    print(f\"❌ Failed to add constraint {constraint_name}: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_before_write(df, table_path):\n",
    "        \"\"\"\n",
    "        Validate DataFrame against table constraints before writing\n",
    "        This is a proactive validation approach\n",
    "        \"\"\"\n",
    "        print(\"=== Pre-write Validation ===\")\n",
    "        \n",
    "        validation_results = {}\n",
    "        \n",
    "        # Age validation\n",
    "        if \"age\" in df.columns:\n",
    "            invalid_age_count = df.filter((F.col(\"age\") < 18) | (F.col(\"age\") > 120)).count()\n",
    "            validation_results[\"age_check\"] = invalid_age_count == 0\n",
    "            if invalid_age_count > 0:\n",
    "                print(f\"❌ Age validation failed: {invalid_age_count} records with invalid age\")\n",
    "        \n",
    "        # Balance validation\n",
    "        if \"account_balance\" in df.columns:\n",
    "            invalid_balance_count = df.filter(F.col(\"account_balance\") < 0).count()\n",
    "            validation_results[\"balance_check\"] = invalid_balance_count == 0\n",
    "            if invalid_balance_count > 0:\n",
    "                print(f\"❌ Balance validation failed: {invalid_balance_count} records with negative balance\")\n",
    "        \n",
    "        # Tier validation\n",
    "        if \"tier\" in df.columns:\n",
    "            valid_tiers = ['Standard', 'Premium', 'Gold']\n",
    "            invalid_tier_count = df.filter(~F.col(\"tier\").isin(valid_tiers)).count()\n",
    "            validation_results[\"tier_check\"] = invalid_tier_count == 0\n",
    "            if invalid_tier_count > 0:\n",
    "                print(f\"❌ Tier validation failed: {invalid_tier_count} records with invalid tier\")\n",
    "        \n",
    "        # Email validation\n",
    "        if \"email\" in df.columns:\n",
    "            invalid_email_count = df.filter(~F.col(\"email\").contains(\"@\")).count()\n",
    "            validation_results[\"email_format_check\"] = invalid_email_count == 0\n",
    "            if invalid_email_count > 0:\n",
    "                print(f\"❌ Email validation failed: {invalid_email_count} records with invalid email\")\n",
    "        \n",
    "        all_passed = all(validation_results.values())\n",
    "        \n",
    "        if all_passed:\n",
    "            print(\"✅ All validations passed\")\n",
    "        else:\n",
    "            print(\"❌ Some validations failed\")\n",
    "        \n",
    "        return all_passed, validation_results\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_write_to_delta(df, table_path, mode=\"append\", validate=True):\n",
    "        \"\"\"\n",
    "        Safely write DataFrame to Delta table with validation\n",
    "        \"\"\"\n",
    "        if validate:\n",
    "            is_valid, validation_results = DeltaTableManager.validate_before_write(df, table_path)\n",
    "            if not is_valid:\n",
    "                raise ValueError(\"Data validation failed. Cannot write to Delta table.\")\n",
    "        \n",
    "        try:\n",
    "            (df\n",
    "             .write\n",
    "             .format(\"delta\")\n",
    "             .mode(mode)\n",
    "             .save(table_path))\n",
    "            print(f\"✅ Successfully wrote {df.count()} records to {table_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Write failed: {e}\")\n",
    "            raise\n",
    "\n",
    "print(\"DeltaTableManager utility class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the declarative data quality functions\n",
    "print(\"=== Testing Declarative Data Quality ===\")\n",
    "\n",
    "managed_table_path = \"/tmp/delta_customers_managed\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.rm(managed_table_path, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create managed table with constraints\n",
    "DeltaTableManager.create_customer_table(managed_table_path, constraints=True)\n",
    "\n",
    "# Test with valid data\n",
    "print(\"\\n1. Testing with valid data:\")\n",
    "valid_customers = [\n",
    "    (1, \"Alice Johnson\", \"alice@email.com\", 28, \"Premium\", 1200.50, \"2023-01-15\", \"New York\", \"Engineering\"),\n",
    "    (2, \"Bob Smith\", \"bob@email.com\", 35, \"Gold\", 2500.00, \"2023-01-16\", \"California\", \"Sales\")\n",
    "]\n",
    "\n",
    "valid_df = spark.createDataFrame(valid_customers, \n",
    "                                [\"customer_id\", \"name\", \"email\", \"age\", \"tier\", \n",
    "                                 \"account_balance\", \"created_date\", \"city\", \"department\"])\n",
    "valid_df = valid_df.withColumn(\"created_date\", F.to_date(\"created_date\"))\n",
    "\n",
    "DeltaTableManager.safe_write_to_delta(valid_df, managed_table_path)\n",
    "\n",
    "# Test with invalid data (pre-validation should catch this)\n",
    "print(\"\\n2. Testing with invalid data:\")\n",
    "invalid_customers = [\n",
    "    (3, \"Too Young\", \"young@email.com\", 15, \"Standard\", 100.00, \"2023-01-17\", \"Texas\", \"Support\"),  # Invalid age\n",
    "    (4, \"Negative Balance\", \"negative@email.com\", 30, \"Premium\", -500.00, \"2023-01-18\", \"Florida\", \"Marketing\")  # Invalid balance\n",
    "]\n",
    "\n",
    "invalid_df = spark.createDataFrame(invalid_customers,\n",
    "                                  [\"customer_id\", \"name\", \"email\", \"age\", \"tier\", \n",
    "                                   \"account_balance\", \"created_date\", \"city\", \"department\"])\n",
    "invalid_df = invalid_df.withColumn(\"created_date\", F.to_date(\"created_date\"))\n",
    "\n",
    "try:\n",
    "    DeltaTableManager.safe_write_to_delta(invalid_df, managed_table_path)\n",
    "except ValueError as e:\n",
    "    print(f\"✅ Expected: Pre-validation prevented write - {e}\")\n",
    "\n",
    "# Show final table contents\n",
    "final_table = spark.read.format(\"delta\").load(managed_table_path)\n",
    "print(f\"\\nFinal table record count: {final_table.count()}\")\n",
    "final_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Schema Enforcement Benefits**:\n",
    "   - Prevents data corruption at write time\n",
    "   - Ensures data consistency across the lakehouse\n",
    "   - Eliminates need for application-level schema validation\n",
    "\n",
    "2. **Delta Lake Constraints**:\n",
    "   - CHECK constraints enforce business rules declaratively\n",
    "   - Constraints are evaluated at write time\n",
    "   - Support complex expressions and relationships\n",
    "\n",
    "3. **Schema Evolution**:\n",
    "   - Use `mergeSchema=true` for safe schema evolution\n",
    "   - Add optional columns without breaking existing queries\n",
    "   - Maintain backward compatibility\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Define constraints early in table lifecycle\n",
    "   - Use descriptive constraint names\n",
    "   - Implement pre-write validation for better error messages\n",
    "   - Build reusable constraint management utilities\n",
    "\n",
    "5. **Functional Programming Alignment**:\n",
    "   - Constraints are declarative (what, not how)\n",
    "   - Immutable data guarantees with schema enforcement\n",
    "   - Composable validation functions\n",
    "   - Pure functions for constraint checking\n",
    "\n",
    "**Benefits of Declarative Data Quality**:\n",
    "- Centralized data quality rules\n",
    "- Automatic enforcement without application logic\n",
    "- Clear error messages for quality violations\n",
    "- Consistent quality across all write paths\n",
    "- Self-documenting data contracts\n",
    "\n",
    "**Next Steps**: In the next notebook, we'll explore Delta Live Tables (DLT) for even more advanced declarative data quality patterns and automated pipeline management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Practice implementing schema enforcement and constraints:\n",
    "\n",
    "1. Create a Delta table for your domain with appropriate constraints\n",
    "2. Test constraint violations with realistic bad data\n",
    "3. Implement safe schema evolution by adding new optional columns\n",
    "4. Build a validation utility class for your specific use case\n",
    "5. Create a complete data quality pipeline with pre-write validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "\n",
    "# 1. Create your domain-specific Delta table\n",
    "def create_your_table(table_path):\n",
    "    \"\"\"\n",
    "    Create a Delta table for your specific domain\n",
    "    \"\"\"\n",
    "    # Your table creation logic\n",
    "    pass\n",
    "\n",
    "# 2. Define domain-specific constraints\n",
    "def add_your_constraints(table_path):\n",
    "    \"\"\"\n",
    "    Add constraints relevant to your domain\n",
    "    \"\"\"\n",
    "    # Your constraint logic\n",
    "    pass\n",
    "\n",
    "# 3. Create validation functions\n",
    "def validate_your_data(df):\n",
    "    \"\"\"\n",
    "    Pre-write validation for your data\n",
    "    \"\"\"\n",
    "    # Your validation logic\n",
    "    pass\n",
    "\n",
    "# 4. Test with sample data\n",
    "# your_test_data = [...]\n",
    "# test_df = spark.createDataFrame(your_test_data, your_schema)\n",
    "# validate_your_data(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygtml_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}