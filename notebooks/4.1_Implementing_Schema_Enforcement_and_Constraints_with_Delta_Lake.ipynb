{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Implementing Schema Enforcement and Constraints with Delta Lake\n",
    "\n",
    "This notebook demonstrates how to implement robust data quality controls using Delta Lake's schema enforcement and constraint capabilities.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Delta Lake's schema enforcement mechanisms\n",
    "- Implement CHECK constraints for data validation\n",
    "- Handle schema evolution safely\n",
    "- Use Delta Lake features for data integrity\n",
    "- Build declarative data quality pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Delta Lake Schema Enforcement\n",
    "\n",
    "Delta Lake provides strong schema enforcement that prevents data corruption by ensuring all writes conform to the table's schema. This is a cornerstone of data reliability in the lakehouse architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/04 21:11:07 WARN Utils: Your hostname, Tarski.local resolves to a loopback address: 127.0.0.1; using 10.0.0.16 instead (on interface en0)\n",
      "25/11/04 21:11:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/04 21:11:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/04 21:11:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/04 21:11:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/11/04 21:11:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created local SparkSession\n",
      "   Note: Delta Lake disabled by default for compatibility\n",
      "   To enable: Install matching delta-spark version and set enable_delta=True\n",
      "‚úì Using mock dbutils for local development\n",
      "============================================================\n",
      "LOCAL ENVIRONMENT SETUP COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìç Platform: Local Python\n",
      "\n",
      "‚ö° Spark Configuration:\n",
      "   Version: 3.5.2\n",
      "   App Name: PySpark_Best_Practices\n",
      "   Delta Lake: ‚úì Available\n",
      "\n",
      "üêç Python Version: 3.13.4\n",
      "\n",
      "üîß Available Utilities:\n",
      "   - get_storage_path(): Get platform-aware storage paths\n",
      "   - get_spark_session(): Create configured Spark session\n",
      "   - dbutils: Mock implementation for local development\n",
      "\n",
      "üìö Common Imports:\n",
      "   - pyspark.sql.functions as F\n",
      "   - pyspark.sql.types (all types)\n",
      "\n",
      "============================================================\n",
      "Ready to run PySpark code!\n",
      "============================================================\n",
      "Sample DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+-----------+\n",
      "|   name|age|age_plus_10|\n",
      "+-------+---+-----------+\n",
      "|  Alice| 25|         35|\n",
      "|    Bob| 30|         40|\n",
      "|Charlie| 35|         45|\n",
      "+-------+---+-----------+\n",
      "\n",
      "Storage path for 'my_table': /Users/dw31/Projects/pyspark_best_practices/notebooks/data/delta/my_table\n",
      "Customer data created:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/04 21:11:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o70.showString.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:295)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:295)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:275)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4322)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 65 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m customers_df = spark.createDataFrame(customer_data, customer_schema)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCustomer data created:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mcustomers_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m customers_df.printSchema()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deltalake/lib/python3.13/site-packages/pyspark/sql/dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deltalake/lib/python3.13/site-packages/pyspark/sql/dataframe.py:965\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    960\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    962\u001b[39m     )\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deltalake/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deltalake/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deltalake/lib/python3.13/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o70.showString.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:295)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:295)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:275)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4322)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 65 more\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "# FOR LOCAL DEVELOPMENT: Uncomment the line below to run the setup notebook\n",
    "%run 00_Environment_Setup.ipynb\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from delta import *\n",
    "\n",
    "# Configure Spark for Delta Lake\n",
    "# Note: In Databricks, this is pre-configured\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DeltaLakeSchemaEnforcement\")\n",
    "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Sample customer data with various data quality issues\n",
    "customer_data = [\n",
    "    (1, \"John Doe\", \"john.doe@email.com\", 28, \"Premium\", 1200.50, \"2023-01-15\"),\n",
    "    (2, \"Jane Smith\", \"jane.smith@email.com\", 35, \"Standard\", 850.75, \"2023-01-16\"),\n",
    "    (3, \"Bob Johnson\", \"bob@company.com\", 42, \"Premium\", 2100.00, \"2023-01-17\"),\n",
    "    (4, \"Alice Brown\", \"alice.brown@service.org\", 29, \"Gold\", 1500.25, \"2023-01-18\"),\n",
    "    (5, \"Charlie Wilson\", \"charlie@email.com\", 33, \"Standard\", 750.00, \"2023-01-19\")\n",
    "]\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),  # NOT NULL\n",
    "    StructField(\"name\", StringType(), False),          # NOT NULL\n",
    "    StructField(\"email\", StringType(), False),         # NOT NULL\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"account_balance\", DoubleType(), True),\n",
    "    StructField(\"created_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "customers_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "\n",
    "print(\"Customer data created:\")\n",
    "customers_df.show()\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Delta Tables with Schema Enforcement\n",
    "\n",
    "Let's create a Delta table and demonstrate basic schema enforcement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delta table with initial schema\n",
    "delta_table_path = \"/tmp/delta_customers\"\n",
    "\n",
    "# Clear existing data for demo\n",
    "try:\n",
    "    dbutils.fs.rm(delta_table_path, True)\n",
    "except:\n",
    "    pass  # Path doesn't exist yet\n",
    "\n",
    "print(\"=== Creating Initial Delta Table ===\")\n",
    "\n",
    "# Write initial data to create the Delta table\n",
    "(customers_df\n",
    " .withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .save(delta_table_path))\n",
    "\n",
    "print(f\"Delta table created at: {delta_table_path}\")\n",
    "\n",
    "# Read the table back to verify\n",
    "delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "print(\"\\nTable contents:\")\n",
    "delta_df.show()\n",
    "print(\"\\nTable schema:\")\n",
    "delta_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Schema Enforcement\n",
    "\n",
    "Now let's see how Delta Lake prevents schema violations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Schema Enforcement Demonstrations ===\")\n",
    "\n",
    "# 1. Adding extra columns (will fail without schema evolution)\n",
    "print(\"\\n1. Testing extra column addition:\")\n",
    "\n",
    "bad_data_extra_col = [\n",
    "    (6, \"David Lee\", \"david@email.com\", 31, \"Premium\", 900.00, \"2023-01-20\", \"New York\")  # Extra column\n",
    "]\n",
    "\n",
    "bad_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"account_balance\", DoubleType(), True),\n",
    "    StructField(\"created_date\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True)  # Extra column!\n",
    "])\n",
    "\n",
    "bad_df = spark.createDataFrame(bad_data_extra_col, bad_schema)\n",
    "bad_df = bad_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (bad_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(delta_table_path))\n",
    "    print(\"‚ùå Unexpected: Write succeeded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Expected: Schema enforcement prevented write - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Wrong data types (will fail)\n",
    "print(\"\\n2. Testing wrong data types:\")\n",
    "\n",
    "bad_data_types = [\n",
    "    (\"seven\", \"Eve Green\", \"eve@email.com\", 25, \"Standard\", 600.00, \"2023-01-21\")  # String instead of Int\n",
    "]\n",
    "\n",
    "bad_type_df = spark.createDataFrame(bad_data_types, \n",
    "                                   [\"customer_id\", \"name\", \"email\", \"age\", \"tier\", \"account_balance\", \"created_date\"])\n",
    "bad_type_df = bad_type_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (bad_type_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(delta_table_path))\n",
    "    print(\"‚ùå Unexpected: Write succeeded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Expected: Type mismatch prevented write - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Missing required columns (will fail)\n",
    "print(\"\\n3. Testing missing required columns:\")\n",
    "\n",
    "incomplete_data = [\n",
    "    (7, \"Frank Miller\", 40, \"Premium\", 1100.00)  # Missing email column\n",
    "]\n",
    "\n",
    "incomplete_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    # Missing email field!\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"account_balance\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "incomplete_df = spark.createDataFrame(incomplete_data, incomplete_schema)\n",
    "\n",
    "try:\n",
    "    (incomplete_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(delta_table_path))\n",
    "    print(\"‚ùå Unexpected: Write succeeded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Expected: Missing column prevented write - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe Schema Evolution\n",
    "\n",
    "When you need to evolve schemas, Delta Lake provides safe mechanisms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Safe Schema Evolution ===\")\n",
    "\n",
    "# 1. Adding optional columns with mergeSchema option\n",
    "print(\"\\n1. Adding new optional columns:\")\n",
    "\n",
    "new_customer_data = [\n",
    "    (6, \"David Lee\", \"david@email.com\", 31, \"Premium\", 900.00, \"2023-01-20\", \"New York\", \"Engineering\")\n",
    "]\n",
    "\n",
    "evolved_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"account_balance\", DoubleType(), True),\n",
    "    StructField(\"created_date\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),        # New optional column\n",
    "    StructField(\"department\", StringType(), True)   # New optional column\n",
    "])\n",
    "\n",
    "evolved_df = spark.createDataFrame(new_customer_data, evolved_schema)\n",
    "evolved_df = evolved_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Enable schema merging\n",
    "(evolved_df\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"append\")\n",
    " .option(\"mergeSchema\", \"true\")  # This allows schema evolution\n",
    " .save(delta_table_path))\n",
    "\n",
    "print(\"‚úÖ Schema evolution successful!\")\n",
    "\n",
    "# Verify the evolved schema\n",
    "evolved_table = spark.read.format(\"delta\").load(delta_table_path)\n",
    "print(\"\\nEvolved table schema:\")\n",
    "evolved_table.printSchema()\n",
    "\n",
    "print(\"\\nTable contents after evolution:\")\n",
    "evolved_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing CHECK Constraints\n",
    "\n",
    "Delta Lake supports CHECK constraints to enforce business rules at the table level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Implementing CHECK Constraints ===\")\n",
    "\n",
    "# Create a new table with constraints for demonstration\n",
    "constrained_table_path = \"/tmp/delta_customers_constrained\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.rm(constrained_table_path, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create table using SQL for easier constraint definition\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE delta.`{constrained_table_path}` (\n",
    "    customer_id INT NOT NULL,\n",
    "    name STRING NOT NULL,\n",
    "    email STRING NOT NULL,\n",
    "    age INT,\n",
    "    tier STRING,\n",
    "    account_balance DOUBLE,\n",
    "    created_date DATE\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table created with basic schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CHECK constraints\n",
    "print(\"\\nAdding CHECK constraints:\")\n",
    "\n",
    "# 1. Age constraint\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{constrained_table_path}`\n",
    "ADD CONSTRAINT age_check CHECK (age >= 18 AND age <= 120)\n",
    "\"\"\")\n",
    "print(\"‚úÖ Age constraint added: age >= 18 AND age <= 120\")\n",
    "\n",
    "# 2. Account balance constraint\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{constrained_table_path}`\n",
    "ADD CONSTRAINT balance_check CHECK (account_balance >= 0)\n",
    "\"\"\")\n",
    "print(\"‚úÖ Balance constraint added: account_balance >= 0\")\n",
    "\n",
    "# 3. Tier validation constraint\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{constrained_table_path}`\n",
    "ADD CONSTRAINT tier_check CHECK (tier IN ('Standard', 'Premium', 'Gold'))\n",
    "\"\"\")\n",
    "print(\"‚úÖ Tier constraint added: tier IN ('Standard', 'Premium', 'Gold')\")\n",
    "\n",
    "# 4. Email format constraint (basic)\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{constrained_table_path}`\n",
    "ADD CONSTRAINT email_format_check CHECK (email LIKE '%@%')\n",
    "\"\"\")\n",
    "print(\"‚úÖ Email format constraint added: email LIKE '%@%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table constraints\n",
    "print(\"\\n=== Current Table Constraints ===\")\n",
    "constraints_df = spark.sql(f\"DESCRIBE TABLE EXTENDED delta.`{constrained_table_path}`\")\n",
    "constraints_df.filter(F.col(\"col_name\").contains(\"Constraint\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Constraint Enforcement\n",
    "\n",
    "Let's test how constraints prevent invalid data from entering the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing Constraint Enforcement ===\")\n",
    "\n",
    "# 1. Valid data (should succeed)\n",
    "print(\"\\n1. Inserting valid data:\")\n",
    "\n",
    "valid_data = [\n",
    "    (1, \"John Doe\", \"john@email.com\", 25, \"Standard\", 500.00, \"2023-01-15\"),\n",
    "    (2, \"Jane Smith\", \"jane@email.com\", 35, \"Premium\", 1500.00, \"2023-01-16\")\n",
    "]\n",
    "\n",
    "valid_df = spark.createDataFrame(valid_data, customer_schema)\n",
    "valid_df = valid_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (valid_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(constrained_table_path))\n",
    "    print(\"‚úÖ Valid data inserted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected failure: {e}\")\n",
    "\n",
    "# Verify insertion\n",
    "result_df = spark.read.format(\"delta\").load(constrained_table_path)\n",
    "print(f\"Current record count: {result_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Invalid age (should fail)\n",
    "print(\"\\n2. Testing age constraint violation:\")\n",
    "\n",
    "invalid_age_data = [\n",
    "    (3, \"Too Young\", \"young@email.com\", 15, \"Standard\", 100.00, \"2023-01-17\")  # Age < 18\n",
    "]\n",
    "\n",
    "invalid_age_df = spark.createDataFrame(invalid_age_data, customer_schema)\n",
    "invalid_age_df = invalid_age_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (invalid_age_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(constrained_table_path))\n",
    "    print(\"‚ùå Unexpected: Invalid age data was inserted\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Expected: Age constraint prevented insertion - {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Invalid balance (should fail)\n",
    "print(\"\\n3. Testing balance constraint violation:\")\n",
    "\n",
    "invalid_balance_data = [\n",
    "    (4, \"Negative Balance\", \"negative@email.com\", 30, \"Standard\", -100.00, \"2023-01-18\")\n",
    "]\n",
    "\n",
    "invalid_balance_df = spark.createDataFrame(invalid_balance_data, customer_schema)\n",
    "invalid_balance_df = invalid_balance_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (invalid_balance_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(constrained_table_path))\n",
    "    print(\"‚ùå Unexpected: Negative balance data was inserted\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Expected: Balance constraint prevented insertion - {str(e)[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Invalid tier (should fail)\n",
    "print(\"\\n4. Testing tier constraint violation:\")\n",
    "\n",
    "invalid_tier_data = [\n",
    "    (5, \"Invalid Tier\", \"invalid@email.com\", 25, \"Platinum\", 2000.00, \"2023-01-19\")  # Invalid tier\n",
    "]\n",
    "\n",
    "invalid_tier_df = spark.createDataFrame(invalid_tier_data, customer_schema)\n",
    "invalid_tier_df = invalid_tier_df.withColumn(\"created_date\", F.to_date(F.col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "try:\n",
    "    (invalid_tier_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(constrained_table_path))\n",
    "    print(\"‚ùå Unexpected: Invalid tier data was inserted\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Expected: Tier constraint prevented insertion - {str(e)[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Constraint Patterns\n",
    "\n",
    "Let's explore more sophisticated constraint patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Constraint Patterns ===\")\n",
    "\n",
    "# Create a more complex table for advanced constraints\n",
    "advanced_table_path = \"/tmp/delta_orders_advanced\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.rm(advanced_table_path, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create orders table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE delta.`{advanced_table_path}` (\n",
    "    order_id STRING NOT NULL,\n",
    "    customer_id INT NOT NULL,\n",
    "    order_date DATE NOT NULL,\n",
    "    ship_date DATE,\n",
    "    order_amount DECIMAL(10,2) NOT NULL,\n",
    "    discount_percent DECIMAL(5,2),\n",
    "    status STRING NOT NULL,\n",
    "    priority STRING,\n",
    "    created_timestamp TIMESTAMP NOT NULL\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"Advanced orders table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sophisticated constraints\n",
    "print(\"\\nAdding advanced constraints:\")\n",
    "\n",
    "# 1. Date logic constraint\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{advanced_table_path}`\n",
    "ADD CONSTRAINT date_logic_check \n",
    "CHECK (ship_date IS NULL OR ship_date >= order_date)\n",
    "\"\"\")\n",
    "print(\"‚úÖ Date logic constraint: ship_date >= order_date\")\n",
    "\n",
    "# 2. Amount and discount relationship\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{advanced_table_path}`\n",
    "ADD CONSTRAINT discount_logic_check \n",
    "CHECK (discount_percent IS NULL OR (discount_percent >= 0 AND discount_percent <= 50))\n",
    "\"\"\")\n",
    "print(\"‚úÖ Discount constraint: 0 <= discount_percent <= 50\")\n",
    "\n",
    "# 3. Status and priority relationship\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{advanced_table_path}`\n",
    "ADD CONSTRAINT status_check \n",
    "CHECK (status IN ('Pending', 'Processing', 'Shipped', 'Delivered', 'Cancelled'))\n",
    "\"\"\")\n",
    "print(\"‚úÖ Status constraint: Valid status values\")\n",
    "\n",
    "# 4. Order ID format constraint\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{advanced_table_path}`\n",
    "ADD CONSTRAINT order_id_format_check \n",
    "CHECK (order_id RLIKE '^ORD-[0-9]{{8}}$')\n",
    "\"\"\")\n",
    "print(\"‚úÖ Order ID format constraint: ORD-########\")\n",
    "\n",
    "# 5. Minimum order amount\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{advanced_table_path}`\n",
    "ADD CONSTRAINT minimum_order_check \n",
    "CHECK (order_amount >= 0.01)\n",
    "\"\"\")\n",
    "print(\"‚úÖ Minimum order amount constraint: >= $0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced constraints\n",
    "print(\"\\n=== Testing Advanced Constraints ===\")\n",
    "\n",
    "# Valid order data\n",
    "valid_orders = [\n",
    "    (\"ORD-00000001\", 1, \"2023-01-15\", \"2023-01-18\", 125.99, 5.0, \"Shipped\", \"High\", \"2023-01-15 10:30:00\"),\n",
    "    (\"ORD-00000002\", 2, \"2023-01-16\", None, 89.50, None, \"Processing\", \"Normal\", \"2023-01-16 14:20:00\")\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", StringType(), False),\n",
    "    StructField(\"ship_date\", StringType(), True),\n",
    "    StructField(\"order_amount\", DoubleType(), False),\n",
    "    StructField(\"discount_percent\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"priority\", StringType(), True),\n",
    "    StructField(\"created_timestamp\", StringType(), False)\n",
    "])\n",
    "\n",
    "valid_orders_df = spark.createDataFrame(valid_orders, orders_schema)\n",
    "valid_orders_df = (valid_orders_df\n",
    "                  .withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "                  .withColumn(\"ship_date\", F.to_date(\"ship_date\"))\n",
    "                  .withColumn(\"created_timestamp\", F.to_timestamp(\"created_timestamp\")))\n",
    "\n",
    "# Insert valid data\n",
    "try:\n",
    "    (valid_orders_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(advanced_table_path))\n",
    "    print(\"‚úÖ Valid orders inserted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected failure: {e}\")\n",
    "\n",
    "# Show current data\n",
    "current_orders = spark.read.format(\"delta\").load(advanced_table_path)\n",
    "print(\"\\nCurrent orders:\")\n",
    "current_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test constraint violations\n",
    "print(\"\\n=== Testing Constraint Violations ===\")\n",
    "\n",
    "# 1. Invalid order ID format\n",
    "print(\"\\n1. Testing invalid order ID format:\")\n",
    "invalid_id_orders = [\n",
    "    (\"INVALID-ID\", 3, \"2023-01-17\", None, 50.00, None, \"Pending\", \"Low\", \"2023-01-17 09:00:00\")\n",
    "]\n",
    "\n",
    "invalid_id_df = spark.createDataFrame(invalid_id_orders, orders_schema)\n",
    "invalid_id_df = (invalid_id_df\n",
    "                .withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "                .withColumn(\"ship_date\", F.to_date(\"ship_date\"))\n",
    "                .withColumn(\"created_timestamp\", F.to_timestamp(\"created_timestamp\")))\n",
    "\n",
    "try:\n",
    "    (invalid_id_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(advanced_table_path))\n",
    "    print(\"‚ùå Unexpected: Invalid order ID was accepted\")\n",
    "except Exception as e:\n",
    "    print(\"‚úÖ Expected: Order ID format constraint prevented insertion\")\n",
    "\n",
    "# 2. Invalid date logic (ship_date < order_date)\n",
    "print(\"\\n2. Testing invalid date logic:\")\n",
    "invalid_date_orders = [\n",
    "    (\"ORD-00000003\", 4, \"2023-01-20\", \"2023-01-18\", 75.00, None, \"Shipped\", \"Normal\", \"2023-01-20 11:00:00\")\n",
    "]\n",
    "\n",
    "invalid_date_df = spark.createDataFrame(invalid_date_orders, orders_schema)\n",
    "invalid_date_df = (invalid_date_df\n",
    "                  .withColumn(\"order_date\", F.to_date(\"order_date\"))\n",
    "                  .withColumn(\"ship_date\", F.to_date(\"ship_date\"))\n",
    "                  .withColumn(\"created_timestamp\", F.to_timestamp(\"created_timestamp\")))\n",
    "\n",
    "try:\n",
    "    (invalid_date_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"append\")\n",
    "     .save(advanced_table_path))\n",
    "    print(\"‚ùå Unexpected: Invalid date logic was accepted\")\n",
    "except Exception as e:\n",
    "    print(\"‚úÖ Expected: Date logic constraint prevented insertion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Declarative Data Quality Functions\n",
    "\n",
    "Let's create reusable functions that leverage Delta Lake's constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Declarative Data Quality Functions ===\")\n",
    "\n",
    "class DeltaTableManager:\n",
    "    \"\"\"\n",
    "    Utility class for managing Delta tables with constraints\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_customer_table(table_path, constraints=True):\n",
    "        \"\"\"\n",
    "        Create a customer table with optional constraints\n",
    "        \"\"\"\n",
    "        # Create base table\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS delta.`{table_path}` (\n",
    "            customer_id INT NOT NULL,\n",
    "            name STRING NOT NULL,\n",
    "            email STRING NOT NULL,\n",
    "            age INT,\n",
    "            tier STRING,\n",
    "            account_balance DOUBLE,\n",
    "            created_date DATE,\n",
    "            city STRING,\n",
    "            department STRING\n",
    "        ) USING DELTA\n",
    "        \"\"\")\n",
    "        \n",
    "        if constraints:\n",
    "            DeltaTableManager.add_customer_constraints(table_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_customer_constraints(table_path):\n",
    "        \"\"\"\n",
    "        Add standard customer constraints to a Delta table\n",
    "        \"\"\"\n",
    "        constraints = [\n",
    "            (\"age_check\", \"age >= 18 AND age <= 120\"),\n",
    "            (\"balance_check\", \"account_balance >= 0\"),\n",
    "            (\"tier_check\", \"tier IN ('Standard', 'Premium', 'Gold')\"),\n",
    "            (\"email_format_check\", \"email LIKE '%@%'\")\n",
    "        ]\n",
    "        \n",
    "        for constraint_name, constraint_expr in constraints:\n",
    "            try:\n",
    "                spark.sql(f\"\"\"\n",
    "                ALTER TABLE delta.`{table_path}`\n",
    "                ADD CONSTRAINT {constraint_name} CHECK ({constraint_expr})\n",
    "                \"\"\")\n",
    "                print(f\"‚úÖ Added constraint: {constraint_name}\")\n",
    "            except Exception as e:\n",
    "                if \"already exists\" in str(e).lower():\n",
    "                    print(f\"‚ö†Ô∏è  Constraint {constraint_name} already exists\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to add constraint {constraint_name}: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_before_write(df, table_path):\n",
    "        \"\"\"\n",
    "        Validate DataFrame against table constraints before writing\n",
    "        This is a proactive validation approach\n",
    "        \"\"\"\n",
    "        print(\"=== Pre-write Validation ===\")\n",
    "        \n",
    "        validation_results = {}\n",
    "        \n",
    "        # Age validation\n",
    "        if \"age\" in df.columns:\n",
    "            invalid_age_count = df.filter((F.col(\"age\") < 18) | (F.col(\"age\") > 120)).count()\n",
    "            validation_results[\"age_check\"] = invalid_age_count == 0\n",
    "            if invalid_age_count > 0:\n",
    "                print(f\"‚ùå Age validation failed: {invalid_age_count} records with invalid age\")\n",
    "        \n",
    "        # Balance validation\n",
    "        if \"account_balance\" in df.columns:\n",
    "            invalid_balance_count = df.filter(F.col(\"account_balance\") < 0).count()\n",
    "            validation_results[\"balance_check\"] = invalid_balance_count == 0\n",
    "            if invalid_balance_count > 0:\n",
    "                print(f\"‚ùå Balance validation failed: {invalid_balance_count} records with negative balance\")\n",
    "        \n",
    "        # Tier validation\n",
    "        if \"tier\" in df.columns:\n",
    "            valid_tiers = ['Standard', 'Premium', 'Gold']\n",
    "            invalid_tier_count = df.filter(~F.col(\"tier\").isin(valid_tiers)).count()\n",
    "            validation_results[\"tier_check\"] = invalid_tier_count == 0\n",
    "            if invalid_tier_count > 0:\n",
    "                print(f\"‚ùå Tier validation failed: {invalid_tier_count} records with invalid tier\")\n",
    "        \n",
    "        # Email validation\n",
    "        if \"email\" in df.columns:\n",
    "            invalid_email_count = df.filter(~F.col(\"email\").contains(\"@\")).count()\n",
    "            validation_results[\"email_format_check\"] = invalid_email_count == 0\n",
    "            if invalid_email_count > 0:\n",
    "                print(f\"‚ùå Email validation failed: {invalid_email_count} records with invalid email\")\n",
    "        \n",
    "        all_passed = all(validation_results.values())\n",
    "        \n",
    "        if all_passed:\n",
    "            print(\"‚úÖ All validations passed\")\n",
    "        else:\n",
    "            print(\"‚ùå Some validations failed\")\n",
    "        \n",
    "        return all_passed, validation_results\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_write_to_delta(df, table_path, mode=\"append\", validate=True):\n",
    "        \"\"\"\n",
    "        Safely write DataFrame to Delta table with validation\n",
    "        \"\"\"\n",
    "        if validate:\n",
    "            is_valid, validation_results = DeltaTableManager.validate_before_write(df, table_path)\n",
    "            if not is_valid:\n",
    "                raise ValueError(\"Data validation failed. Cannot write to Delta table.\")\n",
    "        \n",
    "        try:\n",
    "            (df\n",
    "             .write\n",
    "             .format(\"delta\")\n",
    "             .mode(mode)\n",
    "             .save(table_path))\n",
    "            print(f\"‚úÖ Successfully wrote {df.count()} records to {table_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Write failed: {e}\")\n",
    "            raise\n",
    "\n",
    "print(\"DeltaTableManager utility class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the declarative data quality functions\n",
    "print(\"=== Testing Declarative Data Quality ===\")\n",
    "\n",
    "managed_table_path = \"/tmp/delta_customers_managed\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.rm(managed_table_path, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create managed table with constraints\n",
    "DeltaTableManager.create_customer_table(managed_table_path, constraints=True)\n",
    "\n",
    "# Test with valid data\n",
    "print(\"\\n1. Testing with valid data:\")\n",
    "valid_customers = [\n",
    "    (1, \"Alice Johnson\", \"alice@email.com\", 28, \"Premium\", 1200.50, \"2023-01-15\", \"New York\", \"Engineering\"),\n",
    "    (2, \"Bob Smith\", \"bob@email.com\", 35, \"Gold\", 2500.00, \"2023-01-16\", \"California\", \"Sales\")\n",
    "]\n",
    "\n",
    "valid_df = spark.createDataFrame(valid_customers, \n",
    "                                [\"customer_id\", \"name\", \"email\", \"age\", \"tier\", \n",
    "                                 \"account_balance\", \"created_date\", \"city\", \"department\"])\n",
    "valid_df = valid_df.withColumn(\"created_date\", F.to_date(\"created_date\"))\n",
    "\n",
    "DeltaTableManager.safe_write_to_delta(valid_df, managed_table_path)\n",
    "\n",
    "# Test with invalid data (pre-validation should catch this)\n",
    "print(\"\\n2. Testing with invalid data:\")\n",
    "invalid_customers = [\n",
    "    (3, \"Too Young\", \"young@email.com\", 15, \"Standard\", 100.00, \"2023-01-17\", \"Texas\", \"Support\"),  # Invalid age\n",
    "    (4, \"Negative Balance\", \"negative@email.com\", 30, \"Premium\", -500.00, \"2023-01-18\", \"Florida\", \"Marketing\")  # Invalid balance\n",
    "]\n",
    "\n",
    "invalid_df = spark.createDataFrame(invalid_customers,\n",
    "                                  [\"customer_id\", \"name\", \"email\", \"age\", \"tier\", \n",
    "                                   \"account_balance\", \"created_date\", \"city\", \"department\"])\n",
    "invalid_df = invalid_df.withColumn(\"created_date\", F.to_date(\"created_date\"))\n",
    "\n",
    "try:\n",
    "    DeltaTableManager.safe_write_to_delta(invalid_df, managed_table_path)\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Expected: Pre-validation prevented write - {e}\")\n",
    "\n",
    "# Show final table contents\n",
    "final_table = spark.read.format(\"delta\").load(managed_table_path)\n",
    "print(f\"\\nFinal table record count: {final_table.count()}\")\n",
    "final_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Schema Enforcement Benefits**:\n",
    "   - Prevents data corruption at write time\n",
    "   - Ensures data consistency across the lakehouse\n",
    "   - Eliminates need for application-level schema validation\n",
    "\n",
    "2. **Delta Lake Constraints**:\n",
    "   - CHECK constraints enforce business rules declaratively\n",
    "   - Constraints are evaluated at write time\n",
    "   - Support complex expressions and relationships\n",
    "\n",
    "3. **Schema Evolution**:\n",
    "   - Use `mergeSchema=true` for safe schema evolution\n",
    "   - Add optional columns without breaking existing queries\n",
    "   - Maintain backward compatibility\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Define constraints early in table lifecycle\n",
    "   - Use descriptive constraint names\n",
    "   - Implement pre-write validation for better error messages\n",
    "   - Build reusable constraint management utilities\n",
    "\n",
    "5. **Functional Programming Alignment**:\n",
    "   - Constraints are declarative (what, not how)\n",
    "   - Immutable data guarantees with schema enforcement\n",
    "   - Composable validation functions\n",
    "   - Pure functions for constraint checking\n",
    "\n",
    "**Benefits of Declarative Data Quality**:\n",
    "- Centralized data quality rules\n",
    "- Automatic enforcement without application logic\n",
    "- Clear error messages for quality violations\n",
    "- Consistent quality across all write paths\n",
    "- Self-documenting data contracts\n",
    "\n",
    "**Next Steps**: In the next notebook, we'll explore Delta Live Tables (DLT) for even more advanced declarative data quality patterns and automated pipeline management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Practice implementing schema enforcement and constraints:\n",
    "\n",
    "1. Create a Delta table for your domain with appropriate constraints\n",
    "2. Test constraint violations with realistic bad data\n",
    "3. Implement safe schema evolution by adding new optional columns\n",
    "4. Build a validation utility class for your specific use case\n",
    "5. Create a complete data quality pipeline with pre-write validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "\n",
    "# 1. Create your domain-specific Delta table\n",
    "def create_your_table(table_path):\n",
    "    \"\"\"\n",
    "    Create a Delta table for your specific domain\n",
    "    \"\"\"\n",
    "    # Your table creation logic\n",
    "    pass\n",
    "\n",
    "# 2. Define domain-specific constraints\n",
    "def add_your_constraints(table_path):\n",
    "    \"\"\"\n",
    "    Add constraints relevant to your domain\n",
    "    \"\"\"\n",
    "    # Your constraint logic\n",
    "    pass\n",
    "\n",
    "# 3. Create validation functions\n",
    "def validate_your_data(df):\n",
    "    \"\"\"\n",
    "    Pre-write validation for your data\n",
    "    \"\"\"\n",
    "    # Your validation logic\n",
    "    pass\n",
    "\n",
    "# 4. Test with sample data\n",
    "# your_test_data = [...]\n",
    "# test_df = spark.createDataFrame(your_test_data, your_schema)\n",
    "# validate_your_data(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltalake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
