{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.2 Declarative Data Quality with Delta Live Tables (DLT) Expectations\n",
        "\n",
        "This notebook demonstrates how to implement declarative data quality using Delta Live Tables (DLT) expectations in a functional programming approach. We'll explore how DLT expectations align with functional principles and provide robust, automated data quality management.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand how to:\n",
        "- Design functional data pipelines with DLT expectations\n",
        "- Implement declarative data quality rules using expectations\n",
        "- Create composable expectation patterns\n",
        "- Handle data quality violations with different strategies (warn, drop, fail)\n",
        "- Monitor and alert on data quality metrics\n",
        "- Test DLT pipelines with functional approaches\n",
        "- Compare declarative vs imperative data quality patterns\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Understanding of PySpark DataFrames\n",
        "- Knowledge of functional programming concepts\n",
        "- Familiarity with Delta Lake basics\n",
        "- Experience with data validation patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Delta Live Tables and Expectations\n",
        "\n",
        "Delta Live Tables (DLT) is a declarative framework for building reliable data pipelines on Databricks. DLT expectations provide automated data quality testing built directly into the pipeline definition.\n",
        "\n",
        "### Why DLT Expectations?\n",
        "\n",
        "**Traditional Imperative Approach:**\n",
        "```python\n",
        "# Scattered validation logic throughout code\n",
        "if df.filter(col(\"age\") < 0).count() > 0:\n",
        "    raise ValueError(\"Invalid ages found\")\n",
        "\n",
        "# Manual error handling and logging\n",
        "# Difficult to track quality metrics\n",
        "# Quality checks can be missed\n",
        "```\n",
        "\n",
        "**DLT Declarative Approach:**\n",
        "```python\n",
        "@dlt.expect(\"valid_age\", \"age >= 0\")\n",
        "@dlt.expect(\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+$'\")\n",
        "@dlt.table\n",
        "def customers():\n",
        "    return spark.read.table(\"source_customers\")\n",
        "```\n",
        "\n",
        "### Functional Programming Alignment\n",
        "\n",
        "DLT expectations embody functional programming principles:\n",
        "- **Declarative**: Define *what* quality rules should be enforced, not *how*\n",
        "- **Composable**: Stack multiple expectations on the same table\n",
        "- **Immutable**: Expectations don't modify the pipeline definition\n",
        "- **Side-effect isolation**: Quality actions (warn/drop/fail) are explicitly declared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports for DLT pipeline demonstration\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from typing import Dict, List, Tuple, Optional, Callable, Any\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "import json\n",
        "\n",
        "# Initialize Spark session (if not already available)\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"DLTExpectations\").getOrCreate()\n",
        "\n",
        "print(\"‚úÖ Setup complete - Ready for DLT expectations demonstration!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. DLT Expectation Types and Strategies\n",
        "\n",
        "DLT provides three types of expectations with different violation handling strategies:\n",
        "\n",
        "| Expectation Type | Behavior | Use Case | Functional Impact |\n",
        "|-----------------|----------|----------|-------------------|\n",
        "| `@dlt.expect()` | **Warn** - Records violations in metrics | Monitoring non-critical quality | Pure observation, no data modification |\n",
        "| `@dlt.expect_or_drop()` | **Drop** - Removes violating records | Enforce quality constraints | Transforms data by filtering |\n",
        "| `@dlt.expect_or_fail()` | **Fail** - Stops pipeline execution | Critical quality gates | Halts execution on violation |\n",
        "\n",
        "### Conceptual DLT Pipeline Structure\n",
        "\n",
        "Since we're demonstrating concepts (DLT requires Databricks workspace setup), we'll create functional patterns that mirror DLT's declarative approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate DLT expectation framework with functional patterns\n",
        "\n",
        "class ExpectationType(Enum):\n",
        "    \"\"\"Types of expectation enforcement strategies\"\"\"\n",
        "    WARN = \"warn\"           # Record violation, continue processing\n",
        "    DROP = \"drop\"           # Remove violating records\n",
        "    FAIL = \"fail\"           # Stop pipeline on violation\n",
        "\n",
        "@dataclass\n",
        "class Expectation:\n",
        "    \"\"\"Immutable expectation definition\"\"\"\n",
        "    name: str\n",
        "    constraint: str\n",
        "    expectation_type: ExpectationType\n",
        "    \n",
        "    def evaluate(self, df: DataFrame) -> Tuple[DataFrame, 'ExpectationResult']:\n",
        "        \"\"\"\n",
        "        Pure function to evaluate expectation on DataFrame.\n",
        "        Returns tuple of (transformed_df, result) without side effects.\n",
        "        \"\"\"\n",
        "        total_count = df.count()\n",
        "        \n",
        "        # Count violating records\n",
        "        violating_count = df.filter(f\"NOT ({self.constraint})\").count()\n",
        "        valid_count = total_count - violating_count\n",
        "        \n",
        "        # Create result\n",
        "        result = ExpectationResult(\n",
        "            expectation_name=self.name,\n",
        "            constraint=self.constraint,\n",
        "            expectation_type=self.expectation_type,\n",
        "            total_records=total_count,\n",
        "            valid_records=valid_count,\n",
        "            violated_records=violating_count,\n",
        "            passed=violating_count == 0\n",
        "        )\n",
        "        \n",
        "        # Apply transformation based on expectation type\n",
        "        if self.expectation_type == ExpectationType.WARN:\n",
        "            # Return original DataFrame unchanged\n",
        "            return df, result\n",
        "        \n",
        "        elif self.expectation_type == ExpectationType.DROP:\n",
        "            # Return filtered DataFrame with violating records removed\n",
        "            filtered_df = df.filter(self.constraint)\n",
        "            return filtered_df, result\n",
        "        \n",
        "        elif self.expectation_type == ExpectationType.FAIL:\n",
        "            # Return DataFrame but signal failure in result\n",
        "            return df, result\n",
        "\n",
        "@dataclass\n",
        "class ExpectationResult:\n",
        "    \"\"\"Immutable expectation evaluation result\"\"\"\n",
        "    expectation_name: str\n",
        "    constraint: str\n",
        "    expectation_type: ExpectationType\n",
        "    total_records: int\n",
        "    valid_records: int\n",
        "    violated_records: int\n",
        "    passed: bool\n",
        "    \n",
        "    @property\n",
        "    def violation_rate(self) -> float:\n",
        "        \"\"\"Calculate percentage of records violating the expectation\"\"\"\n",
        "        return (self.violated_records / self.total_records * 100) if self.total_records > 0 else 0.0\n",
        "    \n",
        "    def __str__(self) -> str:\n",
        "        status = \"‚úÖ PASSED\" if self.passed else \"‚ùå FAILED\"\n",
        "        return (f\"{status} - {self.expectation_name} ({self.expectation_type.value})\\n\"\n",
        "                f\"  Constraint: {self.constraint}\\n\"\n",
        "                f\"  Valid: {self.valid_records:,} / {self.total_records:,} \"\n",
        "                f\"({100-self.violation_rate:.1f}%)\\n\"\n",
        "                f\"  Violations: {self.violated_records:,} ({self.violation_rate:.1f}%)\")\n",
        "\n",
        "print(\"‚úÖ Expectation framework classes defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creating Sample Data with Quality Issues\n",
        "\n",
        "Let's create realistic sample data that contains various data quality issues to demonstrate expectation handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_customer_data_with_quality_issues():\n",
        "    \"\"\"\n",
        "    Pure function to create customer data with intentional quality issues.\n",
        "    Demonstrates various types of data quality violations.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Mix of valid and invalid records\n",
        "    data = [\n",
        "        # Valid records\n",
        "        (1, \"Alice Johnson\", \"alice@example.com\", 28, \"2020-01-15\", \"Premium\", 1500.00, \"US\"),\n",
        "        (2, \"Bob Smith\", \"bob.smith@company.com\", 35, \"2019-06-20\", \"Standard\", 800.00, \"CA\"),\n",
        "        (3, \"Carol Davis\", \"carol.d@email.com\", 42, \"2021-03-10\", \"Premium\", 2000.00, \"UK\"),\n",
        "        (4, \"David Wilson\", \"david.w@example.org\", 31, \"2020-11-05\", \"Standard\", 950.00, \"US\"),\n",
        "        (5, \"Emma Brown\", \"emma.brown@mail.com\", 29, \"2022-01-18\", \"Premium\", 1800.00, \"AU\"),\n",
        "        \n",
        "        # Records with quality issues\n",
        "        (6, \"Frank Miller\", \"invalid-email\", 45, \"2021-07-22\", \"Standard\", 700.00, \"US\"),  # Invalid email\n",
        "        (7, \"Grace Lee\", \"grace@example.com\", -5, \"2020-09-15\", \"Premium\", 1200.00, \"CA\"),  # Negative age\n",
        "        (8, \"Henry Chen\", \"henry.chen@email.com\", 150, \"2019-12-01\", \"Standard\", 600.00, \"CN\"),  # Age > 120\n",
        "        (9, None, \"unknown@email.com\", 30, \"2021-04-10\", \"Standard\", 500.00, \"UK\"),  # Null name\n",
        "        (10, \"Isabel Garcia\", None, 33, \"2020-08-25\", \"Premium\", 1600.00, \"ES\"),  # Null email\n",
        "        (11, \"Jack Taylor\", \"jack@example.com\", 28, \"invalid-date\", \"Standard\", 850.00, \"US\"),  # Invalid date\n",
        "        (12, \"Kate Anderson\", \"kate@email.com\", 40, \"2021-02-14\", \"InvalidTier\", 1100.00, \"AU\"),  # Invalid tier\n",
        "        (13, \"Leo Martinez\", \"leo@example.com\", 35, \"2020-05-30\", \"Premium\", -500.00, \"MX\"),  # Negative balance\n",
        "        (14, \"Maria Rodriguez\", \"maria@email.com\", 38, \"2019-10-18\", \"Standard\", None, \"BR\"),  # Null balance\n",
        "        (15, \"Nathan White\", \"nathan@example.com\", 27, \"2022-03-22\", \"Premium\", 1700.00, None),  # Null country\n",
        "        \n",
        "        # More valid records\n",
        "        (16, \"Olivia Harris\", \"olivia.h@email.com\", 32, \"2021-11-08\", \"Standard\", 920.00, \"US\"),\n",
        "        (17, \"Paul Clark\", \"paul.clark@company.com\", 44, \"2020-04-17\", \"Premium\", 1950.00, \"CA\"),\n",
        "        (18, \"Quinn Lewis\", \"quinn@example.com\", 36, \"2021-08-29\", \"Standard\", 780.00, \"UK\"),\n",
        "    ]\n",
        "    \n",
        "    schema = StructType([\n",
        "        StructField(\"customer_id\", IntegerType(), False),\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"email\", StringType(), True),\n",
        "        StructField(\"age\", IntegerType(), True),\n",
        "        StructField(\"signup_date\", StringType(), True),\n",
        "        StructField(\"tier\", StringType(), True),\n",
        "        StructField(\"account_balance\", DoubleType(), True),\n",
        "        StructField(\"country\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    return spark.createDataFrame(data, schema)\n",
        "\n",
        "# Create sample data\n",
        "customers_df = create_customer_data_with_quality_issues()\n",
        "\n",
        "print(f\"Created customer dataset with {customers_df.count()} records\")\n",
        "print(\"\\nSample data (showing quality issues):\")\n",
        "customers_df.show(truncate=False)\n",
        "\n",
        "print(\"\\nData quality issues present:\")\n",
        "print(\"  ‚úó Invalid email formats\")\n",
        "print(\"  ‚úó Invalid ages (negative, > 120)\")\n",
        "print(\"  ‚úó Null values in required fields\")\n",
        "print(\"  ‚úó Invalid date formats\")\n",
        "print(\"  ‚úó Invalid tier values\")\n",
        "print(\"  ‚úó Negative account balances\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Implementing DLT-Style Expectations\n",
        "\n",
        "Let's create expectations for our customer data using the three different strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define expectations for customer data quality\n",
        "\n",
        "# WARN expectations - Monitor quality issues without blocking\n",
        "warn_expectations = [\n",
        "    Expectation(\n",
        "        name=\"valid_email_format\",\n",
        "        constraint=\"email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\\\\\.[a-zA-Z]{2,}$'\",\n",
        "        expectation_type=ExpectationType.WARN\n",
        "    ),\n",
        "    Expectation(\n",
        "        name=\"valid_signup_date\",\n",
        "        constraint=\"signup_date RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\",\n",
        "        expectation_type=ExpectationType.WARN\n",
        "    ),\n",
        "]\n",
        "\n",
        "# DROP expectations - Remove records that violate constraints\n",
        "drop_expectations = [\n",
        "    Expectation(\n",
        "        name=\"valid_age_range\",\n",
        "        constraint=\"age >= 0 AND age <= 120\",\n",
        "        expectation_type=ExpectationType.DROP\n",
        "    ),\n",
        "    Expectation(\n",
        "        name=\"valid_tier\",\n",
        "        constraint=\"tier IN ('Standard', 'Premium')\",\n",
        "        expectation_type=ExpectationType.DROP\n",
        "    ),\n",
        "    Expectation(\n",
        "        name=\"non_negative_balance\",\n",
        "        constraint=\"account_balance >= 0\",\n",
        "        expectation_type=ExpectationType.DROP\n",
        "    ),\n",
        "]\n",
        "\n",
        "# FAIL expectations - Critical quality gates that must pass\n",
        "fail_expectations = [\n",
        "    Expectation(\n",
        "        name=\"required_name\",\n",
        "        constraint=\"name IS NOT NULL AND length(name) > 0\",\n",
        "        expectation_type=ExpectationType.FAIL\n",
        "    ),\n",
        "    Expectation(\n",
        "        name=\"required_email\",\n",
        "        constraint=\"email IS NOT NULL AND length(email) > 0\",\n",
        "        expectation_type=ExpectationType.FAIL\n",
        "    ),\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Expectations defined:\")\n",
        "print(f\"  - WARN expectations: {len(warn_expectations)}\")\n",
        "print(f\"  - DROP expectations: {len(drop_expectations)}\")\n",
        "print(f\"  - FAIL expectations: {len(fail_expectations)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. DLT Pipeline Simulation\n",
        "\n",
        "Let's create a functional pipeline that applies expectations in the proper order: FAIL ‚Üí DROP ‚Üí WARN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DLTPipeline:\n",
        "    \"\"\"\n",
        "    Functional DLT pipeline that applies expectations declaratively.\n",
        "    Immutable and composable.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "        self.expectations: List[Expectation] = []\n",
        "        self.results: List[ExpectationResult] = []\n",
        "    \n",
        "    def add_expectation(self, expectation: Expectation) -> 'DLTPipeline':\n",
        "        \"\"\"Add an expectation to the pipeline (returns new pipeline)\"\"\"\n",
        "        new_pipeline = DLTPipeline(self.name)\n",
        "        new_pipeline.expectations = self.expectations + [expectation]\n",
        "        new_pipeline.results = self.results.copy()\n",
        "        return new_pipeline\n",
        "    \n",
        "    def add_expectations(self, expectations: List[Expectation]) -> 'DLTPipeline':\n",
        "        \"\"\"Add multiple expectations (returns new pipeline)\"\"\"\n",
        "        new_pipeline = DLTPipeline(self.name)\n",
        "        new_pipeline.expectations = self.expectations + expectations\n",
        "        new_pipeline.results = self.results.copy()\n",
        "        return new_pipeline\n",
        "    \n",
        "    def execute(self, df: DataFrame) -> Tuple[DataFrame, List[ExpectationResult]]:\n",
        "        \"\"\"\n",
        "        Execute pipeline with all expectations.\n",
        "        Pure function that returns transformed DataFrame and results.\n",
        "        \"\"\"\n",
        "        current_df = df\n",
        "        all_results = []\n",
        "        \n",
        "        # Sort expectations by type: FAIL first, then DROP, then WARN\n",
        "        expectation_order = {\n",
        "            ExpectationType.FAIL: 0,\n",
        "            ExpectationType.DROP: 1,\n",
        "            ExpectationType.WARN: 2\n",
        "        }\n",
        "        sorted_expectations = sorted(\n",
        "            self.expectations,\n",
        "            key=lambda e: expectation_order[e.expectation_type]\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nüîÑ Executing DLT Pipeline: {self.name}\")\n",
        "        print(f\"Total expectations: {len(sorted_expectations)}\\n\")\n",
        "        \n",
        "        for expectation in sorted_expectations:\n",
        "            # Evaluate expectation\n",
        "            transformed_df, result = expectation.evaluate(current_df)\n",
        "            all_results.append(result)\n",
        "            \n",
        "            # Print result\n",
        "            print(f\"{'='*70}\")\n",
        "            print(result)\n",
        "            print()\n",
        "            \n",
        "            # Handle FAIL expectation\n",
        "            if expectation.expectation_type == ExpectationType.FAIL and not result.passed:\n",
        "                print(\"üö® PIPELINE FAILED - Critical expectation violated!\")\n",
        "                print(f\"   Stopping execution at expectation: {expectation.name}\\n\")\n",
        "                return current_df, all_results\n",
        "            \n",
        "            # Update current DataFrame\n",
        "            current_df = transformed_df\n",
        "        \n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"‚úÖ Pipeline completed successfully\")\n",
        "        print(f\"   Input records: {df.count():,}\")\n",
        "        print(f\"   Output records: {current_df.count():,}\")\n",
        "        print(f\"   Records dropped: {df.count() - current_df.count():,}\\n\")\n",
        "        \n",
        "        return current_df, all_results\n",
        "\n",
        "# Create and configure DLT pipeline\n",
        "customer_pipeline = (\n",
        "    DLTPipeline(\"customer_quality_pipeline\")\n",
        "    .add_expectations(fail_expectations)   # Critical gates first\n",
        "    .add_expectations(drop_expectations)   # Filter violations\n",
        "    .add_expectations(warn_expectations)   # Monitor quality\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Pipeline configured with {len(customer_pipeline.expectations)} expectations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the DLT pipeline\n",
        "cleaned_customers_df, pipeline_results = customer_pipeline.execute(customers_df)\n",
        "\n",
        "# Show cleaned data\n",
        "print(\"\\nüìä Cleaned Customer Data:\")\n",
        "cleaned_customers_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Quality Metrics and Monitoring\n",
        "\n",
        "DLT expectations automatically generate quality metrics. Let's create a functional metrics reporting system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class QualityReport:\n",
        "    \"\"\"Immutable quality report aggregating expectation results\"\"\"\n",
        "    pipeline_name: str\n",
        "    results: List[ExpectationResult]\n",
        "    input_record_count: int\n",
        "    output_record_count: int\n",
        "    \n",
        "    @property\n",
        "    def total_expectations(self) -> int:\n",
        "        return len(self.results)\n",
        "    \n",
        "    @property\n",
        "    def passed_expectations(self) -> int:\n",
        "        return sum(1 for r in self.results if r.passed)\n",
        "    \n",
        "    @property\n",
        "    def failed_expectations(self) -> int:\n",
        "        return sum(1 for r in self.results if not r.passed)\n",
        "    \n",
        "    @property\n",
        "    def records_dropped(self) -> int:\n",
        "        return self.input_record_count - self.output_record_count\n",
        "    \n",
        "    @property\n",
        "    def drop_rate(self) -> float:\n",
        "        return (self.records_dropped / self.input_record_count * 100) if self.input_record_count > 0 else 0.0\n",
        "    \n",
        "    def get_results_by_type(self, expectation_type: ExpectationType) -> List[ExpectationResult]:\n",
        "        \"\"\"Filter results by expectation type\"\"\"\n",
        "        return [r for r in self.results if r.expectation_type == expectation_type]\n",
        "    \n",
        "    def print_summary(self):\n",
        "        \"\"\"Print comprehensive quality report\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"üìä DATA QUALITY REPORT: {self.pipeline_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        print(f\"\\nüìà Pipeline Statistics:\")\n",
        "        print(f\"  Input Records:     {self.input_record_count:,}\")\n",
        "        print(f\"  Output Records:    {self.output_record_count:,}\")\n",
        "        print(f\"  Records Dropped:   {self.records_dropped:,} ({self.drop_rate:.1f}%)\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ Expectation Summary:\")\n",
        "        print(f\"  Total Expectations: {self.total_expectations}\")\n",
        "        print(f\"  Passed:            {self.passed_expectations}\")\n",
        "        print(f\"  Failed:            {self.failed_expectations}\")\n",
        "        \n",
        "        # Break down by expectation type\n",
        "        for exp_type in ExpectationType:\n",
        "            type_results = self.get_results_by_type(exp_type)\n",
        "            if type_results:\n",
        "                passed = sum(1 for r in type_results if r.passed)\n",
        "                total = len(type_results)\n",
        "                print(f\"\\n  {exp_type.value.upper()} Expectations: {passed}/{total} passed\")\n",
        "                \n",
        "                for result in type_results:\n",
        "                    status = \"‚úÖ\" if result.passed else \"‚ùå\"\n",
        "                    print(f\"    {status} {result.expectation_name}: \"\n",
        "                          f\"{result.violated_records:,} violations ({result.violation_rate:.1f}%)\")\n",
        "        \n",
        "        print(f\"\\n{'='*80}\\n\")\n",
        "    \n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert report to dictionary for JSON serialization\"\"\"\n",
        "        return {\n",
        "            \"pipeline_name\": self.pipeline_name,\n",
        "            \"input_record_count\": self.input_record_count,\n",
        "            \"output_record_count\": self.output_record_count,\n",
        "            \"records_dropped\": self.records_dropped,\n",
        "            \"drop_rate\": self.drop_rate,\n",
        "            \"total_expectations\": self.total_expectations,\n",
        "            \"passed_expectations\": self.passed_expectations,\n",
        "            \"failed_expectations\": self.failed_expectations,\n",
        "            \"expectations\": [\n",
        "                {\n",
        "                    \"name\": r.expectation_name,\n",
        "                    \"type\": r.expectation_type.value,\n",
        "                    \"constraint\": r.constraint,\n",
        "                    \"passed\": r.passed,\n",
        "                    \"total_records\": r.total_records,\n",
        "                    \"valid_records\": r.valid_records,\n",
        "                    \"violated_records\": r.violated_records,\n",
        "                    \"violation_rate\": r.violation_rate\n",
        "                }\n",
        "                for r in self.results\n",
        "            ]\n",
        "        }\n",
        "\n",
        "# Generate quality report\n",
        "quality_report = QualityReport(\n",
        "    pipeline_name=customer_pipeline.name,\n",
        "    results=pipeline_results,\n",
        "    input_record_count=customers_df.count(),\n",
        "    output_record_count=cleaned_customers_df.count()\n",
        ")\n",
        "\n",
        "quality_report.print_summary()\n",
        "\n",
        "# Export to JSON for monitoring systems\n",
        "print(\"üìÑ Quality Report (JSON format for monitoring):\")\n",
        "print(json.dumps(quality_report.to_dict(), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Actual DLT Pipeline Code Examples\n",
        "\n",
        "Here's how you would write real DLT pipelines in Databricks. These examples show the actual Python code you would use in a Databricks notebook configured as a DLT pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE: This code is for demonstration and would run in a Databricks DLT pipeline notebook\n",
        "# It will not execute in a standard notebook without DLT runtime\n",
        "\n",
        "# Example 1: Bronze Layer with WARN expectations\n",
        "# Monitor data quality issues without blocking ingestion\n",
        "\n",
        "\"\"\"\n",
        "import dlt\n",
        "from pyspark.sql.functions import col, to_date\n",
        "\n",
        "@dlt.table(\n",
        "    name=\"customers_bronze\",\n",
        "    comment=\"Raw customer data with quality monitoring\"\n",
        ")\n",
        "@dlt.expect(\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+$'\")\n",
        "@dlt.expect(\"valid_date_format\", \"signup_date RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\")\n",
        "def customers_bronze():\n",
        "    return (\n",
        "        spark.readStream\n",
        "        .format(\"cloudFiles\")\n",
        "        .option(\"cloudFiles.format\", \"json\")\n",
        "        .load(\"/mnt/source/customers/\")\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example 1: Bronze layer with WARN expectations\")\n",
        "print(\"  - Monitors email format quality\")\n",
        "print(\"  - Monitors date format compliance\")\n",
        "print(\"  - Records metrics but doesn't drop records\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Silver Layer with DROP expectations\n",
        "# Remove records that violate business rules\n",
        "\n",
        "\"\"\"\n",
        "@dlt.table(\n",
        "    name=\"customers_silver\",\n",
        "    comment=\"Cleaned customer data with enforced quality constraints\"\n",
        ")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")\n",
        "@dlt.expect_or_drop(\"valid_tier\", \"tier IN ('Standard', 'Premium', 'Enterprise')\")\n",
        "@dlt.expect_or_drop(\"positive_balance\", \"account_balance >= 0\")\n",
        "@dlt.expect_or_drop(\"valid_country\", \"country IS NOT NULL AND length(country) = 2\")\n",
        "def customers_silver():\n",
        "    return (\n",
        "        dlt.read_stream(\"customers_bronze\")\n",
        "        .select(\n",
        "            \"customer_id\",\n",
        "            \"name\",\n",
        "            \"email\",\n",
        "            col(\"age\").cast(\"int\"),\n",
        "            to_date(\"signup_date\").alias(\"signup_date\"),\n",
        "            \"tier\",\n",
        "            col(\"account_balance\").cast(\"double\"),\n",
        "            \"country\"\n",
        "        )\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example 2: Silver layer with DROP expectations\")\n",
        "print(\"  - Enforces age range (18-120)\")\n",
        "print(\"  - Validates tier values\")\n",
        "print(\"  - Ensures positive account balance\")\n",
        "print(\"  - Validates country code format\")\n",
        "print(\"  - Drops records that violate any constraint\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Gold Layer with FAIL expectations\n",
        "# Critical quality gates for business-critical tables\n",
        "\n",
        "\"\"\"\n",
        "@dlt.table(\n",
        "    name=\"customers_gold\",\n",
        "    comment=\"Production-ready customer data with strict quality guarantees\"\n",
        ")\n",
        "@dlt.expect_or_fail(\"no_nulls_in_key_fields\", \n",
        "                     \"customer_id IS NOT NULL AND name IS NOT NULL AND email IS NOT NULL\")\n",
        "@dlt.expect_or_fail(\"unique_customer_id\", \"customer_id IS NOT NULL\")\n",
        "@dlt.expect(\"high_quality_data\", \"age BETWEEN 18 AND 120\")\n",
        "def customers_gold():\n",
        "    return (\n",
        "        dlt.read(\"customers_silver\")\n",
        "        .groupBy(\"customer_id\")  # Ensure uniqueness\n",
        "        .agg(\n",
        "            F.first(\"name\").alias(\"name\"),\n",
        "            F.first(\"email\").alias(\"email\"),\n",
        "            F.first(\"age\").alias(\"age\"),\n",
        "            F.first(\"signup_date\").alias(\"signup_date\"),\n",
        "            F.first(\"tier\").alias(\"tier\"),\n",
        "            F.sum(\"account_balance\").alias(\"total_balance\"),\n",
        "            F.first(\"country\").alias(\"country\")\n",
        "        )\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example 3: Gold layer with FAIL expectations\")\n",
        "print(\"  - Fails pipeline if key fields are null\")\n",
        "print(\"  - Ensures customer_id uniqueness\")\n",
        "print(\"  - Monitors (but doesn't fail on) age quality\")\n",
        "print(\"  - Stops pipeline execution on critical violations\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Advanced DLT patterns with custom Python functions\n",
        "\n",
        "\"\"\"\n",
        "# Custom validation function (pure function)\n",
        "def is_valid_email_domain(email: str, allowed_domains: List[str]) -> bool:\n",
        "    \\\"\\\"\\\"Pure function to validate email domain\\\"\\\"\\\"\n",
        "    if not email or '@' not in email:\n",
        "        return False\n",
        "    domain = email.split('@')[1]\n",
        "    return domain in allowed_domains\n",
        "\n",
        "# Register as UDF for use in SQL expressions\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import BooleanType\n",
        "\n",
        "is_valid_email_domain_udf = udf(is_valid_email_domain, BooleanType())\n",
        "\n",
        "@dlt.table(\n",
        "    name=\"customers_with_domain_validation\",\n",
        "    comment=\"Customers with domain-specific validation\"\n",
        ")\n",
        "@dlt.expect_or_drop(\n",
        "    \"approved_email_domain\",\n",
        "    \"is_valid_email_domain(email, array('example.com', 'company.com', 'email.com'))\"\n",
        ")\n",
        "def customers_with_domain_validation():\n",
        "    return (\n",
        "        dlt.read(\"customers_silver\")\n",
        "        .filter(col(\"tier\") == \"Premium\")  # Only Premium customers need domain validation\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example 4: Advanced patterns with custom functions\")\n",
        "print(\"  - Uses custom Python functions for complex validation\")\n",
        "print(\"  - Combines functional programming with DLT expectations\")\n",
        "print(\"  - Demonstrates composable validation logic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comparing Imperative vs Declarative Approaches\n",
        "\n",
        "Let's contrast imperative data quality checks with DLT's declarative approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"IMPERATIVE vs DECLARATIVE DATA QUALITY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n‚ùå IMPERATIVE APPROACH (Anti-Pattern):\")\n",
        "print(\"\"\"\n",
        "def process_customers_imperative(df):\n",
        "    # Scattered validation logic with side effects\n",
        "    \n",
        "    # Manual null checks\n",
        "    null_names = df.filter(col(\"name\").isNull()).count()\n",
        "    if null_names > 0:\n",
        "        print(f\"WARNING: {null_names} records with null names\")\n",
        "        df = df.filter(col(\"name\").isNotNull())\n",
        "    \n",
        "    # Manual age validation\n",
        "    invalid_ages = df.filter((col(\"age\") < 0) | (col(\"age\") > 120)).count()\n",
        "    if invalid_ages > 0:\n",
        "        print(f\"WARNING: {invalid_ages} records with invalid ages\")\n",
        "        df = df.filter((col(\"age\") >= 0) & (col(\"age\") <= 120))\n",
        "    \n",
        "    # Manual email validation\n",
        "    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'\n",
        "    invalid_emails = df.filter(~col(\"email\").rlike(email_pattern)).count()\n",
        "    if invalid_emails > 0:\n",
        "        print(f\"WARNING: {invalid_emails} records with invalid emails\")\n",
        "        # Maybe drop, maybe keep - inconsistent handling\n",
        "    \n",
        "    # Manual tier validation\n",
        "    df = df.filter(col(\"tier\").isin(['Standard', 'Premium']))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Issues:\n",
        "# 1. Side effects (printing) mixed with transformation logic\n",
        "# 2. Inconsistent error handling (some drop, some warn)\n",
        "# 3. No centralized quality metrics\n",
        "# 4. Hard to test individual validations\n",
        "# 5. Validation logic scattered throughout code\n",
        "# 6. No automatic quality monitoring\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n‚úÖ DECLARATIVE APPROACH (Best Practice):\")\n",
        "print(\"\"\"\n",
        "@dlt.table(name=\"customers_clean\")\n",
        "@dlt.expect_or_fail(\"valid_name\", \"name IS NOT NULL\")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 0 AND age <= 120\")\n",
        "@dlt.expect(\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+$'\")\n",
        "@dlt.expect_or_drop(\"valid_tier\", \"tier IN ('Standard', 'Premium')\")\n",
        "def customers_clean():\n",
        "    return spark.table(\"customers_source\")\n",
        "\n",
        "# Benefits:\n",
        "# 1. Clear separation of concerns (expectations vs transformations)\n",
        "# 2. Consistent, explicit error handling strategy\n",
        "# 3. Automatic quality metrics collection\n",
        "# 4. Easy to test and reason about\n",
        "# 5. Self-documenting quality requirements\n",
        "# 6. Built-in monitoring and alerting\n",
        "# 7. Composable and reusable\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nüìä Key Differences:\")\n",
        "print(\"\"\"\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Aspect              ‚îÇ Imperative               ‚îÇ Declarative (DLT)         ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Code Style          ‚îÇ How to validate          ‚îÇ What to validate          ‚îÇ\n",
        "‚îÇ Side Effects        ‚îÇ Mixed with logic         ‚îÇ Isolated in decorators    ‚îÇ\n",
        "‚îÇ Quality Metrics     ‚îÇ Manual tracking          ‚îÇ Automatic collection      ‚îÇ\n",
        "‚îÇ Consistency         ‚îÇ Varies by implementation ‚îÇ Standardized              ‚îÇ\n",
        "‚îÇ Testability         ‚îÇ Difficult                ‚îÇ Easy                      ‚îÇ\n",
        "‚îÇ Monitoring          ‚îÇ Custom implementation    ‚îÇ Built-in                  ‚îÇ\n",
        "‚îÇ Composability       ‚îÇ Low                      ‚îÇ High                      ‚îÇ\n",
        "‚îÇ Maintainability     ‚îÇ Low                      ‚îÇ High                      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Best Practices and Patterns\n",
        "\n",
        "Guidelines for effective use of DLT expectations in production pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"DLT EXPECTATIONS BEST PRACTICES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "1. ‚úÖ CHOOSE THE RIGHT EXPECTATION TYPE\n",
        "\n",
        "   @dlt.expect() - WARN\n",
        "   ‚Ä¢ Use for: Monitoring data quality trends\n",
        "   ‚Ä¢ Example: Email format compliance, date format standardization\n",
        "   ‚Ä¢ Benefit: Visibility without blocking data flow\n",
        "   \n",
        "   @dlt.expect_or_drop() - DROP\n",
        "   ‚Ä¢ Use for: Business rule enforcement\n",
        "   ‚Ä¢ Example: Valid age ranges, approved categories, positive amounts\n",
        "   ‚Ä¢ Benefit: Clean data without pipeline failures\n",
        "   \n",
        "   @dlt.expect_or_fail() - FAIL\n",
        "   ‚Ä¢ Use for: Critical quality gates\n",
        "   ‚Ä¢ Example: Required fields, data corruption detection\n",
        "   ‚Ä¢ Benefit: Prevent bad data from reaching production\n",
        "\n",
        "2. ‚úÖ LAYER YOUR EXPECTATIONS\n",
        "\n",
        "   Bronze Layer:\n",
        "   ‚Ä¢ Use mostly WARN expectations\n",
        "   ‚Ä¢ Monitor raw data quality issues\n",
        "   ‚Ä¢ Maintain complete audit trail\n",
        "   \n",
        "   Silver Layer:\n",
        "   ‚Ä¢ Use DROP expectations for business rules\n",
        "   ‚Ä¢ Apply data cleansing and standardization\n",
        "   ‚Ä¢ Create clean, validated datasets\n",
        "   \n",
        "   Gold Layer:\n",
        "   ‚Ä¢ Use FAIL expectations for critical constraints\n",
        "   ‚Ä¢ Enforce strict quality for business-critical tables\n",
        "   ‚Ä¢ Ensure production-ready data quality\n",
        "\n",
        "3. ‚úÖ WRITE CLEAR, TESTABLE CONSTRAINTS\n",
        "\n",
        "   Good:\n",
        "   @dlt.expect(\"positive_amount\", \"amount > 0\")\n",
        "   @dlt.expect(\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+$'\")\n",
        "   \n",
        "   Better:\n",
        "   # More descriptive names and comprehensive checks\n",
        "   @dlt.expect(\"transaction_amount_positive\", \"amount > 0 AND amount < 1000000\")\n",
        "   @dlt.expect(\"customer_email_format_valid\", \n",
        "               \"email IS NOT NULL AND email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'\")\n",
        "\n",
        "4. ‚úÖ USE MEANINGFUL EXPECTATION NAMES\n",
        "\n",
        "   Pattern: [entity]_[field]_[validation_type]\n",
        "   \n",
        "   Good names:\n",
        "   ‚Ä¢ customer_age_within_valid_range\n",
        "   ‚Ä¢ order_total_positive_value\n",
        "   ‚Ä¢ product_sku_unique_identifier\n",
        "   ‚Ä¢ transaction_date_not_future\n",
        "   \n",
        "   Avoid:\n",
        "   ‚Ä¢ check1, validation_rule_2\n",
        "   ‚Ä¢ test, verify\n",
        "   ‚Ä¢ age_check (too vague)\n",
        "\n",
        "5. ‚úÖ COMPOSE EXPECTATIONS FUNCTIONALLY\n",
        "\n",
        "   # Create reusable expectation configurations\n",
        "   def standard_customer_expectations():\n",
        "       return [\n",
        "           (\"valid_age\", \"age >= 18 AND age <= 120\", \"drop\"),\n",
        "           (\"valid_email\", \"email IS NOT NULL\", \"fail\"),\n",
        "           (\"valid_tier\", \"tier IN ('Standard', 'Premium')\", \"drop\"),\n",
        "       ]\n",
        "   \n",
        "   # Apply to multiple tables\n",
        "   @dlt.table(name=\"us_customers\")\n",
        "   @apply_expectations(standard_customer_expectations())\n",
        "   def us_customers():\n",
        "       return spark.table(\"customers\").filter(col(\"country\") == \"US\")\n",
        "\n",
        "6. ‚úÖ MONITOR AND ALERT ON QUALITY METRICS\n",
        "\n",
        "   ‚Ä¢ Set up dashboards for expectation violations\n",
        "   ‚Ä¢ Configure alerts for critical expectation failures\n",
        "   ‚Ä¢ Track quality trends over time\n",
        "   ‚Ä¢ Review violation patterns regularly\n",
        "   ‚Ä¢ Use metrics to improve upstream data sources\n",
        "\n",
        "7. ‚úÖ TEST EXPECTATIONS BEFORE PRODUCTION\n",
        "\n",
        "   ‚Ä¢ Use sample data to validate expectation logic\n",
        "   ‚Ä¢ Test with both valid and invalid records\n",
        "   ‚Ä¢ Verify expectation names and constraints\n",
        "   ‚Ä¢ Ensure appropriate expectation types\n",
        "   ‚Ä¢ Check drop rates are reasonable\n",
        "\n",
        "8. ‚úÖ DOCUMENT BUSINESS RULES IN EXPECTATIONS\n",
        "\n",
        "   @dlt.table(\n",
        "       name=\"customers\",\n",
        "       comment=\"Customer master table with enforced quality standards\"\n",
        "   )\n",
        "   @dlt.expect_or_drop(\n",
        "       \"customer_age_legal_minimum\",\n",
        "       \"age >= 18\",\n",
        "       \"Customers must be 18 or older per legal requirements\"\n",
        "   )\n",
        "   def customers():\n",
        "       return spark.table(\"source_customers\")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Anti-Patterns to Avoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"DLT EXPECTATIONS ANTI-PATTERNS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "‚ùå 1. OVERLY COMPLEX EXPECTATIONS\n",
        "\n",
        "Bad:\n",
        "@dlt.expect(\"complex_validation\",\n",
        "           \"(age >= 18 AND tier = 'Premium' AND balance > 1000) OR \n",
        "            (age >= 21 AND tier = 'Standard' AND balance > 500) OR \n",
        "            (country IN ('US', 'CA') AND age >= 16)\")\n",
        "\n",
        "Better:\n",
        "# Break into separate, testable expectations\n",
        "@dlt.expect_or_drop(\"minimum_age\", \"age >= 16\")\n",
        "@dlt.expect_or_drop(\"premium_requirements\", \n",
        "                     \"tier != 'Premium' OR (age >= 18 AND balance > 1000)\")\n",
        "@dlt.expect_or_drop(\"standard_requirements\",\n",
        "                     \"tier != 'Standard' OR (age >= 21 AND balance > 500)\")\n",
        "\n",
        "‚ùå 2. USING WRONG EXPECTATION TYPE\n",
        "\n",
        "Bad:\n",
        "# Using FAIL for non-critical quality monitoring\n",
        "@dlt.expect_or_fail(\"preferred_email_domain\", \n",
        "                     \"email LIKE '%@company.com'\")\n",
        "\n",
        "Better:\n",
        "# Use WARN for monitoring, not enforcement\n",
        "@dlt.expect(\"preferred_email_domain\",\n",
        "           \"email LIKE '%@company.com'\")\n",
        "\n",
        "‚ùå 3. MIXING VALIDATION WITH TRANSFORMATION\n",
        "\n",
        "Bad:\n",
        "@dlt.table(name=\"customers\")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18\")\n",
        "def customers():\n",
        "    # Don't mix validation logic in the transformation\n",
        "    df = spark.table(\"source\")\n",
        "    df = df.filter(col(\"country\") == \"US\")  # ‚ùå Buried business logic\n",
        "    df = df.filter(col(\"status\") == \"active\")  # ‚ùå Hard to track\n",
        "    return df\n",
        "\n",
        "Better:\n",
        "@dlt.table(name=\"customers\")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18\")\n",
        "@dlt.expect_or_drop(\"us_customers_only\", \"country = 'US'\")\n",
        "@dlt.expect_or_drop(\"active_customers_only\", \"status = 'active'\")\n",
        "def customers():\n",
        "    # Pure transformation, validation in expectations\n",
        "    return spark.table(\"source\")\n",
        "\n",
        "‚ùå 4. NO MONITORING OR ALERTING\n",
        "\n",
        "Bad:\n",
        "# Define expectations but never check the results\n",
        "@dlt.expect(\"quality_check\", \"amount > 0\")\n",
        "# No dashboard, no alerts, no review process\n",
        "\n",
        "Better:\n",
        "# Set up comprehensive monitoring\n",
        "# - Create quality dashboards\n",
        "# - Configure alerts for critical violations\n",
        "# - Regular review of quality metrics\n",
        "# - Automated reports on data quality trends\n",
        "\n",
        "‚ùå 5. VAGUE EXPECTATION NAMES\n",
        "\n",
        "Bad:\n",
        "@dlt.expect(\"check1\", \"age > 0\")\n",
        "@dlt.expect(\"validation\", \"email IS NOT NULL\")\n",
        "@dlt.expect(\"test\", \"balance >= 0\")\n",
        "\n",
        "Better:\n",
        "@dlt.expect(\"customer_age_positive\", \"age > 0\")\n",
        "@dlt.expect(\"customer_email_required\", \"email IS NOT NULL\")\n",
        "@dlt.expect(\"account_balance_non_negative\", \"balance >= 0\")\n",
        "\n",
        "‚ùå 6. IGNORING DROPPED RECORDS\n",
        "\n",
        "Bad:\n",
        "@dlt.expect_or_drop(\"valid_data\", \"complex_condition\")\n",
        "# Never check how many records are being dropped\n",
        "# Could be silently losing important data\n",
        "\n",
        "Better:\n",
        "# Monitor drop rates\n",
        "# Alert if drop rate exceeds threshold (e.g., >5%)\n",
        "# Investigate root causes of violations\n",
        "# Fix upstream data issues\n",
        "\n",
        "‚ùå 7. DUPLICATE VALIDATION LOGIC\n",
        "\n",
        "Bad:\n",
        "# Same validation in multiple places\n",
        "@dlt.table(name=\"customers_us\")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")\n",
        "def customers_us():\n",
        "    return spark.table(\"source\").filter(col(\"country\") == \"US\")\n",
        "\n",
        "@dlt.table(name=\"customers_ca\")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")  # ‚ùå Duplicate\n",
        "def customers_ca():\n",
        "    return spark.table(\"source\").filter(col(\"country\") == \"CA\")\n",
        "\n",
        "Better:\n",
        "# Create reusable expectation definitions\n",
        "STANDARD_AGE_VALIDATION = (\"valid_age\", \"age >= 18 AND age <= 120\")\n",
        "\n",
        "@dlt.table(name=\"customers_us\")\n",
        "@dlt.expect_or_drop(*STANDARD_AGE_VALIDATION)\n",
        "def customers_us():\n",
        "    return spark.table(\"source\").filter(col(\"country\") == \"US\")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Testing DLT Pipelines\n",
        "\n",
        "Functional approaches to testing DLT expectations before deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing framework for DLT expectations\n",
        "\n",
        "def test_expectation(expectation: Expectation, test_data: DataFrame, \n",
        "                    expected_violations: int) -> bool:\n",
        "    \"\"\"\n",
        "    Pure function to test an expectation against test data.\n",
        "    Returns True if test passes, False otherwise.\n",
        "    \"\"\"\n",
        "    _, result = expectation.evaluate(test_data)\n",
        "    \n",
        "    passed = result.violated_records == expected_violations\n",
        "    \n",
        "    if passed:\n",
        "        print(f\"‚úÖ PASS: {expectation.name}\")\n",
        "        print(f\"   Expected {expected_violations} violations, got {result.violated_records}\")\n",
        "    else:\n",
        "        print(f\"‚ùå FAIL: {expectation.name}\")\n",
        "        print(f\"   Expected {expected_violations} violations, got {result.violated_records}\")\n",
        "    \n",
        "    return passed\n",
        "\n",
        "# Create test datasets\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING DLT EXPECTATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test 1: Valid age expectation\n",
        "print(\"\\nTest 1: Age validation expectation\")\n",
        "age_test_data = spark.createDataFrame([\n",
        "    (1, 25),   # Valid\n",
        "    (2, 30),   # Valid\n",
        "    (3, -5),   # Invalid - negative\n",
        "    (4, 150),  # Invalid - too high\n",
        "    (5, 45),   # Valid\n",
        "], [\"id\", \"age\"])\n",
        "\n",
        "age_expectation = Expectation(\n",
        "    name=\"valid_age_range\",\n",
        "    constraint=\"age >= 0 AND age <= 120\",\n",
        "    expectation_type=ExpectationType.DROP\n",
        ")\n",
        "\n",
        "test_expectation(age_expectation, age_test_data, expected_violations=2)\n",
        "\n",
        "# Test 2: Email format expectation\n",
        "print(\"\\nTest 2: Email format validation\")\n",
        "email_test_data = spark.createDataFrame([\n",
        "    (1, \"valid@example.com\"),      # Valid\n",
        "    (2, \"another@test.org\"),       # Valid\n",
        "    (3, \"invalid-email\"),           # Invalid\n",
        "    (4, \"missing-at-sign.com\"),    # Invalid\n",
        "    (5, \"user@domain.co.uk\"),      # Valid\n",
        "], [\"id\", \"email\"])\n",
        "\n",
        "email_expectation = Expectation(\n",
        "    name=\"valid_email_format\",\n",
        "    constraint=\"email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\\\\\.[a-zA-Z]{2,}$'\",\n",
        "    expectation_type=ExpectationType.WARN\n",
        ")\n",
        "\n",
        "test_expectation(email_expectation, email_test_data, expected_violations=2)\n",
        "\n",
        "# Test 3: Tier validation\n",
        "print(\"\\nTest 3: Tier value validation\")\n",
        "tier_test_data = spark.createDataFrame([\n",
        "    (1, \"Standard\"),     # Valid\n",
        "    (2, \"Premium\"),      # Valid\n",
        "    (3, \"InvalidTier\"),  # Invalid\n",
        "    (4, \"Standard\"),     # Valid\n",
        "    (5, None),           # Invalid\n",
        "], [\"id\", \"tier\"])\n",
        "\n",
        "tier_expectation = Expectation(\n",
        "    name=\"valid_tier\",\n",
        "    constraint=\"tier IN ('Standard', 'Premium')\",\n",
        "    expectation_type=ExpectationType.DROP\n",
        ")\n",
        "\n",
        "test_expectation(tier_expectation, tier_test_data, expected_violations=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ All expectation tests completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we explored declarative data quality with Delta Live Tables expectations:\n",
        "\n",
        "### Key Concepts Covered\n",
        "\n",
        "1. **DLT Expectations Framework**\n",
        "   - Three expectation types: WARN, DROP, FAIL\n",
        "   - Declarative quality rules defined as decorators\n",
        "   - Automatic quality metrics collection\n",
        "\n",
        "2. **Functional Programming Alignment**\n",
        "   - Declarative vs imperative approaches\n",
        "   - Pure functions for expectation evaluation\n",
        "   - Immutable expectation and result objects\n",
        "   - Composable quality rules\n",
        "\n",
        "3. **Quality Metrics and Monitoring**\n",
        "   - Automated violation tracking\n",
        "   - Quality reports and dashboards\n",
        "   - Alerting on critical failures\n",
        "\n",
        "4. **Layered Quality Strategy**\n",
        "   - Bronze: WARN expectations for monitoring\n",
        "   - Silver: DROP expectations for cleansing\n",
        "   - Gold: FAIL expectations for critical gates\n",
        "\n",
        "5. **Testing and Validation**\n",
        "   - Functional testing patterns for expectations\n",
        "   - Test data generation\n",
        "   - Expectation verification before deployment\n",
        "\n",
        "### Best Practices Demonstrated\n",
        "\n",
        "- ‚úÖ **Declarative Quality**: Define what quality means, not how to enforce it\n",
        "- ‚úÖ **Separation of Concerns**: Quality rules separate from transformation logic\n",
        "- ‚úÖ **Composability**: Reusable expectation patterns across tables\n",
        "- ‚úÖ **Observability**: Built-in quality metrics and monitoring\n",
        "- ‚úÖ **Testability**: Pure functions for expectation validation\n",
        "\n",
        "### Functional Programming Benefits\n",
        "\n",
        "- **Declarative**: Expectations describe desired quality state\n",
        "- **Immutable**: Expectations don't modify pipeline definitions\n",
        "- **Composable**: Stack multiple expectations on tables\n",
        "- **Testable**: Pure evaluation functions easy to test\n",
        "- **Maintainable**: Clear, self-documenting quality requirements\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Practice defining expectations for your data\n",
        "- Set up DLT pipelines in Databricks\n",
        "- Create quality monitoring dashboards\n",
        "- Implement automated quality alerts\n",
        "- Build reusable expectation libraries\n",
        "\n",
        "Delta Live Tables expectations provide a powerful, functional approach to data quality that aligns perfectly with modern data engineering best practices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "Practice implementing DLT-style expectations for your own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EXERCISES: Practice DLT Expectations\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "Exercise 1: Create Transaction Validation Expectations\n",
        "--------------------------------------------------------\n",
        "Create a set of expectations for a transaction dataset:\n",
        "- transaction_id must be unique and not null (FAIL)\n",
        "- amount must be positive (DROP)\n",
        "- transaction_date must be valid date format (DROP)\n",
        "- payment_method must be in approved list (DROP)\n",
        "- Monitor transaction amounts > $10,000 (WARN)\n",
        "\n",
        "Exercise 2: Implement Multi-Layer Quality Strategy\n",
        "---------------------------------------------------\n",
        "Design bronze/silver/gold expectations for product data:\n",
        "Bronze:\n",
        "- Monitor SKU format compliance\n",
        "- Track missing product descriptions\n",
        "\n",
        "Silver:\n",
        "- Drop products with invalid categories\n",
        "- Remove products with negative prices\n",
        "\n",
        "Gold:\n",
        "- Fail if required fields are null\n",
        "- Ensure price consistency across systems\n",
        "\n",
        "Exercise 3: Create Reusable Expectation Patterns\n",
        "-------------------------------------------------\n",
        "Build a library of reusable expectations:\n",
        "- Email validation pattern\n",
        "- Phone number validation pattern\n",
        "- Date range validation pattern\n",
        "- Amount range validation pattern\n",
        "\n",
        "Exercise 4: Implement Quality Monitoring\n",
        "-----------------------------------------\n",
        "Create a quality monitoring dashboard:\n",
        "- Track violation rates over time\n",
        "- Alert when drop rate exceeds threshold\n",
        "- Generate quality trend reports\n",
        "- Identify top quality issues\n",
        "\n",
        "Exercise 5: Test Your Expectations\n",
        "-----------------------------------\n",
        "Write comprehensive tests for your expectations:\n",
        "- Create test data with known violations\n",
        "- Verify expectation behavior\n",
        "- Test edge cases\n",
        "- Validate quality metrics\n",
        "\"\"\")\n",
        "\n",
        "# Exercise templates (implement these!)\n",
        "\n",
        "def create_transaction_expectations() -> List[Expectation]:\n",
        "    \"\"\"\n",
        "    YOUR TASK: Create expectations for transaction data\n",
        "    \"\"\"\n",
        "    # TODO: Implement transaction expectations\n",
        "    pass\n",
        "\n",
        "def create_product_expectations_bronze() -> List[Expectation]:\n",
        "    \"\"\"\n",
        "    YOUR TASK: Create bronze layer expectations for products\n",
        "    \"\"\"\n",
        "    # TODO: Implement bronze layer expectations\n",
        "    pass\n",
        "\n",
        "def create_reusable_email_expectation(column_name: str) -> Expectation:\n",
        "    \"\"\"\n",
        "    YOUR TASK: Create reusable email validation expectation\n",
        "    \"\"\"\n",
        "    # TODO: Implement reusable email expectation\n",
        "    pass\n",
        "\n",
        "print(\"\\nüìù Complete the exercises above to master DLT expectations!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
