{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.2 Declarative Data Quality with Delta Live Tables (DLT) Expectations\n",
        "\n",
        "This notebook demonstrates how to implement declarative data quality using Delta Live Tables (DLT) expectations in a functional programming approach. We'll explore how DLT expectations align with functional principles and provide robust, automated data quality management.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand how to:\n",
        "- Design functional data pipelines with DLT expectations\n",
        "- Implement declarative data quality rules using expectations\n",
        "- Create composable expectation patterns\n",
        "- Handle data quality violations with different strategies (warn, drop, fail)\n",
        "- Monitor and alert on data quality metrics\n",
        "- Test DLT pipelines with functional approaches\n",
        "- Compare declarative vs imperative data quality patterns\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Understanding of PySpark DataFrames\n",
        "- Knowledge of functional programming concepts\n",
        "- Familiarity with Delta Lake basics\n",
        "- Experience with data validation patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Delta Live Tables and Expectations\n",
        "\n",
        "Delta Live Tables (DLT) is a declarative framework for building reliable data pipelines on Databricks. DLT expectations provide automated data quality testing built directly into the pipeline definition.\n",
        "\n",
        "### Why DLT Expectations?\n",
        "\n",
        "**Traditional Imperative Approach:**\n",
        "```python\n",
        "# Scattered validation logic throughout code\n",
        "if df.filter(col(\"age\") < 0).count() > 0:\n",
        "    raise ValueError(\"Invalid ages found\")\n",
        "\n",
        "# Manual error handling and logging\n",
        "# Difficult to track quality metrics\n",
        "# Quality checks can be missed\n",
        "```\n",
        "\n",
        "**DLT Declarative Approach:**\n",
        "```python\n",
        "@dlt.expect(\"valid_age\", \"age >= 0\")\n",
        "@dlt.expect(\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+$'\")\n",
        "@dlt.table\n",
        "def customers():\n",
        "    return spark.read.table(\"source_customers\")\n",
        "```\n",
        "\n",
        "### Functional Programming Alignment\n",
        "\n",
        "DLT expectations embody functional programming principles:\n",
        "- **Declarative**: Define *what* quality rules should be enforced, not *how*\n",
        "- **Composable**: Stack multiple expectations on the same table\n",
        "- **Immutable**: Expectations don't modify the pipeline definition\n",
        "- **Side-effect isolation**: Quality actions (warn/drop/fail) are explicitly declared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports for DLT pipeline demonstration\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from typing import Dict, List, Tuple, Optional, Callable, Any\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "import json\n",
        "\n",
        "# Initialize Spark session (if not already available)\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"DLTExpectations\").getOrCreate()\n",
        "\n",
        "print(\"✅ Setup complete - Ready for DLT expectations demonstration!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. DLT Expectation Types and Strategies\n",
        "\n",
        "DLT provides three types of expectations with different violation handling strategies:\n",
        "\n",
        "| Expectation Type | Behavior | Use Case | Functional Impact |\n",
        "|-----------------|----------|----------|-------------------|\n",
        "| `@dlt.expect()` | **Warn** - Records violations in metrics | Monitoring non-critical quality | Pure observation, no data modification |\n",
        "| `@dlt.expect_or_drop()` | **Drop** - Removes violating records | Enforce quality constraints | Transforms data by filtering |\n",
        "| `@dlt.expect_or_fail()` | **Fail** - Stops pipeline execution | Critical quality gates | Halts execution on violation |\n",
        "\n",
        "### Conceptual DLT Pipeline Structure\n",
        "\n",
        "Since we're demonstrating concepts (DLT requires Databricks workspace setup), we'll create functional patterns that mirror DLT's declarative approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate DLT expectation framework with functional patterns\n",
        "\n",
        "class ExpectationType(Enum):\n",
        "    \"\"\"Types of expectation enforcement strategies\"\"\"\n",
        "    WARN = \"warn\"           # Record violation, continue processing\n",
        "    DROP = \"drop\"           # Remove violating records\n",
        "    FAIL = \"fail\"           # Stop pipeline on violation\n",
        "\n",
        "@dataclass\n",
        "class Expectation:\n",
        "    \"\"\"Immutable expectation definition\"\"\"\n",
        "    name: str\n",
        "    constraint: str\n",
        "    expectation_type: ExpectationType\n",
        "    \n",
        "    def evaluate(self, df: DataFrame) -> Tuple[DataFrame, 'ExpectationResult']:\n",
        "        \"\"\"\n",
        "        Pure function to evaluate expectation on DataFrame.\n",
        "        Returns tuple of (transformed_df, result) without side effects.\n",
        "        \"\"\"\n",
        "        total_count = df.count()\n",
        "        \n",
        "        # Count violating records\n",
        "        violating_count = df.filter(f\"NOT ({self.constraint})\").count()\n",
        "        valid_count = total_count - violating_count\n",
        "        \n",
        "        # Create result\n",
        "        result = ExpectationResult(\n",
        "            expectation_name=self.name,\n",
        "            constraint=self.constraint,\n",
        "            expectation_type=self.expectation_type,\n",
        "            total_records=total_count,\n",
        "            valid_records=valid_count,\n",
        "            violated_records=violating_count,\n",
        "            passed=violating_count == 0\n",
        "        )\n",
        "        \n",
        "        # Apply transformation based on expectation type\n",
        "        if self.expectation_type == ExpectationType.WARN:\n",
        "            # Return original DataFrame unchanged\n",
        "            return df, result\n",
        "        \n",
        "        elif self.expectation_type == ExpectationType.DROP:\n",
        "            # Return filtered DataFrame with violating records removed\n",
        "            filtered_df = df.filter(self.constraint)\n",
        "            return filtered_df, result\n",
        "        \n",
        "        elif self.expectation_type == ExpectationType.FAIL:\n",
        "            # Return DataFrame but signal failure in result\n",
        "            return df, result\n",
        "\n",
        "@dataclass\n",
        "class ExpectationResult:\n",
        "    \"\"\"Immutable expectation evaluation result\"\"\"\n",
        "    expectation_name: str\n",
        "    constraint: str\n",
        "    expectation_type: ExpectationType\n",
        "    total_records: int\n",
        "    valid_records: int\n",
        "    violated_records: int\n",
        "    passed: bool\n",
        "    \n",
        "    @property\n",
        "    def violation_rate(self) -> float:\n",
        "        \"\"\"Calculate percentage of records violating the expectation\"\"\"\n",
        "        return (self.violated_records / self.total_records * 100) if self.total_records > 0 else 0.0\n",
        "    \n",
        "    def __str__(self) -> str:\n",
        "        status = \"✅ PASSED\" if self.passed else \"❌ FAILED\"\n",
        "        return (f\"{status} - {self.expectation_name} ({self.expectation_type.value})\\n\"\n",
        "                f\"  Constraint: {self.constraint}\\n\"\n",
        "                f\"  Valid: {self.valid_records:,} / {self.total_records:,} \"\n",
        "                f\"({100-self.violation_rate:.1f}%)\\n\"\n",
        "                f\"  Violations: {self.violated_records:,} ({self.violation_rate:.1f}%)\")\n",
        "\n",
        "print(\"✅ Expectation framework classes defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creating Sample Data with Quality Issues\n",
        "\n",
        "Let's create realistic sample data that contains various data quality issues to demonstrate expectation handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_customer_data_with_quality_issues():\n",
        "    \"\"\"\n",
        "    Pure function to create customer data with intentional quality issues.\n",
        "    Demonstrates various types of data quality violations.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Mix of valid and invalid records\n",
        "    data = [\n",
        "        # Valid records\n",
        "        (1, \"Alice Johnson\", \"alice@example.com\", 28, \"2020-01-15\", \"Premium\", 1500.00, \"US\"),\n",
        "        (2, \"Bob Smith\", \"bob.smith@company.com\", 35, \"2019-06-20\", \"Standard\", 800.00, \"CA\"),\n",
        "        (3, \"Carol Davis\", \"carol.d@email.com\", 42, \"2021-03-10\", \"Premium\", 2000.00, \"UK\"),\n",
        "        (4, \"David Wilson\", \"david.w@example.org\", 31, \"2020-11-05\", \"Standard\", 950.00, \"US\"),\n",
        "        (5, \"Emma Brown\", \"emma.brown@mail.com\", 29, \"2022-01-18\", \"Premium\", 1800.00, \"AU\"),\n",
        "        \n",
        "        # Records with quality issues\n",
        "        (6, \"Frank Miller\", \"invalid-email\", 45, \"2021-07-22\", \"Standard\", 700.00, \"US\"),  # Invalid email\n",
        "        (7, \"Grace Lee\", \"grace@example.com\", -5, \"2020-09-15\", \"Premium\", 1200.00, \"CA\"),  # Negative age\n",
        "        (8, \"Henry Chen\", \"henry.chen@email.com\", 150, \"2019-12-01\", \"Standard\", 600.00, \"CN\"),  # Age > 120\n",
        "        (9, None, \"unknown@email.com\", 30, \"2021-04-10\", \"Standard\", 500.00, \"UK\"),  # Null name\n",
        "        (10, \"Isabel Garcia\", None, 33, \"2020-08-25\", \"Premium\", 1600.00, \"ES\"),  # Null email\n",
        "        (11, \"Jack Taylor\", \"jack@example.com\", 28, \"invalid-date\", \"Standard\", 850.00, \"US\"),  # Invalid date\n",
        "        (12, \"Kate Anderson\", \"kate@email.com\", 40, \"2021-02-14\", \"InvalidTier\", 1100.00, \"AU\"),  # Invalid tier\n",
        "        (13, \"Leo Martinez\", \"leo@example.com\", 35, \"2020-05-30\", \"Premium\", -500.00, \"MX\"),  # Negative balance\n",
        "        (14, \"Maria Rodriguez\", \"maria@email.com\", 38, \"2019-10-18\", \"Standard\", None, \"BR\"),  # Null balance\n",
        "        (15, \"Nathan White\", \"nathan@example.com\", 27, \"2022-03-22\", \"Premium\", 1700.00, None),  # Null country\n",
        "        \n",
        "        # More valid records\n",
        "        (16, \"Olivia Harris\", \"olivia.h@email.com\", 32, \"2021-11-08\", \"Standard\", 920.00, \"US\"),\n",
        "        (17, \"Paul Clark\", \"paul.clark@company.com\", 44, \"2020-04-17\", \"Premium\", 1950.00, \"CA\"),\n",
        "        (18, \"Quinn Lewis\", \"quinn@example.com\", 36, \"2021-08-29\", \"Standard\", 780.00, \"UK\"),\n",
        "    ]\n",
        "    \n",
        "    schema = StructType([\n",
        "        StructField(\"customer_id\", IntegerType(), False),\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"email\", StringType(), True),\n",
        "        StructField(\"age\", IntegerType(), True),\n",
        "        StructField(\"signup_date\", StringType(), True),\n",
        "        StructField(\"tier\", StringType(), True),\n",
        "        StructField(\"account_balance\", DoubleType(), True),\n",
        "        StructField(\"country\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    return spark.createDataFrame(data, schema)\n",
        "\n",
        "# Create sample data\n",
        "customers_df = create_customer_data_with_quality_issues()\n",
        "\n",
        "print(f\"Created customer dataset with {customers_df.count()} records\")\n",
        "print(\"\\nSample data (showing quality issues):\")\n",
        "customers_df.show(truncate=False)\n",
        "\n",
        "print(\"\\nData quality issues present:\")\n",
        "print(\"  ✗ Invalid email formats\")\n",
        "print(\"  ✗ Invalid ages (negative, > 120)\")\n",
        "print(\"  ✗ Null values in required fields\")\n",
        "print(\"  ✗ Invalid date formats\")\n",
        "print(\"  ✗ Invalid tier values\")\n",
        "print(\"  ✗ Negative account balances\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Implementing DLT-Style Expectations\n",
        "\n",
        "Let's create expectations for our customer data using the three different strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define expectations for customer data quality\n",
        "\n",
        "# WARN expectations - Monitor quality issues without blocking\n",
        "warn_expectations = [\n",
        "    Expectation(\n",
        "        name=\"valid_email_format\",\n",
        "        constraint=\"email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\\\\\.[a-zA-Z]{2,}$'\",\n",
        "        expectation_type=ExpectationType.WARN\n",
        "    ),\n",
        "    Expectation(\n",
        "        name=\"valid_signup_date\",\n",
        "        constraint=\"signup_date RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\",\n",
        "        expectation_type=ExpectationType.WARN\n",
        "    ),\n",
        "]\n",
        "\n",
        "# DROP expectations - Remove records that violate constraints\n",
        "drop_expectations = [\n",
        "    Expectation(\n",
        "        name=\"valid_age_range\",\n",
        "        constraint=\"age >= 0 AND age <= 120\",\n",
        "        expectation_type=ExpectationType.DROP\n",
        "    ),\n",
        "    Expectation(\n",
        "        name=\"valid_tier\",\n",
        "        constraint=\"tier IN ('Standard', 'Premium')\",\n",
        "        expectation_type=ExpectationType.DROP\n",
        "    ),\n",
        "    Expectation(\n",
        "        name=\"non_negative_balance\",\n",
        "        constraint=\"account_balance >= 0\",\n",
        "        expectation_type=ExpectationType.DROP\n",
        "    ),\n",
        "]\n",
        "\n",
        "# FAIL expectations - Critical quality gates that must pass\n",
        "fail_expectations = [\n",
        "    Expectation(\n",
        "        name=\"required_name\",\n",
        "        constraint=\"name IS NOT NULL AND length(name) > 0\",\n",
        "        expectation_type=ExpectationType.FAIL\n",
        "    ),\n",
        "    Expectation(\n",
        "        name=\"required_email\",\n",
        "        constraint=\"email IS NOT NULL AND length(email) > 0\",\n",
        "        expectation_type=ExpectationType.FAIL\n",
        "    ),\n",
        "]\n",
        "\n",
        "print(\"✅ Expectations defined:\")\n",
        "print(f\"  - WARN expectations: {len(warn_expectations)}\")\n",
        "print(f\"  - DROP expectations: {len(drop_expectations)}\")\n",
        "print(f\"  - FAIL expectations: {len(fail_expectations)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. DLT Pipeline Simulation\n",
        "\n",
        "Let's create a functional pipeline that applies expectations in the proper order: FAIL → DROP → WARN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DLTPipeline:\n",
        "    \"\"\"\n",
        "    Functional DLT pipeline that applies expectations declaratively.\n",
        "    Immutable and composable.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "        self.expectations: List[Expectation] = []\n",
        "        self.results: List[ExpectationResult] = []\n",
        "    \n",
        "    def add_expectation(self, expectation: Expectation) -> 'DLTPipeline':\n",
        "        \"\"\"Add an expectation to the pipeline (returns new pipeline)\"\"\"\n",
        "        new_pipeline = DLTPipeline(self.name)\n",
        "        new_pipeline.expectations = self.expectations + [expectation]\n",
        "        new_pipeline.results = self.results.copy()\n",
        "        return new_pipeline\n",
        "    \n",
        "    def add_expectations(self, expectations: List[Expectation]) -> 'DLTPipeline':\n",
        "        \"\"\"Add multiple expectations (returns new pipeline)\"\"\"\n",
        "        new_pipeline = DLTPipeline(self.name)\n",
        "        new_pipeline.expectations = self.expectations + expectations\n",
        "        new_pipeline.results = self.results.copy()\n",
        "        return new_pipeline\n",
        "    \n",
        "    def execute(self, df: DataFrame) -> Tuple[DataFrame, List[ExpectationResult]]:\n",
        "        \"\"\"\n",
        "        Execute pipeline with all expectations.\n",
        "        Pure function that returns transformed DataFrame and results.\n",
        "        \"\"\"\n",
        "        current_df = df\n",
        "        all_results = []\n",
        "        \n",
        "        # Sort expectations by type: FAIL first, then DROP, then WARN\n",
        "        expectation_order = {\n",
        "            ExpectationType.FAIL: 0,\n",
        "            ExpectationType.DROP: 1,\n",
        "            ExpectationType.WARN: 2\n",
        "        }\n",
        "        sorted_expectations = sorted(\n",
        "            self.expectations,\n",
        "            key=lambda e: expectation_order[e.expectation_type]\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n🔄 Executing DLT Pipeline: {self.name}\")\n",
        "        print(f\"Total expectations: {len(sorted_expectations)}\\n\")\n",
        "        \n",
        "        for expectation in sorted_expectations:\n",
        "            # Evaluate expectation\n",
        "            transformed_df, result = expectation.evaluate(current_df)\n",
        "            all_results.append(result)\n",
        "            \n",
        "            # Print result\n",
        "            print(f\"{'='*70}\")\n",
        "            print(result)\n",
        "            print()\n",
        "            \n",
        "            # Handle FAIL expectation\n",
        "            if expectation.expectation_type == ExpectationType.FAIL and not result.passed:\n",
        "                print(\"🚨 PIPELINE FAILED - Critical expectation violated!\")\n",
        "                print(f\"   Stopping execution at expectation: {expectation.name}\\n\")\n",
        "                return current_df, all_results\n",
        "            \n",
        "            # Update current DataFrame\n",
        "            current_df = transformed_df\n",
        "        \n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"✅ Pipeline completed successfully\")\n",
        "        print(f\"   Input records: {df.count():,}\")\n",
        "        print(f\"   Output records: {current_df.count():,}\")\n",
        "        print(f\"   Records dropped: {df.count() - current_df.count():,}\\n\")\n",
        "        \n",
        "        return current_df, all_results\n",
        "\n",
        "# Create and configure DLT pipeline\n",
        "customer_pipeline = (\n",
        "    DLTPipeline(\"customer_quality_pipeline\")\n",
        "    .add_expectations(fail_expectations)   # Critical gates first\n",
        "    .add_expectations(drop_expectations)   # Filter violations\n",
        "    .add_expectations(warn_expectations)   # Monitor quality\n",
        ")\n",
        "\n",
        "print(f\"✅ Pipeline configured with {len(customer_pipeline.expectations)} expectations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the DLT pipeline\n",
        "cleaned_customers_df, pipeline_results = customer_pipeline.execute(customers_df)\n",
        "\n",
        "# Show cleaned data\n",
        "print(\"\\n📊 Cleaned Customer Data:\")\n",
        "cleaned_customers_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Quality Metrics and Monitoring\n",
        "\n",
        "DLT expectations automatically generate quality metrics. Let's create a functional metrics reporting system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class QualityReport:\n",
        "    \"\"\"Immutable quality report aggregating expectation results\"\"\"\n",
        "    pipeline_name: str\n",
        "    results: List[ExpectationResult]\n",
        "    input_record_count: int\n",
        "    output_record_count: int\n",
        "    \n",
        "    @property\n",
        "    def total_expectations(self) -> int:\n",
        "        return len(self.results)\n",
        "    \n",
        "    @property\n",
        "    def passed_expectations(self) -> int:\n",
        "        return sum(1 for r in self.results if r.passed)\n",
        "    \n",
        "    @property\n",
        "    def failed_expectations(self) -> int:\n",
        "        return sum(1 for r in self.results if not r.passed)\n",
        "    \n",
        "    @property\n",
        "    def records_dropped(self) -> int:\n",
        "        return self.input_record_count - self.output_record_count\n",
        "    \n",
        "    @property\n",
        "    def drop_rate(self) -> float:\n",
        "        return (self.records_dropped / self.input_record_count * 100) if self.input_record_count > 0 else 0.0\n",
        "    \n",
        "    def get_results_by_type(self, expectation_type: ExpectationType) -> List[ExpectationResult]:\n",
        "        \"\"\"Filter results by expectation type\"\"\"\n",
        "        return [r for r in self.results if r.expectation_type == expectation_type]\n",
        "    \n",
        "    def print_summary(self):\n",
        "        \"\"\"Print comprehensive quality report\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"📊 DATA QUALITY REPORT: {self.pipeline_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        print(f\"\\n📈 Pipeline Statistics:\")\n",
        "        print(f\"  Input Records:     {self.input_record_count:,}\")\n",
        "        print(f\"  Output Records:    {self.output_record_count:,}\")\n",
        "        print(f\"  Records Dropped:   {self.records_dropped:,} ({self.drop_rate:.1f}%)\")\n",
        "        \n",
        "        print(f\"\\n✅ Expectation Summary:\")\n",
        "        print(f\"  Total Expectations: {self.total_expectations}\")\n",
        "        print(f\"  Passed:            {self.passed_expectations}\")\n",
        "        print(f\"  Failed:            {self.failed_expectations}\")\n",
        "        \n",
        "        # Break down by expectation type\n",
        "        for exp_type in ExpectationType:\n",
        "            type_results = self.get_results_by_type(exp_type)\n",
        "            if type_results:\n",
        "                passed = sum(1 for r in type_results if r.passed)\n",
        "                total = len(type_results)\n",
        "                print(f\"\\n  {exp_type.value.upper()} Expectations: {passed}/{total} passed\")\n",
        "                \n",
        "                for result in type_results:\n",
        "                    status = \"✅\" if result.passed else \"❌\"\n",
        "                    print(f\"    {status} {result.expectation_name}: \"\n",
        "                          f\"{result.violated_records:,} violations ({result.violation_rate:.1f}%)\")\n",
        "        \n",
        "        print(f\"\\n{'='*80}\\n\")\n",
        "    \n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert report to dictionary for JSON serialization\"\"\"\n",
        "        return {\n",
        "            \"pipeline_name\": self.pipeline_name,\n",
        "            \"input_record_count\": self.input_record_count,\n",
        "            \"output_record_count\": self.output_record_count,\n",
        "            \"records_dropped\": self.records_dropped,\n",
        "            \"drop_rate\": self.drop_rate,\n",
        "            \"total_expectations\": self.total_expectations,\n",
        "            \"passed_expectations\": self.passed_expectations,\n",
        "            \"failed_expectations\": self.failed_expectations,\n",
        "            \"expectations\": [\n",
        "                {\n",
        "                    \"name\": r.expectation_name,\n",
        "                    \"type\": r.expectation_type.value,\n",
        "                    \"constraint\": r.constraint,\n",
        "                    \"passed\": r.passed,\n",
        "                    \"total_records\": r.total_records,\n",
        "                    \"valid_records\": r.valid_records,\n",
        "                    \"violated_records\": r.violated_records,\n",
        "                    \"violation_rate\": r.violation_rate\n",
        "                }\n",
        "                for r in self.results\n",
        "            ]\n",
        "        }\n",
        "\n",
        "# Generate quality report\n",
        "quality_report = QualityReport(\n",
        "    pipeline_name=customer_pipeline.name,\n",
        "    results=pipeline_results,\n",
        "    input_record_count=customers_df.count(),\n",
        "    output_record_count=cleaned_customers_df.count()\n",
        ")\n",
        "\n",
        "quality_report.print_summary()\n",
        "\n",
        "# Export to JSON for monitoring systems\n",
        "print(\"📄 Quality Report (JSON format for monitoring):\")\n",
        "print(json.dumps(quality_report.to_dict(), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Actual DLT Pipeline Code Examples\n",
        "\n",
        "Here's how you would write real DLT pipelines in Databricks. These examples show the actual Python code you would use in a Databricks notebook configured as a DLT pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE: This code is for demonstration and would run in a Databricks DLT pipeline notebook\n",
        "# It will not execute in a standard notebook without DLT runtime\n",
        "\n",
        "# Example 1: Bronze Layer with WARN expectations\n",
        "# Monitor data quality issues without blocking ingestion\n",
        "\n",
        "\"\"\"\n",
        "import dlt\n",
        "from pyspark.sql.functions import col, to_date\n",
        "\n",
        "@dlt.table(\n",
        "    name=\"customers_bronze\",\n",
        "    comment=\"Raw customer data with quality monitoring\"\n",
        ")\n",
        "@dlt.expect(\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+$'\")\n",
        "@dlt.expect(\"valid_date_format\", \"signup_date RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\")\n",
        "def customers_bronze():\n",
        "    return (\n",
        "        spark.readStream\n",
        "        .format(\"cloudFiles\")\n",
        "        .option(\"cloudFiles.format\", \"json\")\n",
        "        .load(\"/mnt/source/customers/\")\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example 1: Bronze layer with WARN expectations\")\n",
        "print(\"  - Monitors email format quality\")\n",
        "print(\"  - Monitors date format compliance\")\n",
        "print(\"  - Records metrics but doesn't drop records\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Silver Layer with DROP expectations\n",
        "# Remove records that violate business rules\n",
        "\n",
        "\"\"\"\n",
        "@dlt.table(\n",
        "    name=\"customers_silver\",\n",
        "    comment=\"Cleaned customer data with enforced quality constraints\"\n",
        ")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")\n",
        "@dlt.expect_or_drop(\"valid_tier\", \"tier IN ('Standard', 'Premium', 'Enterprise')\")\n",
        "@dlt.expect_or_drop(\"positive_balance\", \"account_balance >= 0\")\n",
        "@dlt.expect_or_drop(\"valid_country\", \"country IS NOT NULL AND length(country) = 2\")\n",
        "def customers_silver():\n",
        "    return (\n",
        "        dlt.read_stream(\"customers_bronze\")\n",
        "        .select(\n",
        "            \"customer_id\",\n",
        "            \"name\",\n",
        "            \"email\",\n",
        "            col(\"age\").cast(\"int\"),\n",
        "            to_date(\"signup_date\").alias(\"signup_date\"),\n",
        "            \"tier\",\n",
        "            col(\"account_balance\").cast(\"double\"),\n",
        "            \"country\"\n",
        "        )\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example 2: Silver layer with DROP expectations\")\n",
        "print(\"  - Enforces age range (18-120)\")\n",
        "print(\"  - Validates tier values\")\n",
        "print(\"  - Ensures positive account balance\")\n",
        "print(\"  - Validates country code format\")\n",
        "print(\"  - Drops records that violate any constraint\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Gold Layer with FAIL expectations\n",
        "# Critical quality gates for business-critical tables\n",
        "\n",
        "\"\"\"\n",
        "@dlt.table(\n",
        "    name=\"customers_gold\",\n",
        "    comment=\"Production-ready customer data with strict quality guarantees\"\n",
        ")\n",
        "@dlt.expect_or_fail(\"no_nulls_in_key_fields\", \n",
        "                     \"customer_id IS NOT NULL AND name IS NOT NULL AND email IS NOT NULL\")\n",
        "@dlt.expect_or_fail(\"unique_customer_id\", \"customer_id IS NOT NULL\")\n",
        "@dlt.expect(\"high_quality_data\", \"age BETWEEN 18 AND 120\")\n",
        "def customers_gold():\n",
        "    return (\n",
        "        dlt.read(\"customers_silver\")\n",
        "        .groupBy(\"customer_id\")  # Ensure uniqueness\n",
        "        .agg(\n",
        "            F.first(\"name\").alias(\"name\"),\n",
        "            F.first(\"email\").alias(\"email\"),\n",
        "            F.first(\"age\").alias(\"age\"),\n",
        "            F.first(\"signup_date\").alias(\"signup_date\"),\n",
        "            F.first(\"tier\").alias(\"tier\"),\n",
        "            F.sum(\"account_balance\").alias(\"total_balance\"),\n",
        "            F.first(\"country\").alias(\"country\")\n",
        "        )\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example 3: Gold layer with FAIL expectations\")\n",
        "print(\"  - Fails pipeline if key fields are null\")\n",
        "print(\"  - Ensures customer_id uniqueness\")\n",
        "print(\"  - Monitors (but doesn't fail on) age quality\")\n",
        "print(\"  - Stops pipeline execution on critical violations\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Advanced DLT patterns with custom Python functions\n",
        "\n",
        "\"\"\"\n",
        "# Custom validation function (pure function)\n",
        "def is_valid_email_domain(email: str, allowed_domains: List[str]) -> bool:\n",
        "    \\\"\\\"\\\"Pure function to validate email domain\\\"\\\"\\\"\n",
        "    if not email or '@' not in email:\n",
        "        return False\n",
        "    domain = email.split('@')[1]\n",
        "    return domain in allowed_domains\n",
        "\n",
        "# Register as UDF for use in SQL expressions\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import BooleanType\n",
        "\n",
        "is_valid_email_domain_udf = udf(is_valid_email_domain, BooleanType())\n",
        "\n",
        "@dlt.table(\n",
        "    name=\"customers_with_domain_validation\",\n",
        "    comment=\"Customers with domain-specific validation\"\n",
        ")\n",
        "@dlt.expect_or_drop(\n",
        "    \"approved_email_domain\",\n",
        "    \"is_valid_email_domain(email, array('example.com', 'company.com', 'email.com'))\"\n",
        ")\n",
        "def customers_with_domain_validation():\n",
        "    return (\n",
        "        dlt.read(\"customers_silver\")\n",
        "        .filter(col(\"tier\") == \"Premium\")  # Only Premium customers need domain validation\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example 4: Advanced patterns with custom functions\")\n",
        "print(\"  - Uses custom Python functions for complex validation\")\n",
        "print(\"  - Combines functional programming with DLT expectations\")\n",
        "print(\"  - Demonstrates composable validation logic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comparing Imperative vs Declarative Approaches\n",
        "\n",
        "Let's contrast imperative data quality checks with DLT's declarative approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"IMPERATIVE vs DECLARATIVE DATA QUALITY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n❌ IMPERATIVE APPROACH (Anti-Pattern):\")\n",
        "print(\"\"\"\n",
        "def process_customers_imperative(df):\n",
        "    # Scattered validation logic with side effects\n",
        "    \n",
        "    # Manual null checks\n",
        "    null_names = df.filter(col(\"name\").isNull()).count()\n",
        "    if null_names > 0:\n",
        "        print(f\"WARNING: {null_names} records with null names\")\n",
        "        df = df.filter(col(\"name\").isNotNull())\n",
        "    \n",
        "    # Manual age validation\n",
        "    invalid_ages = df.filter((col(\"age\") < 0) | (col(\"age\") > 120)).count()\n",
        "    if invalid_ages > 0:\n",
        "        print(f\"WARNING: {invalid_ages} records with invalid ages\")\n",
        "        df = df.filter((col(\"age\") >= 0) & (col(\"age\") <= 120))\n",
        "    \n",
        "    # Manual email validation\n",
        "    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'\n",
        "    invalid_emails = df.filter(~col(\"email\").rlike(email_pattern)).count()\n",
        "    if invalid_emails > 0:\n",
        "        print(f\"WARNING: {invalid_emails} records with invalid emails\")\n",
        "        # Maybe drop, maybe keep - inconsistent handling\n",
        "    \n",
        "    # Manual tier validation\n",
        "    df = df.filter(col(\"tier\").isin(['Standard', 'Premium']))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Issues:\n",
        "# 1. Side effects (printing) mixed with transformation logic\n",
        "# 2. Inconsistent error handling (some drop, some warn)\n",
        "# 3. No centralized quality metrics\n",
        "# 4. Hard to test individual validations\n",
        "# 5. Validation logic scattered throughout code\n",
        "# 6. No automatic quality monitoring\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n✅ DECLARATIVE APPROACH (Best Practice):\")\n",
        "print(\"\"\"\n",
        "@dlt.table(name=\"customers_clean\")\n",
        "@dlt.expect_or_fail(\"valid_name\", \"name IS NOT NULL\")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 0 AND age <= 120\")\n",
        "@dlt.expect(\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+$'\")\n",
        "@dlt.expect_or_drop(\"valid_tier\", \"tier IN ('Standard', 'Premium')\")\n",
        "def customers_clean():\n",
        "    return spark.table(\"customers_source\")\n",
        "\n",
        "# Benefits:\n",
        "# 1. Clear separation of concerns (expectations vs transformations)\n",
        "# 2. Consistent, explicit error handling strategy\n",
        "# 3. Automatic quality metrics collection\n",
        "# 4. Easy to test and reason about\n",
        "# 5. Self-documenting quality requirements\n",
        "# 6. Built-in monitoring and alerting\n",
        "# 7. Composable and reusable\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n📊 Key Differences:\")\n",
        "print(\"\"\"\n",
        "┌─────────────────────┬──────────────────────────┬───────────────────────────┐\n",
        "│ Aspect              │ Imperative               │ Declarative (DLT)         │\n",
        "├─────────────────────┼──────────────────────────┼───────────────────────────┤\n",
        "│ Code Style          │ How to validate          │ What to validate          │\n",
        "│ Side Effects        │ Mixed with logic         │ Isolated in decorators    │\n",
        "│ Quality Metrics     │ Manual tracking          │ Automatic collection      │\n",
        "│ Consistency         │ Varies by implementation │ Standardized              │\n",
        "│ Testability         │ Difficult                │ Easy                      │\n",
        "│ Monitoring          │ Custom implementation    │ Built-in                  │\n",
        "│ Composability       │ Low                      │ High                      │\n",
        "│ Maintainability     │ Low                      │ High                      │\n",
        "└─────────────────────┴──────────────────────────┴───────────────────────────┘\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Best Practices and Patterns\n",
        "\n",
        "Guidelines for effective use of DLT expectations in production pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"DLT EXPECTATIONS BEST PRACTICES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "1. ✅ CHOOSE THE RIGHT EXPECTATION TYPE\n",
        "\n",
        "   @dlt.expect() - WARN\n",
        "   • Use for: Monitoring data quality trends\n",
        "   • Example: Email format compliance, date format standardization\n",
        "   • Benefit: Visibility without blocking data flow\n",
        "   \n",
        "   @dlt.expect_or_drop() - DROP\n",
        "   • Use for: Business rule enforcement\n",
        "   • Example: Valid age ranges, approved categories, positive amounts\n",
        "   • Benefit: Clean data without pipeline failures\n",
        "   \n",
        "   @dlt.expect_or_fail() - FAIL\n",
        "   • Use for: Critical quality gates\n",
        "   • Example: Required fields, data corruption detection\n",
        "   • Benefit: Prevent bad data from reaching production\n",
        "\n",
        "2. ✅ LAYER YOUR EXPECTATIONS\n",
        "\n",
        "   Bronze Layer:\n",
        "   • Use mostly WARN expectations\n",
        "   • Monitor raw data quality issues\n",
        "   • Maintain complete audit trail\n",
        "   \n",
        "   Silver Layer:\n",
        "   • Use DROP expectations for business rules\n",
        "   • Apply data cleansing and standardization\n",
        "   • Create clean, validated datasets\n",
        "   \n",
        "   Gold Layer:\n",
        "   • Use FAIL expectations for critical constraints\n",
        "   • Enforce strict quality for business-critical tables\n",
        "   • Ensure production-ready data quality\n",
        "\n",
        "3. ✅ WRITE CLEAR, TESTABLE CONSTRAINTS\n",
        "\n",
        "   Good:\n",
        "   @dlt.expect(\"positive_amount\", \"amount > 0\")\n",
        "   @dlt.expect(\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+$'\")\n",
        "   \n",
        "   Better:\n",
        "   # More descriptive names and comprehensive checks\n",
        "   @dlt.expect(\"transaction_amount_positive\", \"amount > 0 AND amount < 1000000\")\n",
        "   @dlt.expect(\"customer_email_format_valid\", \n",
        "               \"email IS NOT NULL AND email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'\")\n",
        "\n",
        "4. ✅ USE MEANINGFUL EXPECTATION NAMES\n",
        "\n",
        "   Pattern: [entity]_[field]_[validation_type]\n",
        "   \n",
        "   Good names:\n",
        "   • customer_age_within_valid_range\n",
        "   • order_total_positive_value\n",
        "   • product_sku_unique_identifier\n",
        "   • transaction_date_not_future\n",
        "   \n",
        "   Avoid:\n",
        "   • check1, validation_rule_2\n",
        "   • test, verify\n",
        "   • age_check (too vague)\n",
        "\n",
        "5. ✅ COMPOSE EXPECTATIONS FUNCTIONALLY\n",
        "\n",
        "   # Create reusable expectation configurations\n",
        "   def standard_customer_expectations():\n",
        "       return [\n",
        "           (\"valid_age\", \"age >= 18 AND age <= 120\", \"drop\"),\n",
        "           (\"valid_email\", \"email IS NOT NULL\", \"fail\"),\n",
        "           (\"valid_tier\", \"tier IN ('Standard', 'Premium')\", \"drop\"),\n",
        "       ]\n",
        "   \n",
        "   # Apply to multiple tables\n",
        "   @dlt.table(name=\"us_customers\")\n",
        "   @apply_expectations(standard_customer_expectations())\n",
        "   def us_customers():\n",
        "       return spark.table(\"customers\").filter(col(\"country\") == \"US\")\n",
        "\n",
        "6. ✅ MONITOR AND ALERT ON QUALITY METRICS\n",
        "\n",
        "   • Set up dashboards for expectation violations\n",
        "   • Configure alerts for critical expectation failures\n",
        "   • Track quality trends over time\n",
        "   • Review violation patterns regularly\n",
        "   • Use metrics to improve upstream data sources\n",
        "\n",
        "7. ✅ TEST EXPECTATIONS BEFORE PRODUCTION\n",
        "\n",
        "   • Use sample data to validate expectation logic\n",
        "   • Test with both valid and invalid records\n",
        "   • Verify expectation names and constraints\n",
        "   • Ensure appropriate expectation types\n",
        "   • Check drop rates are reasonable\n",
        "\n",
        "8. ✅ DOCUMENT BUSINESS RULES IN EXPECTATIONS\n",
        "\n",
        "   @dlt.table(\n",
        "       name=\"customers\",\n",
        "       comment=\"Customer master table with enforced quality standards\"\n",
        "   )\n",
        "   @dlt.expect_or_drop(\n",
        "       \"customer_age_legal_minimum\",\n",
        "       \"age >= 18\",\n",
        "       \"Customers must be 18 or older per legal requirements\"\n",
        "   )\n",
        "   def customers():\n",
        "       return spark.table(\"source_customers\")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Anti-Patterns to Avoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"DLT EXPECTATIONS ANTI-PATTERNS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "❌ 1. OVERLY COMPLEX EXPECTATIONS\n",
        "\n",
        "Bad:\n",
        "@dlt.expect(\"complex_validation\",\n",
        "           \"(age >= 18 AND tier = 'Premium' AND balance > 1000) OR \n",
        "            (age >= 21 AND tier = 'Standard' AND balance > 500) OR \n",
        "            (country IN ('US', 'CA') AND age >= 16)\")\n",
        "\n",
        "Better:\n",
        "# Break into separate, testable expectations\n",
        "@dlt.expect_or_drop(\"minimum_age\", \"age >= 16\")\n",
        "@dlt.expect_or_drop(\"premium_requirements\", \n",
        "                     \"tier != 'Premium' OR (age >= 18 AND balance > 1000)\")\n",
        "@dlt.expect_or_drop(\"standard_requirements\",\n",
        "                     \"tier != 'Standard' OR (age >= 21 AND balance > 500)\")\n",
        "\n",
        "❌ 2. USING WRONG EXPECTATION TYPE\n",
        "\n",
        "Bad:\n",
        "# Using FAIL for non-critical quality monitoring\n",
        "@dlt.expect_or_fail(\"preferred_email_domain\", \n",
        "                     \"email LIKE '%@company.com'\")\n",
        "\n",
        "Better:\n",
        "# Use WARN for monitoring, not enforcement\n",
        "@dlt.expect(\"preferred_email_domain\",\n",
        "           \"email LIKE '%@company.com'\")\n",
        "\n",
        "❌ 3. MIXING VALIDATION WITH TRANSFORMATION\n",
        "\n",
        "Bad:\n",
        "@dlt.table(name=\"customers\")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18\")\n",
        "def customers():\n",
        "    # Don't mix validation logic in the transformation\n",
        "    df = spark.table(\"source\")\n",
        "    df = df.filter(col(\"country\") == \"US\")  # ❌ Buried business logic\n",
        "    df = df.filter(col(\"status\") == \"active\")  # ❌ Hard to track\n",
        "    return df\n",
        "\n",
        "Better:\n",
        "@dlt.table(name=\"customers\")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18\")\n",
        "@dlt.expect_or_drop(\"us_customers_only\", \"country = 'US'\")\n",
        "@dlt.expect_or_drop(\"active_customers_only\", \"status = 'active'\")\n",
        "def customers():\n",
        "    # Pure transformation, validation in expectations\n",
        "    return spark.table(\"source\")\n",
        "\n",
        "❌ 4. NO MONITORING OR ALERTING\n",
        "\n",
        "Bad:\n",
        "# Define expectations but never check the results\n",
        "@dlt.expect(\"quality_check\", \"amount > 0\")\n",
        "# No dashboard, no alerts, no review process\n",
        "\n",
        "Better:\n",
        "# Set up comprehensive monitoring\n",
        "# - Create quality dashboards\n",
        "# - Configure alerts for critical violations\n",
        "# - Regular review of quality metrics\n",
        "# - Automated reports on data quality trends\n",
        "\n",
        "❌ 5. VAGUE EXPECTATION NAMES\n",
        "\n",
        "Bad:\n",
        "@dlt.expect(\"check1\", \"age > 0\")\n",
        "@dlt.expect(\"validation\", \"email IS NOT NULL\")\n",
        "@dlt.expect(\"test\", \"balance >= 0\")\n",
        "\n",
        "Better:\n",
        "@dlt.expect(\"customer_age_positive\", \"age > 0\")\n",
        "@dlt.expect(\"customer_email_required\", \"email IS NOT NULL\")\n",
        "@dlt.expect(\"account_balance_non_negative\", \"balance >= 0\")\n",
        "\n",
        "❌ 6. IGNORING DROPPED RECORDS\n",
        "\n",
        "Bad:\n",
        "@dlt.expect_or_drop(\"valid_data\", \"complex_condition\")\n",
        "# Never check how many records are being dropped\n",
        "# Could be silently losing important data\n",
        "\n",
        "Better:\n",
        "# Monitor drop rates\n",
        "# Alert if drop rate exceeds threshold (e.g., >5%)\n",
        "# Investigate root causes of violations\n",
        "# Fix upstream data issues\n",
        "\n",
        "❌ 7. DUPLICATE VALIDATION LOGIC\n",
        "\n",
        "Bad:\n",
        "# Same validation in multiple places\n",
        "@dlt.table(name=\"customers_us\")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")\n",
        "def customers_us():\n",
        "    return spark.table(\"source\").filter(col(\"country\") == \"US\")\n",
        "\n",
        "@dlt.table(name=\"customers_ca\")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")  # ❌ Duplicate\n",
        "def customers_ca():\n",
        "    return spark.table(\"source\").filter(col(\"country\") == \"CA\")\n",
        "\n",
        "Better:\n",
        "# Create reusable expectation definitions\n",
        "STANDARD_AGE_VALIDATION = (\"valid_age\", \"age >= 18 AND age <= 120\")\n",
        "\n",
        "@dlt.table(name=\"customers_us\")\n",
        "@dlt.expect_or_drop(*STANDARD_AGE_VALIDATION)\n",
        "def customers_us():\n",
        "    return spark.table(\"source\").filter(col(\"country\") == \"US\")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Testing DLT Pipelines\n",
        "\n",
        "Functional approaches to testing DLT expectations before deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing framework for DLT expectations\n",
        "\n",
        "def test_expectation(expectation: Expectation, test_data: DataFrame, \n",
        "                    expected_violations: int) -> bool:\n",
        "    \"\"\"\n",
        "    Pure function to test an expectation against test data.\n",
        "    Returns True if test passes, False otherwise.\n",
        "    \"\"\"\n",
        "    _, result = expectation.evaluate(test_data)\n",
        "    \n",
        "    passed = result.violated_records == expected_violations\n",
        "    \n",
        "    if passed:\n",
        "        print(f\"✅ PASS: {expectation.name}\")\n",
        "        print(f\"   Expected {expected_violations} violations, got {result.violated_records}\")\n",
        "    else:\n",
        "        print(f\"❌ FAIL: {expectation.name}\")\n",
        "        print(f\"   Expected {expected_violations} violations, got {result.violated_records}\")\n",
        "    \n",
        "    return passed\n",
        "\n",
        "# Create test datasets\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING DLT EXPECTATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test 1: Valid age expectation\n",
        "print(\"\\nTest 1: Age validation expectation\")\n",
        "age_test_data = spark.createDataFrame([\n",
        "    (1, 25),   # Valid\n",
        "    (2, 30),   # Valid\n",
        "    (3, -5),   # Invalid - negative\n",
        "    (4, 150),  # Invalid - too high\n",
        "    (5, 45),   # Valid\n",
        "], [\"id\", \"age\"])\n",
        "\n",
        "age_expectation = Expectation(\n",
        "    name=\"valid_age_range\",\n",
        "    constraint=\"age >= 0 AND age <= 120\",\n",
        "    expectation_type=ExpectationType.DROP\n",
        ")\n",
        "\n",
        "test_expectation(age_expectation, age_test_data, expected_violations=2)\n",
        "\n",
        "# Test 2: Email format expectation\n",
        "print(\"\\nTest 2: Email format validation\")\n",
        "email_test_data = spark.createDataFrame([\n",
        "    (1, \"valid@example.com\"),      # Valid\n",
        "    (2, \"another@test.org\"),       # Valid\n",
        "    (3, \"invalid-email\"),           # Invalid\n",
        "    (4, \"missing-at-sign.com\"),    # Invalid\n",
        "    (5, \"user@domain.co.uk\"),      # Valid\n",
        "], [\"id\", \"email\"])\n",
        "\n",
        "email_expectation = Expectation(\n",
        "    name=\"valid_email_format\",\n",
        "    constraint=\"email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\\\\\.[a-zA-Z]{2,}$'\",\n",
        "    expectation_type=ExpectationType.WARN\n",
        ")\n",
        "\n",
        "test_expectation(email_expectation, email_test_data, expected_violations=2)\n",
        "\n",
        "# Test 3: Tier validation\n",
        "print(\"\\nTest 3: Tier value validation\")\n",
        "tier_test_data = spark.createDataFrame([\n",
        "    (1, \"Standard\"),     # Valid\n",
        "    (2, \"Premium\"),      # Valid\n",
        "    (3, \"InvalidTier\"),  # Invalid\n",
        "    (4, \"Standard\"),     # Valid\n",
        "    (5, None),           # Invalid\n",
        "], [\"id\", \"tier\"])\n",
        "\n",
        "tier_expectation = Expectation(\n",
        "    name=\"valid_tier\",\n",
        "    constraint=\"tier IN ('Standard', 'Premium')\",\n",
        "    expectation_type=ExpectationType.DROP\n",
        ")\n",
        "\n",
        "test_expectation(tier_expectation, tier_test_data, expected_violations=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ All expectation tests completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we explored declarative data quality with Delta Live Tables expectations:\n",
        "\n",
        "### Key Concepts Covered\n",
        "\n",
        "1. **DLT Expectations Framework**\n",
        "   - Three expectation types: WARN, DROP, FAIL\n",
        "   - Declarative quality rules defined as decorators\n",
        "   - Automatic quality metrics collection\n",
        "\n",
        "2. **Functional Programming Alignment**\n",
        "   - Declarative vs imperative approaches\n",
        "   - Pure functions for expectation evaluation\n",
        "   - Immutable expectation and result objects\n",
        "   - Composable quality rules\n",
        "\n",
        "3. **Quality Metrics and Monitoring**\n",
        "   - Automated violation tracking\n",
        "   - Quality reports and dashboards\n",
        "   - Alerting on critical failures\n",
        "\n",
        "4. **Layered Quality Strategy**\n",
        "   - Bronze: WARN expectations for monitoring\n",
        "   - Silver: DROP expectations for cleansing\n",
        "   - Gold: FAIL expectations for critical gates\n",
        "\n",
        "5. **Testing and Validation**\n",
        "   - Functional testing patterns for expectations\n",
        "   - Test data generation\n",
        "   - Expectation verification before deployment\n",
        "\n",
        "### Best Practices Demonstrated\n",
        "\n",
        "- ✅ **Declarative Quality**: Define what quality means, not how to enforce it\n",
        "- ✅ **Separation of Concerns**: Quality rules separate from transformation logic\n",
        "- ✅ **Composability**: Reusable expectation patterns across tables\n",
        "- ✅ **Observability**: Built-in quality metrics and monitoring\n",
        "- ✅ **Testability**: Pure functions for expectation validation\n",
        "\n",
        "### Functional Programming Benefits\n",
        "\n",
        "- **Declarative**: Expectations describe desired quality state\n",
        "- **Immutable**: Expectations don't modify pipeline definitions\n",
        "- **Composable**: Stack multiple expectations on tables\n",
        "- **Testable**: Pure evaluation functions easy to test\n",
        "- **Maintainable**: Clear, self-documenting quality requirements\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Practice defining expectations for your data\n",
        "- Set up DLT pipelines in Databricks\n",
        "- Create quality monitoring dashboards\n",
        "- Implement automated quality alerts\n",
        "- Build reusable expectation libraries\n",
        "\n",
        "Delta Live Tables expectations provide a powerful, functional approach to data quality that aligns perfectly with modern data engineering best practices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "Practice implementing DLT-style expectations for your own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EXERCISES: Practice DLT Expectations\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "Exercise 1: Create Transaction Validation Expectations\n",
        "--------------------------------------------------------\n",
        "Create a set of expectations for a transaction dataset:\n",
        "- transaction_id must be unique and not null (FAIL)\n",
        "- amount must be positive (DROP)\n",
        "- transaction_date must be valid date format (DROP)\n",
        "- payment_method must be in approved list (DROP)\n",
        "- Monitor transaction amounts > $10,000 (WARN)\n",
        "\n",
        "Exercise 2: Implement Multi-Layer Quality Strategy\n",
        "---------------------------------------------------\n",
        "Design bronze/silver/gold expectations for product data:\n",
        "Bronze:\n",
        "- Monitor SKU format compliance\n",
        "- Track missing product descriptions\n",
        "\n",
        "Silver:\n",
        "- Drop products with invalid categories\n",
        "- Remove products with negative prices\n",
        "\n",
        "Gold:\n",
        "- Fail if required fields are null\n",
        "- Ensure price consistency across systems\n",
        "\n",
        "Exercise 3: Create Reusable Expectation Patterns\n",
        "-------------------------------------------------\n",
        "Build a library of reusable expectations:\n",
        "- Email validation pattern\n",
        "- Phone number validation pattern\n",
        "- Date range validation pattern\n",
        "- Amount range validation pattern\n",
        "\n",
        "Exercise 4: Implement Quality Monitoring\n",
        "-----------------------------------------\n",
        "Create a quality monitoring dashboard:\n",
        "- Track violation rates over time\n",
        "- Alert when drop rate exceeds threshold\n",
        "- Generate quality trend reports\n",
        "- Identify top quality issues\n",
        "\n",
        "Exercise 5: Test Your Expectations\n",
        "-----------------------------------\n",
        "Write comprehensive tests for your expectations:\n",
        "- Create test data with known violations\n",
        "- Verify expectation behavior\n",
        "- Test edge cases\n",
        "- Validate quality metrics\n",
        "\"\"\")\n",
        "\n",
        "# Exercise templates (implement these!)\n",
        "\n",
        "def create_transaction_expectations() -> List[Expectation]:\n",
        "    \"\"\"\n",
        "    YOUR TASK: Create expectations for transaction data\n",
        "    \"\"\"\n",
        "    # TODO: Implement transaction expectations\n",
        "    pass\n",
        "\n",
        "def create_product_expectations_bronze() -> List[Expectation]:\n",
        "    \"\"\"\n",
        "    YOUR TASK: Create bronze layer expectations for products\n",
        "    \"\"\"\n",
        "    # TODO: Implement bronze layer expectations\n",
        "    pass\n",
        "\n",
        "def create_reusable_email_expectation(column_name: str) -> Expectation:\n",
        "    \"\"\"\n",
        "    YOUR TASK: Create reusable email validation expectation\n",
        "    \"\"\"\n",
        "    # TODO: Implement reusable email expectation\n",
        "    pass\n",
        "\n",
        "print(\"\\n📝 Complete the exercises above to master DLT expectations!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
