{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Strategic Data Handling: Caching, Broadcast Joins, and Efficient Formats\n",
    "\n",
    "This notebook demonstrates strategic data handling techniques for optimizing PySpark performance while maintaining functional programming principles.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand when and how to use caching effectively\n",
    "- Master broadcast joins for performance optimization\n",
    "- Choose optimal file formats for different use cases\n",
    "- Apply strategic caching patterns in functional pipelines\n",
    "- Balance performance optimization with functional purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Spark Caching Strategies\n",
    "\n",
    "Caching is a controlled departure from pure statelessness that can dramatically improve performance for iterative algorithms and data reuse patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating large sales dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m spark.createDataFrame(data, schema)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating large sales dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m sales_df = \u001b[43mgenerate_large_sales_data\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 50K records for demo\u001b[39;00m\n\u001b[32m     42\u001b[39m sales_df = sales_df.withColumn(\u001b[33m\"\u001b[39m\u001b[33msale_date\u001b[39m\u001b[33m\"\u001b[39m, F.to_date(\u001b[33m\"\u001b[39m\u001b[33msale_date\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msales_df.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sales records\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mgenerate_large_sales_data\u001b[39m\u001b[34m(num_records)\u001b[39m\n\u001b[32m     16\u001b[39m     data.append((\n\u001b[32m     17\u001b[39m         i + \u001b[32m1\u001b[39m,  \u001b[38;5;66;03m# transaction_id\u001b[39;00m\n\u001b[32m     18\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCustomer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom.randint(\u001b[32m1\u001b[39m,\u001b[38;5;250m \u001b[39mnum_records\u001b[38;5;250m \u001b[39m//\u001b[38;5;250m \u001b[39m\u001b[32m10\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# customer_id\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m2023-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom.randint(\u001b[32m1\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m12\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom.randint(\u001b[32m1\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m28\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# sale_date\u001b[39;00m\n\u001b[32m     25\u001b[39m     ))\n\u001b[32m     27\u001b[39m schema = StructType([\n\u001b[32m     28\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mtransaction_id\u001b[39m\u001b[33m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     29\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33msale_date\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     36\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m.createDataFrame(data, schema)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Create larger dataset for meaningful caching demonstrations\n",
    "def generate_large_sales_data(num_records=100000):\n",
    "    \"\"\"Generate large sales dataset for caching demonstrations\"\"\"\n",
    "    \n",
    "    categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports', 'Beauty', 'Automotive']\n",
    "    regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "    \n",
    "    data = []\n",
    "    for i in range(num_records):\n",
    "        data.append((\n",
    "            i + 1,  # transaction_id\n",
    "            f\"Customer_{random.randint(1, num_records // 10)}\",  # customer_id\n",
    "            f\"Product_{random.randint(1, 1000)}\",  # product_name\n",
    "            random.choice(categories),  # category\n",
    "            random.choice(regions),  # region\n",
    "            round(random.uniform(10, 2000), 2),  # amount\n",
    "            random.randint(1, 5),  # quantity\n",
    "            f\"2023-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}\"  # sale_date\n",
    "        ))\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"transaction_id\", IntegerType(), False),\n",
    "        StructField(\"customer_id\", StringType(), False),\n",
    "        StructField(\"product_name\", StringType(), False),\n",
    "        StructField(\"category\", StringType(), False),\n",
    "        StructField(\"region\", StringType(), False),\n",
    "        StructField(\"amount\", DoubleType(), False),\n",
    "        StructField(\"quantity\", IntegerType(), False),\n",
    "        StructField(\"sale_date\", StringType(), False)\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"Generating large sales dataset...\")\n",
    "sales_df = generate_large_sales_data(50000)  # 50K records for demo\n",
    "sales_df = sales_df.withColumn(\"sale_date\", F.to_date(\"sale_date\"))\n",
    "\n",
    "print(f\"Generated {sales_df.count():,} sales records\")\n",
    "print(\"\\nSample data:\")\n",
    "sales_df.show(5)\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching Performance Comparison\n",
    "\n",
    "Let's demonstrate the performance impact of caching on iterative operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Caching Performance Comparison ===\")\n",
    "\n",
    "def complex_transformation(df):\n",
    "    \"\"\"Complex transformation that we'll reuse multiple times\"\"\"\n",
    "    return (df\n",
    "            .withColumn(\"total_value\", F.col(\"amount\") * F.col(\"quantity\"))\n",
    "            .withColumn(\"year\", F.year(\"sale_date\"))\n",
    "            .withColumn(\"month\", F.month(\"sale_date\"))\n",
    "            .withColumn(\"is_high_value\", F.when(F.col(\"total_value\") > 1000, True).otherwise(False))\n",
    "            .filter(F.col(\"total_value\") > 50)  # Filter out very small transactions\n",
    "           )\n",
    "\n",
    "def multiple_analyses_without_caching(df):\n",
    "    \"\"\"Perform multiple analyses without caching\"\"\"\n",
    "    transformed_df = complex_transformation(df)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Analysis 1: Total revenue by category\n",
    "    category_revenue = (transformed_df\n",
    "                       .groupBy(\"category\")\n",
    "                       .agg(F.sum(\"total_value\").alias(\"total_revenue\"))\n",
    "                       .collect())\n",
    "    \n",
    "    # Analysis 2: High-value transaction count by region\n",
    "    high_value_by_region = (transformed_df\n",
    "                           .filter(F.col(\"is_high_value\"))\n",
    "                           .groupBy(\"region\")\n",
    "                           .count()\n",
    "                           .collect())\n",
    "    \n",
    "    # Analysis 3: Monthly trends\n",
    "    monthly_trends = (transformed_df\n",
    "                     .groupBy(\"year\", \"month\")\n",
    "                     .agg(F.avg(\"total_value\").alias(\"avg_value\"),\n",
    "                          F.count(\"*\").alias(\"transaction_count\"))\n",
    "                     .collect())\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time, len(category_revenue), len(high_value_by_region), len(monthly_trends)\n",
    "\n",
    "def multiple_analyses_with_caching(df):\n",
    "    \"\"\"Perform multiple analyses with caching\"\"\"\n",
    "    transformed_df = complex_transformation(df)\n",
    "    \n",
    "    # Cache the transformed DataFrame\n",
    "    transformed_df.cache()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Analysis 1: Total revenue by category\n",
    "    category_revenue = (transformed_df\n",
    "                       .groupBy(\"category\")\n",
    "                       .agg(F.sum(\"total_value\").alias(\"total_revenue\"))\n",
    "                       .collect())\n",
    "    \n",
    "    # Analysis 2: High-value transaction count by region\n",
    "    high_value_by_region = (transformed_df\n",
    "                           .filter(F.col(\"is_high_value\"))\n",
    "                           .groupBy(\"region\")\n",
    "                           .count()\n",
    "                           .collect())\n",
    "    \n",
    "    # Analysis 3: Monthly trends\n",
    "    monthly_trends = (transformed_df\n",
    "                     .groupBy(\"year\", \"month\")\n",
    "                     .agg(F.avg(\"total_value\").alias(\"avg_value\"),\n",
    "                          F.count(\"*\").alias(\"transaction_count\"))\n",
    "                     .collect())\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Unpersist to free memory\n",
    "    transformed_df.unpersist()\n",
    "    \n",
    "    return end_time - start_time, len(category_revenue), len(high_value_by_region), len(monthly_trends)\n",
    "\n",
    "print(\"\\nRunning analysis without caching...\")\n",
    "time_without_cache, cat_count, region_count, month_count = multiple_analyses_without_caching(sales_df)\n",
    "\n",
    "print(\"\\nRunning analysis with caching...\")\n",
    "time_with_cache, cat_count_cached, region_count_cached, month_count_cached = multiple_analyses_with_caching(sales_df)\n",
    "\n",
    "print(f\"\\n=== Performance Results ===\")\n",
    "print(f\"Without caching: {time_without_cache:.2f} seconds\")\n",
    "print(f\"With caching:    {time_with_cache:.2f} seconds\")\n",
    "print(f\"Performance improvement: {time_without_cache/time_with_cache:.1f}x faster with caching\")\n",
    "print(f\"Time saved: {time_without_cache - time_with_cache:.2f} seconds\")\n",
    "\n",
    "# Verify results are identical\n",
    "assert cat_count == cat_count_cached, \"Category counts should match\"\n",
    "assert region_count == region_count_cached, \"Region counts should match\"\n",
    "assert month_count == month_count_cached, \"Monthly counts should match\"\n",
    "print(\"\\n‚úÖ Results verified: Identical outputs with and without caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategic Caching Patterns\n",
    "\n",
    "Different caching strategies for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Strategic Caching Patterns ===\")\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "class CachingStrategies:\n",
    "    \"\"\"Functional caching strategies for different scenarios\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cache_for_iterative_ml(df, storage_level=StorageLevel.MEMORY_AND_DISK):\n",
    "        \"\"\"\n",
    "        Caching strategy for iterative ML algorithms\n",
    "        Uses MEMORY_AND_DISK for fault tolerance\n",
    "        \"\"\"\n",
    "        return df.persist(storage_level)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cache_for_interactive_analysis(df):\n",
    "        \"\"\"\n",
    "        Caching strategy for interactive data exploration\n",
    "        Prioritizes memory for fast access\n",
    "        \"\"\"\n",
    "        return df.cache()  # Equivalent to MEMORY_ONLY\n",
    "    \n",
    "    @staticmethod\n",
    "    def cache_for_batch_processing(df):\n",
    "        \"\"\"\n",
    "        Caching strategy for batch processing pipelines\n",
    "        Uses disk storage for large datasets\n",
    "        \"\"\"\n",
    "        return df.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "    \n",
    "    @staticmethod\n",
    "    def conditional_caching(df, cache_condition_func, threshold=1000000):\n",
    "        \"\"\"\n",
    "        Conditionally cache based on dataset characteristics\n",
    "        \"\"\"\n",
    "        row_count = df.count()\n",
    "        \n",
    "        if cache_condition_func(row_count, threshold):\n",
    "            print(f\"Dataset size ({row_count:,} rows) meets caching criteria\")\n",
    "            return df.cache()\n",
    "        else:\n",
    "            print(f\"Dataset size ({row_count:,} rows) too small for caching benefits\")\n",
    "            return df\n",
    "\n",
    "# Demonstrate different caching strategies\n",
    "print(\"\\n1. Memory-only caching for interactive analysis:\")\n",
    "interactive_df = CachingStrategies.cache_for_interactive_analysis(sales_df)\n",
    "print(f\"Storage level: {interactive_df.storageLevel}\")\n",
    "\n",
    "print(\"\\n2. Memory and disk caching for ML:\")\n",
    "ml_df = CachingStrategies.cache_for_iterative_ml(sales_df)\n",
    "print(f\"Storage level: {ml_df.storageLevel}\")\n",
    "\n",
    "print(\"\\n3. Serialized caching for batch processing:\")\n",
    "batch_df = CachingStrategies.cache_for_batch_processing(sales_df)\n",
    "print(f\"Storage level: {batch_df.storageLevel}\")\n",
    "\n",
    "print(\"\\n4. Conditional caching:\")\n",
    "# Cache if dataset is large enough\n",
    "cache_condition = lambda count, threshold: count > threshold\n",
    "conditional_df = CachingStrategies.conditional_caching(sales_df, cache_condition, threshold=10000)\n",
    "\n",
    "# Clean up cached DataFrames\n",
    "interactive_df.unpersist()\n",
    "ml_df.unpersist()\n",
    "batch_df.unpersist()\n",
    "conditional_df.unpersist()\n",
    "\n",
    "print(\"\\n‚úÖ Cache cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Cache vs Spark Cache\n",
    "\n",
    "In Databricks, Delta Cache provides additional performance benefits over standard Spark cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Delta Cache vs Spark Cache ===\")\n",
    "\n",
    "# Note: Delta Cache is automatic in Databricks for Delta tables\n",
    "# This section demonstrates concepts and best practices\n",
    "\n",
    "class DeltaCachingPatterns:\n",
    "    \"\"\"Best practices for Delta Cache utilization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_for_delta_cache_optimization(df, path, partition_columns=None):\n",
    "        \"\"\"\n",
    "        Write DataFrame to Delta format optimized for Delta Cache\n",
    "        \"\"\"\n",
    "        writer = df.write.format(\"delta\").mode(\"overwrite\")\n",
    "        \n",
    "        if partition_columns:\n",
    "            writer = writer.partitionBy(*partition_columns)\n",
    "        \n",
    "        # Optimize file size for Delta Cache (128MB - 1GB per file)\n",
    "        writer = writer.option(\"delta.targetFileSize\", \"268435456\")  # 256MB\n",
    "        \n",
    "        writer.save(path)\n",
    "        return path\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_with_delta_cache_awareness(path, select_columns=None, filter_condition=None):\n",
    "        \"\"\"\n",
    "        Read Delta table with patterns that maximize Delta Cache benefits\n",
    "        \"\"\"\n",
    "        df = spark.read.format(\"delta\").load(path)\n",
    "        \n",
    "        # Column pruning helps Delta Cache efficiency\n",
    "        if select_columns:\n",
    "            df = df.select(*select_columns)\n",
    "        \n",
    "        # Predicate pushdown helps with cache locality\n",
    "        if filter_condition:\n",
    "            df = df.filter(filter_condition)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Create optimized Delta table for caching\n",
    "delta_path = \"/tmp/delta_sales_optimized\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.rm(delta_path, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nCreating optimized Delta table...\")\n",
    "DeltaCachingPatterns.write_for_delta_cache_optimization(\n",
    "    sales_df, \n",
    "    delta_path, \n",
    "    partition_columns=[\"category\"]  # Partition by category for better cache locality\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Delta table created with cache optimization\")\n",
    "\n",
    "# Demonstrate cache-aware reading patterns\n",
    "print(\"\\nTesting cache-aware reading patterns:\")\n",
    "\n",
    "# Pattern 1: Column pruning\n",
    "pruned_df = DeltaCachingPatterns.read_with_delta_cache_awareness(\n",
    "    delta_path,\n",
    "    select_columns=[\"category\", \"region\", \"amount\", \"quantity\"],\n",
    "    filter_condition=F.col(\"amount\") > 100\n",
    ")\n",
    "\n",
    "print(f\"Pruned DataFrame columns: {pruned_df.columns}\")\n",
    "print(f\"Filtered records: {pruned_df.count():,}\")\n",
    "\n",
    "# Pattern 2: Repeated access (benefits from Delta Cache)\n",
    "start_time = time.time()\n",
    "for i in range(3):\n",
    "    result = pruned_df.agg(F.avg(\"amount\")).collect()[0][0]\n",
    "    print(f\"Iteration {i+1}: Average amount = ${result:.2f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total time for 3 iterations: {total_time:.2f} seconds\")\n",
    "print(\"(Subsequent iterations should be faster due to Delta Cache)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Joins for Performance\n",
    "\n",
    "Broadcast joins can dramatically improve performance when joining large DataFrames with smaller lookup tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Broadcast Joins Demonstration ===\")\n",
    "\n",
    "# Create lookup tables (small DataFrames perfect for broadcasting)\n",
    "category_metadata = [\n",
    "    (\"Electronics\", \"Technology\", 0.15, \"High-tech consumer goods\"),\n",
    "    (\"Clothing\", \"Fashion\", 0.25, \"Apparel and accessories\"),\n",
    "    (\"Books\", \"Education\", 0.08, \"Educational and recreational reading\"),\n",
    "    (\"Home\", \"Lifestyle\", 0.12, \"Home improvement and furniture\"),\n",
    "    (\"Sports\", \"Recreation\", 0.18, \"Sports equipment and gear\"),\n",
    "    (\"Beauty\", \"Personal Care\", 0.22, \"Cosmetics and personal care items\"),\n",
    "    (\"Automotive\", \"Transportation\", 0.10, \"Vehicle parts and accessories\")\n",
    "]\n",
    "\n",
    "category_schema = StructType([\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"category_group\", StringType(), False),\n",
    "    StructField(\"commission_rate\", DoubleType(), False),\n",
    "    StructField(\"description\", StringType(), True)\n",
    "])\n",
    "\n",
    "category_lookup_df = spark.createDataFrame(category_metadata, category_schema)\n",
    "\n",
    "region_metadata = [\n",
    "    (\"North\", \"Northern Region\", \"Chicago\", 1.05),\n",
    "    (\"South\", \"Southern Region\", \"Atlanta\", 0.98),\n",
    "    (\"East\", \"Eastern Region\", \"New York\", 1.12),\n",
    "    (\"West\", \"Western Region\", \"Los Angeles\", 1.08),\n",
    "    (\"Central\", \"Central Region\", \"Dallas\", 1.02)\n",
    "]\n",
    "\n",
    "region_schema = StructType([\n",
    "    StructField(\"region\", StringType(), False),\n",
    "    StructField(\"region_name\", StringType(), False),\n",
    "    StructField(\"headquarters\", StringType(), False),\n",
    "    StructField(\"cost_multiplier\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "region_lookup_df = spark.createDataFrame(region_metadata, region_schema)\n",
    "\n",
    "print(\"Lookup tables created:\")\n",
    "print(\"\\nCategory lookup:\")\n",
    "category_lookup_df.show()\n",
    "print(\"\\nRegion lookup:\")\n",
    "region_lookup_df.show()\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"Sales data: {sales_df.count():,} rows\")\n",
    "print(f\"Category lookup: {category_lookup_df.count()} rows\")\n",
    "print(f\"Region lookup: {region_lookup_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: Regular join vs Broadcast join\n",
    "print(\"\\n=== Join Performance Comparison ===\")\n",
    "\n",
    "def regular_joins(sales_df, category_df, region_df):\n",
    "    \"\"\"Perform joins without broadcast hint\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = (sales_df\n",
    "             .join(category_df, \"category\")\n",
    "             .join(region_df, \"region\")\n",
    "             .withColumn(\"total_value\", F.col(\"amount\") * F.col(\"quantity\"))\n",
    "             .withColumn(\"commission\", F.col(\"total_value\") * F.col(\"commission_rate\"))\n",
    "             .withColumn(\"adjusted_value\", F.col(\"total_value\") * F.col(\"cost_multiplier\"))\n",
    "            )\n",
    "    \n",
    "    # Force execution\n",
    "    count = result.count()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, count\n",
    "\n",
    "def broadcast_joins(sales_df, category_df, region_df):\n",
    "    \"\"\"Perform joins with broadcast hints\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = (sales_df\n",
    "             .join(F.broadcast(category_df), \"category\")  # Broadcast hint\n",
    "             .join(F.broadcast(region_df), \"region\")      # Broadcast hint\n",
    "             .withColumn(\"total_value\", F.col(\"amount\") * F.col(\"quantity\"))\n",
    "             .withColumn(\"commission\", F.col(\"total_value\") * F.col(\"commission_rate\"))\n",
    "             .withColumn(\"adjusted_value\", F.col(\"total_value\") * F.col(\"cost_multiplier\"))\n",
    "            )\n",
    "    \n",
    "    # Force execution\n",
    "    count = result.count()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, count\n",
    "\n",
    "print(\"Running regular joins...\")\n",
    "regular_time, regular_count = regular_joins(sales_df, category_lookup_df, region_lookup_df)\n",
    "\n",
    "print(\"Running broadcast joins...\")\n",
    "broadcast_time, broadcast_count = broadcast_joins(sales_df, category_lookup_df, region_lookup_df)\n",
    "\n",
    "print(f\"\\n=== Join Performance Results ===\")\n",
    "print(f\"Regular joins:    {regular_time:.2f} seconds ({regular_count:,} rows)\")\n",
    "print(f\"Broadcast joins:  {broadcast_time:.2f} seconds ({broadcast_count:,} rows)\")\n",
    "print(f\"Performance improvement: {regular_time/broadcast_time:.1f}x faster with broadcast\")\n",
    "print(f\"Time saved: {regular_time - broadcast_time:.2f} seconds\")\n",
    "\n",
    "# Verify results are identical\n",
    "assert regular_count == broadcast_count, \"Row counts should match\"\n",
    "print(\"\\n‚úÖ Results verified: Identical outputs with both join strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent Broadcast Join Strategy\n",
    "\n",
    "Let's create a functional approach to automatically determine when to use broadcast joins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Intelligent Broadcast Join Strategy ===\")\n",
    "\n",
    "class BroadcastJoinOptimizer:\n",
    "    \"\"\"Functional utilities for optimizing join strategies\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_dataframe_size(df, sample_fraction=0.01):\n",
    "        \"\"\"\n",
    "        Estimate DataFrame size for broadcast decision\n",
    "        Pure function that doesn't modify the DataFrame\n",
    "        \"\"\"\n",
    "        # Sample the DataFrame to estimate row size\n",
    "        sample_df = df.sample(sample_fraction)\n",
    "        sample_count = sample_df.count()\n",
    "        \n",
    "        if sample_count == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Estimate bytes per row (rough calculation)\n",
    "        # This is a simplified estimation - in practice, you might use more sophisticated methods\n",
    "        columns = len(df.columns)\n",
    "        estimated_bytes_per_row = columns * 50  # Rough estimate\n",
    "        \n",
    "        total_rows = df.count()\n",
    "        estimated_size_mb = (total_rows * estimated_bytes_per_row) / (1024 * 1024)\n",
    "        \n",
    "        return estimated_size_mb\n",
    "    \n",
    "    @staticmethod\n",
    "    def should_broadcast(df, broadcast_threshold_mb=200):\n",
    "        \"\"\"\n",
    "        Determine if a DataFrame should be broadcasted\n",
    "        Pure function for broadcast decision logic\n",
    "        \"\"\"\n",
    "        estimated_size = BroadcastJoinOptimizer.estimate_dataframe_size(df)\n",
    "        return estimated_size < broadcast_threshold_mb, estimated_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def smart_join(left_df, right_df, join_keys, join_type=\"inner\", \n",
    "                   auto_broadcast=True, broadcast_threshold_mb=200):\n",
    "        \"\"\"\n",
    "        Intelligent join function that automatically decides on broadcast strategy\n",
    "        \"\"\"\n",
    "        if not auto_broadcast:\n",
    "            return left_df.join(right_df, join_keys, join_type)\n",
    "        \n",
    "        # Check if either DataFrame should be broadcasted\n",
    "        left_should_broadcast, left_size = BroadcastJoinOptimizer.should_broadcast(\n",
    "            left_df, broadcast_threshold_mb)\n",
    "        right_should_broadcast, right_size = BroadcastJoinOptimizer.should_broadcast(\n",
    "            right_df, broadcast_threshold_mb)\n",
    "        \n",
    "        print(f\"Left DataFrame size estimate: {left_size:.1f} MB\")\n",
    "        print(f\"Right DataFrame size estimate: {right_size:.1f} MB\")\n",
    "        \n",
    "        if right_should_broadcast and not left_should_broadcast:\n",
    "            print(\"üöÄ Broadcasting right DataFrame\")\n",
    "            return left_df.join(F.broadcast(right_df), join_keys, join_type)\n",
    "        elif left_should_broadcast and not right_should_broadcast:\n",
    "            print(\"üöÄ Broadcasting left DataFrame\")\n",
    "            return F.broadcast(left_df).join(right_df, join_keys, join_type)\n",
    "        elif left_should_broadcast and right_should_broadcast:\n",
    "            # Both are small, broadcast the smaller one\n",
    "            if left_size <= right_size:\n",
    "                print(\"üöÄ Broadcasting left DataFrame (smaller of two small DataFrames)\")\n",
    "                return F.broadcast(left_df).join(right_df, join_keys, join_type)\n",
    "            else:\n",
    "                print(\"üöÄ Broadcasting right DataFrame (smaller of two small DataFrames)\")\n",
    "                return left_df.join(F.broadcast(right_df), join_keys, join_type)\n",
    "        else:\n",
    "            print(\"üìä Using regular join (both DataFrames too large for broadcast)\")\n",
    "            return left_df.join(right_df, join_keys, join_type)\n",
    "\n",
    "# Test the intelligent join optimizer\n",
    "print(\"\\nTesting intelligent join strategy:\")\n",
    "\n",
    "# Join with category lookup (should broadcast)\n",
    "print(\"\\n1. Joining with category lookup:\")\n",
    "result1 = BroadcastJoinOptimizer.smart_join(\n",
    "    sales_df, \n",
    "    category_lookup_df, \n",
    "    \"category\"\n",
    ")\n",
    "\n",
    "# Chain with region lookup (should also broadcast)\n",
    "print(\"\\n2. Chaining with region lookup:\")\n",
    "final_result = BroadcastJoinOptimizer.smart_join(\n",
    "    result1,\n",
    "    region_lookup_df,\n",
    "    \"region\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal result count: {final_result.count():,} rows\")\n",
    "print(\"\\nSample of enriched data:\")\n",
    "(final_result\n",
    " .select(\"transaction_id\", \"category\", \"category_group\", \"region\", \"region_name\", \n",
    "         \"amount\", \"commission_rate\")\n",
    " .show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Format Optimization\n",
    "\n",
    "Choosing the right file format is crucial for performance. Let's compare different formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== File Format Performance Comparison ===\")\n",
    "\n",
    "import os\n",
    "\n",
    "class FileFormatOptimizer:\n",
    "    \"\"\"Utilities for file format optimization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_and_measure(df, path, format_type, **options):\n",
    "        \"\"\"\n",
    "        Write DataFrame in specified format and measure performance\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dbutils.fs.rm(path, True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        writer = df.write.format(format_type).mode(\"overwrite\")\n",
    "        \n",
    "        # Apply any format-specific options\n",
    "        for key, value in options.items():\n",
    "            writer = writer.option(key, value)\n",
    "        \n",
    "        writer.save(path)\n",
    "        \n",
    "        write_time = time.time() - start_time\n",
    "        \n",
    "        # Measure file size\n",
    "        try:\n",
    "            file_info = dbutils.fs.ls(path)\n",
    "            total_size = sum([f.size for f in file_info if f.name.endswith('.parquet') \n",
    "                             or f.name.endswith('.json') or f.name.endswith('.csv')]) / (1024*1024)  # MB\n",
    "        except:\n",
    "            total_size = 0\n",
    "        \n",
    "        return write_time, total_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_and_measure(path, format_type, select_columns=None, filter_condition=None):\n",
    "        \"\"\"\n",
    "        Read DataFrame and measure performance\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        df = spark.read.format(format_type).load(path)\n",
    "        \n",
    "        if select_columns:\n",
    "            df = df.select(*select_columns)\n",
    "        \n",
    "        if filter_condition:\n",
    "            df = df.filter(filter_condition)\n",
    "        \n",
    "        # Force execution\n",
    "        count = df.count()\n",
    "        \n",
    "        read_time = time.time() - start_time\n",
    "        \n",
    "        return read_time, count\n",
    "\n",
    "# Test different file formats\n",
    "formats_to_test = [\n",
    "    (\"parquet\", \"/tmp/sales_parquet\", {}),\n",
    "    (\"delta\", \"/tmp/sales_delta\", {}),\n",
    "    (\"json\", \"/tmp/sales_json\", {}),\n",
    "    (\"csv\", \"/tmp/sales_csv\", {\"header\": \"true\"})\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nTesting file format performance...\")\n",
    "for format_name, path, options in formats_to_test:\n",
    "    print(f\"\\nTesting {format_name.upper()} format...\")\n",
    "    \n",
    "    # Write performance\n",
    "    write_time, file_size = FileFormatOptimizer.write_and_measure(\n",
    "        sales_df, path, format_name, **options)\n",
    "    \n",
    "    # Read performance (full read)\n",
    "    read_time_full, record_count = FileFormatOptimizer.read_and_measure(\n",
    "        path, format_name)\n",
    "    \n",
    "    # Read performance (with column pruning)\n",
    "    read_time_pruned, pruned_count = FileFormatOptimizer.read_and_measure(\n",
    "        path, format_name, \n",
    "        select_columns=[\"category\", \"region\", \"amount\"],\n",
    "        filter_condition=F.col(\"amount\") > 500\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'format': format_name.upper(),\n",
    "        'write_time': write_time,\n",
    "        'file_size_mb': file_size,\n",
    "        'read_time_full': read_time_full,\n",
    "        'read_time_pruned': read_time_pruned,\n",
    "        'record_count': record_count,\n",
    "        'pruned_count': pruned_count\n",
    "    })\n",
    "    \n",
    "    print(f\"  Write time: {write_time:.2f}s\")\n",
    "    print(f\"  File size: {file_size:.1f} MB\")\n",
    "    print(f\"  Full read time: {read_time_full:.2f}s\")\n",
    "    print(f\"  Pruned read time: {read_time_pruned:.2f}s\")\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FILE FORMAT PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Format':<8} {'Write(s)':<10} {'Size(MB)':<10} {'Read Full(s)':<12} {'Read Pruned(s)':<15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"{result['format']:<8} {result['write_time']:<10.2f} {result['file_size_mb']:<10.1f} \"\n",
    "          f\"{result['read_time_full']:<12.2f} {result['read_time_pruned']:<15.2f}\")\n",
    "\n",
    "print(\"\\nüìä Performance Analysis:\")\n",
    "parquet_result = next(r for r in results if r['format'] == 'PARQUET')\n",
    "delta_result = next(r for r in results if r['format'] == 'DELTA')\n",
    "json_result = next(r for r in results if r['format'] == 'JSON')\n",
    "\n",
    "print(f\"‚Ä¢ Parquet vs JSON compression: {json_result['file_size_mb']/parquet_result['file_size_mb']:.1f}x smaller\")\n",
    "print(f\"‚Ä¢ Parquet vs JSON read speed: {json_result['read_time_full']/parquet_result['read_time_full']:.1f}x faster\")\n",
    "print(f\"‚Ä¢ Column pruning benefit (Parquet): {parquet_result['read_time_full']/parquet_result['read_time_pruned']:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategic Data Handling Best Practices\n",
    "\n",
    "Let's create a comprehensive framework for strategic data handling decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Strategic Data Handling Framework ===\")\n",
    "\n",
    "class DataHandlingStrategy:\n",
    "    \"\"\"Comprehensive framework for data handling decisions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_dataset_characteristics(df):\n",
    "        \"\"\"\n",
    "        Analyze DataFrame characteristics to inform handling strategy\n",
    "        \"\"\"\n",
    "        characteristics = {}\n",
    "        \n",
    "        # Basic metrics\n",
    "        characteristics['row_count'] = df.count()\n",
    "        characteristics['column_count'] = len(df.columns)\n",
    "        \n",
    "        # Estimate size\n",
    "        characteristics['estimated_size_mb'] = BroadcastJoinOptimizer.estimate_dataframe_size(df)\n",
    "        \n",
    "        # Check for skew (simplified)\n",
    "        if characteristics['row_count'] > 1000:  # Only for reasonably sized datasets\n",
    "            try:\n",
    "                # Check partition count distribution\n",
    "                partition_count = df.rdd.getNumPartitions()\n",
    "                characteristics['partition_count'] = partition_count\n",
    "                \n",
    "                # Estimate skew by checking partition size variation\n",
    "                partition_sizes = df.rdd.mapPartitions(lambda iterator: [sum(1 for _ in iterator)]).collect()\n",
    "                if partition_sizes:\n",
    "                    avg_partition_size = sum(partition_sizes) / len(partition_sizes)\n",
    "                    max_partition_size = max(partition_sizes)\n",
    "                    characteristics['skew_ratio'] = max_partition_size / avg_partition_size if avg_partition_size > 0 else 1\n",
    "                else:\n",
    "                    characteristics['skew_ratio'] = 1\n",
    "            except:\n",
    "                characteristics['partition_count'] = 1\n",
    "                characteristics['skew_ratio'] = 1\n",
    "        else:\n",
    "            characteristics['partition_count'] = 1\n",
    "            characteristics['skew_ratio'] = 1\n",
    "        \n",
    "        return characteristics\n",
    "    \n",
    "    @staticmethod\n",
    "    def recommend_caching_strategy(characteristics, access_pattern=\"multiple\"):\n",
    "        \"\"\"\n",
    "        Recommend caching strategy based on dataset characteristics\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        row_count = characteristics['row_count']\n",
    "        estimated_size = characteristics['estimated_size_mb']\n",
    "        \n",
    "        # Size-based recommendations\n",
    "        if estimated_size < 100:  # Small datasets\n",
    "            if access_pattern == \"multiple\":\n",
    "                recommendations.append(\"‚úÖ CACHE (MEMORY_ONLY) - Small dataset, multiple access\")\n",
    "            else:\n",
    "                recommendations.append(\"‚ùå No caching needed - Single access of small dataset\")\n",
    "        \n",
    "        elif estimated_size < 1000:  # Medium datasets\n",
    "            if access_pattern == \"multiple\":\n",
    "                recommendations.append(\"‚úÖ CACHE (MEMORY_AND_DISK) - Medium dataset, fault tolerance\")\n",
    "            elif access_pattern == \"iterative\":\n",
    "                recommendations.append(\"‚úÖ PERSIST (MEMORY_AND_DISK_SER) - Iterative access pattern\")\n",
    "            else:\n",
    "                recommendations.append(\"‚ö†Ô∏è  Consider caching - Medium dataset, depends on access pattern\")\n",
    "        \n",
    "        else:  # Large datasets\n",
    "            if access_pattern == \"iterative\":\n",
    "                recommendations.append(\"‚úÖ PERSIST (MEMORY_AND_DISK_SER) - Large dataset, iterative ML\")\n",
    "            else:\n",
    "                recommendations.append(\"‚ö†Ô∏è  Selective caching - Cache only frequently accessed subsets\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    @staticmethod\n",
    "    def recommend_join_strategy(left_chars, right_chars):\n",
    "        \"\"\"\n",
    "        Recommend join strategy based on DataFrame characteristics\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        left_size = left_chars['estimated_size_mb']\n",
    "        right_size = right_chars['estimated_size_mb']\n",
    "        \n",
    "        broadcast_threshold = 200  # MB\n",
    "        \n",
    "        if right_size < broadcast_threshold:\n",
    "            recommendations.append(f\"‚úÖ BROADCAST right table ({right_size:.1f} MB < {broadcast_threshold} MB)\")\n",
    "        elif left_size < broadcast_threshold:\n",
    "            recommendations.append(f\"‚úÖ BROADCAST left table ({left_size:.1f} MB < {broadcast_threshold} MB)\")\n",
    "        else:\n",
    "            # Check for skew\n",
    "            if left_chars['skew_ratio'] > 3 or right_chars['skew_ratio'] > 3:\n",
    "                recommendations.append(\"‚ö†Ô∏è  SORT-MERGE JOIN with skew handling (data skew detected)\")\n",
    "            else:\n",
    "                recommendations.append(\"üìä SORT-MERGE JOIN (both tables too large for broadcast)\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    @staticmethod\n",
    "    def recommend_file_format(use_case, data_characteristics):\n",
    "        \"\"\"\n",
    "        Recommend file format based on use case and data characteristics\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if use_case == \"analytics\":\n",
    "            recommendations.append(\"‚úÖ DELTA LAKE - Best for analytics with ACID transactions\")\n",
    "            recommendations.append(\"‚úÖ PARQUET - Good alternative for read-heavy analytics\")\n",
    "        \n",
    "        elif use_case == \"ml_training\":\n",
    "            recommendations.append(\"‚úÖ PARQUET - Optimal for ML feature stores\")\n",
    "            recommendations.append(\"‚úÖ DELTA LAKE - Good for versioned ML datasets\")\n",
    "        \n",
    "        elif use_case == \"streaming\":\n",
    "            recommendations.append(\"‚úÖ DELTA LAKE - Required for streaming analytics\")\n",
    "        \n",
    "        elif use_case == \"data_exchange\":\n",
    "            recommendations.append(\"‚úÖ PARQUET - Standard for data interchange\")\n",
    "            recommendations.append(\"‚ö†Ô∏è  JSON - For schema flexibility (with compression)\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Demonstrate the comprehensive strategy framework\n",
    "print(\"\\nAnalyzing sales dataset characteristics...\")\n",
    "sales_characteristics = DataHandlingStrategy.analyze_dataset_characteristics(sales_df)\n",
    "category_characteristics = DataHandlingStrategy.analyze_dataset_characteristics(category_lookup_df)\n",
    "\n",
    "print(\"\\nüìä Dataset Analysis Results:\")\n",
    "print(f\"Sales Dataset:\")\n",
    "print(f\"  - Rows: {sales_characteristics['row_count']:,}\")\n",
    "print(f\"  - Columns: {sales_characteristics['column_count']}\")\n",
    "print(f\"  - Estimated size: {sales_characteristics['estimated_size_mb']:.1f} MB\")\n",
    "print(f\"  - Partitions: {sales_characteristics['partition_count']}\")\n",
    "print(f\"  - Skew ratio: {sales_characteristics['skew_ratio']:.2f}\")\n",
    "\n",
    "print(f\"\\nCategory Lookup:\")\n",
    "print(f\"  - Rows: {category_characteristics['row_count']}\")\n",
    "print(f\"  - Estimated size: {category_characteristics['estimated_size_mb']:.1f} MB\")\n",
    "\n",
    "# Get recommendations\n",
    "print(\"\\nüéØ Strategic Recommendations:\")\n",
    "\n",
    "print(\"\\nCaching Strategy:\")\n",
    "caching_recs = DataHandlingStrategy.recommend_caching_strategy(sales_characteristics, \"multiple\")\n",
    "for rec in caching_recs:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(\"\\nJoin Strategy:\")\n",
    "join_recs = DataHandlingStrategy.recommend_join_strategy(sales_characteristics, category_characteristics)\n",
    "for rec in join_recs:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(\"\\nFile Format Recommendations:\")\n",
    "format_recs = DataHandlingStrategy.recommend_file_format(\"analytics\", sales_characteristics)\n",
    "for rec in format_recs:\n",
    "    print(f\"  {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Strategic Caching**:\n",
    "   - Cache DataFrames that are accessed multiple times\n",
    "   - Choose appropriate storage levels based on use case\n",
    "   - Use conditional caching based on dataset characteristics\n",
    "   - Always unpersist when done to free resources\n",
    "\n",
    "2. **Broadcast Join Benefits**:\n",
    "   - Dramatic performance improvement for small lookup tables\n",
    "   - Automatic optimization with AQE in Spark 3.0+\n",
    "   - Manual broadcast hints for explicit control\n",
    "   - Intelligent decision making based on dataset size\n",
    "\n",
    "3. **File Format Optimization**:\n",
    "   - Parquet/Delta for analytics workloads\n",
    "   - Columnar formats provide better compression and read performance\n",
    "   - Column pruning benefits with columnar formats\n",
    "   - Consider use case when choosing formats\n",
    "\n",
    "4. **Functional Programming Alignment**:\n",
    "   - Caching as managed \"state\" within the framework\n",
    "   - Pure functions for optimization decision logic\n",
    "   - Immutable DataFrames with strategic persistence\n",
    "   - Composable optimization strategies\n",
    "\n",
    "5. **Performance Principles**:\n",
    "   - Measure before optimizing\n",
    "   - Understand your data characteristics\n",
    "   - Use framework-provided optimizations (AQE, Delta Cache)\n",
    "   - Balance optimization with code maintainability\n",
    "\n",
    "**Best Practices for Strategic Data Handling**:\n",
    "- Analyze dataset characteristics before choosing strategies\n",
    "- Use intelligent decision frameworks rather than hard-coded rules\n",
    "- Leverage platform optimizations (Photon, AQE, Delta Cache)\n",
    "- Monitor and measure the impact of optimizations\n",
    "- Keep optimization logic separate and testable\n",
    "\n",
    "**Next Steps**: In the next notebook, we'll explore techniques for minimizing data shuffling and handling data skew, which are critical for large-scale PySpark performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Apply strategic data handling to your own use case:\n",
    "\n",
    "1. Create a dataset with lookup tables for your domain\n",
    "2. Implement caching performance comparison\n",
    "3. Test broadcast join optimizations\n",
    "4. Compare file format performance for your data\n",
    "5. Build a decision framework for your specific use case\n",
    "6. Create intelligent optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "\n",
    "def create_your_dataset_with_lookups():\n",
    "    \"\"\"\n",
    "    Create a dataset relevant to your domain with appropriate lookup tables\n",
    "    \"\"\"\n",
    "    # Your data generation logic\n",
    "    pass\n",
    "\n",
    "def test_caching_performance(df):\n",
    "    \"\"\"\n",
    "    Test caching performance for your specific access patterns\n",
    "    \"\"\"\n",
    "    # Your caching performance test\n",
    "    pass\n",
    "\n",
    "def optimize_joins_for_your_data(main_df, lookup_df):\n",
    "    \"\"\"\n",
    "    Apply intelligent join optimization for your data\n",
    "    \"\"\"\n",
    "    # Your join optimization logic\n",
    "    pass\n",
    "\n",
    "def build_your_optimization_framework():\n",
    "    \"\"\"\n",
    "    Build optimization decision framework for your domain\n",
    "    \"\"\"\n",
    "    # Your optimization framework\n",
    "    pass\n",
    "\n",
    "# Test your implementations\n",
    "# your_data = create_your_dataset_with_lookups()\n",
    "# test_caching_performance(your_data)\n",
    "# build_your_optimization_framework()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltalake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
