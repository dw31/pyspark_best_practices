{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Logging and Error Reporting in Functional PySpark Pipelines\n",
    "\n",
    "This notebook demonstrates how to implement effective logging and error reporting in PySpark applications while maintaining functional programming principles. We'll explore structured logging, error handling patterns, and debugging strategies that align with functional design.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Implement structured logging in PySpark applications\n",
    "- Use PySpark's built-in logger vs standard Python logging\n",
    "- Handle errors functionally without breaking pure function principles\n",
    "- Create logging utilities that integrate with functional pipelines\n",
    "- Debug PySpark applications using log analysis\n",
    "- Store and analyze logs in Unity Catalog volumes\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of functional programming principles\n",
    "- Experience with PySpark transformations and actions\n",
    "- Knowledge of error handling in distributed systems\n",
    "- Familiarity with JSON and structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for logging and error handling\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FunctionalLoggingErrorHandling\").getOrCreate()\n",
    "\n",
    "print(\"\u2705 Setup complete - Ready for logging and error handling patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logging Approaches: PySpark Logger vs Standard Python Logging\n",
    "\n",
    "Let's compare different logging approaches available in PySpark and understand when to use each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of logging approaches\n",
    "\n",
    "print(\"=== Logging Approach Comparison ===\")\n",
    "\n",
    "# 1. Basic print statements (NOT recommended for production)\n",
    "print(\"\\n\u274c ANTI-PATTERN: Using print statements\")\n",
    "def bad_logging_example(df):\n",
    "    \"\"\"\n",
    "    Anti-pattern: Using print statements for logging\n",
    "    Problems: No log levels, no structured format, performance overhead\n",
    "    \"\"\"\n",
    "    print(\"Starting data processing...\")  # Side effect in transformation\n",
    "    print(f\"Input record count: {df.count()}\")  # Action in transformation!\n",
    "    \n",
    "    result = df.filter(F.col(\"amount\") > 0)\n",
    "    print(f\"After filtering: {result.count()} records\")  # Another action!\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Print statements create side effects and performance issues\")\n",
    "\n",
    "# 2. Standard Python logging (Recommended for most cases)\n",
    "print(\"\\n\u2705 BETTER: Standard Python logging\")\n",
    "\n",
    "# Configure Python logger\n",
    "def setup_standard_logger(name: str, level: int = logging.INFO) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Pure function to set up a standard Python logger\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    \n",
    "    # Avoid duplicate handlers when re-running cells\n",
    "    if not logger.hasHandlers():\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.setLevel(level)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Create a standard logger\n",
    "standard_logger = setup_standard_logger(\"pyspark_pipeline\")\n",
    "\n",
    "print(\"Standard Python logger configured\")\n",
    "standard_logger.info(\"This is an info message\")\n",
    "standard_logger.warning(\"This is a warning message\")\n",
    "standard_logger.error(\"This is an error message\")\n",
    "\n",
    "# 3. PySpark's built-in logger (Spark 4+ / DBR 17.0+)\n",
    "print(\"\\n\u2705 NEWEST: PySpark structured logger\")\n",
    "\n",
    "try:\n",
    "    # For newer Databricks Runtime versions\n",
    "    from pyspark.logger import PySparkLogger\n",
    "    \n",
    "    pyspark_logger = PySparkLogger.getLogger(\"functional_pipeline\")\n",
    "    \n",
    "    print(\"PySpark structured logger configured\")\n",
    "    pyspark_logger.info(\"Data processing started for {file_name}\", file_name=\"input.csv\")\n",
    "    pyspark_logger.warning(\"Low memory warning: {memory_usage}%\", memory_usage=85)\n",
    "    pyspark_logger.error(\"Failed to process record {record_id} due to {error_msg}\", \n",
    "                        record_id=123, error_msg=\"Invalid format\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"PySpark structured logger not available (requires Spark 4+ / DBR 17.0+)\")\n",
    "    print(\"Using standard Python logging for compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Structured Logging for Functional Pipelines\n",
    "\n",
    "Let's create structured logging utilities that work well with functional programming patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured logging utilities for functional pipelines\n",
    "\n",
    "class LogLevel(Enum):\n",
    "    \"\"\"Enum for log levels\"\"\"\n",
    "    DEBUG = \"DEBUG\"\n",
    "    INFO = \"INFO\"\n",
    "    WARNING = \"WARNING\"\n",
    "    ERROR = \"ERROR\"\n",
    "    CRITICAL = \"CRITICAL\"\n",
    "\n",
    "@dataclass\n",
    "class LogEntry:\n",
    "    \"\"\"\n",
    "    Immutable log entry structure for functional logging\n",
    "    \"\"\"\n",
    "    timestamp: str\n",
    "    level: LogLevel\n",
    "    logger_name: str\n",
    "    message: str\n",
    "    context: Dict[str, Any]\n",
    "    function_name: Optional[str] = None\n",
    "    execution_id: Optional[str] = None\n",
    "    \n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Convert log entry to JSON string\"\"\"\n",
    "        return json.dumps({\n",
    "            \"timestamp\": self.timestamp,\n",
    "            \"level\": self.level.value,\n",
    "            \"logger_name\": self.logger_name,\n",
    "            \"message\": self.message,\n",
    "            \"context\": self.context,\n",
    "            \"function_name\": self.function_name,\n",
    "            \"execution_id\": self.execution_id\n",
    "        })\n",
    "\n",
    "class FunctionalLogger:\n",
    "    \"\"\"\n",
    "    Logger designed for functional programming patterns\n",
    "    Maintains separation between logging and business logic\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, execution_id: Optional[str] = None):\n",
    "        self.name = name\n",
    "        self.execution_id = execution_id or f\"exec_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self._standard_logger = setup_standard_logger(name)\n",
    "    \n",
    "    def create_log_entry(self, level: LogLevel, message: str, \n",
    "                        context: Dict[str, Any] = None, \n",
    "                        function_name: str = None) -> LogEntry:\n",
    "        \"\"\"\n",
    "        Pure function to create a log entry\n",
    "        \"\"\"\n",
    "        return LogEntry(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            level=level,\n",
    "            logger_name=self.name,\n",
    "            message=message,\n",
    "            context=context or {},\n",
    "            function_name=function_name,\n",
    "            execution_id=self.execution_id\n",
    "        )\n",
    "    \n",
    "    def log_entry(self, entry: LogEntry) -> None:\n",
    "        \"\"\"\n",
    "        Log an entry using the standard logger\n",
    "        \"\"\"\n",
    "        log_message = f\"[{entry.function_name or 'unknown'}] {entry.message}\"\n",
    "        if entry.context:\n",
    "            log_message += f\" | Context: {json.dumps(entry.context)}\"\n",
    "        \n",
    "        if entry.level == LogLevel.DEBUG:\n",
    "            self._standard_logger.debug(log_message)\n",
    "        elif entry.level == LogLevel.INFO:\n",
    "            self._standard_logger.info(log_message)\n",
    "        elif entry.level == LogLevel.WARNING:\n",
    "            self._standard_logger.warning(log_message)\n",
    "        elif entry.level == LogLevel.ERROR:\n",
    "            self._standard_logger.error(log_message)\n",
    "        elif entry.level == LogLevel.CRITICAL:\n",
    "            self._standard_logger.critical(log_message)\n",
    "    \n",
    "    def info(self, message: str, context: Dict[str, Any] = None, function_name: str = None):\n",
    "        \"\"\"Log an info message\"\"\"\n",
    "        entry = self.create_log_entry(LogLevel.INFO, message, context, function_name)\n",
    "        self.log_entry(entry)\n",
    "        return entry\n",
    "    \n",
    "    def warning(self, message: str, context: Dict[str, Any] = None, function_name: str = None):\n",
    "        \"\"\"Log a warning message\"\"\"\n",
    "        entry = self.create_log_entry(LogLevel.WARNING, message, context, function_name)\n",
    "        self.log_entry(entry)\n",
    "        return entry\n",
    "    \n",
    "    def error(self, message: str, context: Dict[str, Any] = None, function_name: str = None):\n",
    "        \"\"\"Log an error message\"\"\"\n",
    "        entry = self.create_log_entry(LogLevel.ERROR, message, context, function_name)\n",
    "        self.log_entry(entry)\n",
    "        return entry\n",
    "\n",
    "# Create functional logger instance\n",
    "functional_logger = FunctionalLogger(\"functional_pipeline\")\n",
    "\n",
    "# Test the functional logger\n",
    "print(\"=== Testing Functional Logger ===\")\n",
    "\n",
    "# Example log entries\n",
    "info_entry = functional_logger.info(\n",
    "    \"Pipeline started successfully\",\n",
    "    context={\"input_path\": \"/data/input\", \"records_expected\": 1000},\n",
    "    function_name=\"start_pipeline\"\n",
    ")\n",
    "\n",
    "warning_entry = functional_logger.warning(\n",
    "    \"Performance degradation detected\",\n",
    "    context={\"memory_usage\": 85, \"cpu_usage\": 90, \"task_duration\": 45.2},\n",
    "    function_name=\"monitor_performance\"\n",
    ")\n",
    "\n",
    "print(f\"\\nJSON Log Entry Example:\\n{info_entry.to_json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functional Error Handling Patterns\n",
    "\n",
    "Let's explore how to handle errors in a functional way, using Result types and composable error handling patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional error handling with Result types\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Generic, TypeVar, Union\n",
    "\n",
    "T = TypeVar('T')\n",
    "E = TypeVar('E')\n",
    "\n",
    "class Result(Generic[T, E], ABC):\n",
    "    \"\"\"\n",
    "    Result type for functional error handling\n",
    "    Inspired by Rust's Result<T, E> type\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_ok(self) -> bool:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_err(self) -> bool:\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class Ok(Result[T, E]):\n",
    "    \"\"\"Success result containing a value\"\"\"\n",
    "    value: T\n",
    "    \n",
    "    def is_ok(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def is_err(self) -> bool:\n",
    "        return False\n",
    "\n",
    "@dataclass\n",
    "class Err(Result[T, E]):\n",
    "    \"\"\"Error result containing error information\"\"\"\n",
    "    error: E\n",
    "    \n",
    "    def is_ok(self) -> bool:\n",
    "        return False\n",
    "    \n",
    "    def is_err(self) -> bool:\n",
    "        return True\n",
    "\n",
    "@dataclass\n",
    "class PipelineError:\n",
    "    \"\"\"\n",
    "    Structured error information for pipeline operations\n",
    "    \"\"\"\n",
    "    error_type: str\n",
    "    message: str\n",
    "    context: Dict[str, Any]\n",
    "    function_name: str\n",
    "    timestamp: str\n",
    "    original_exception: Optional[Exception] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for logging\"\"\"\n",
    "        return {\n",
    "            \"error_type\": self.error_type,\n",
    "            \"message\": self.message,\n",
    "            \"context\": self.context,\n",
    "            \"function_name\": self.function_name,\n",
    "            \"timestamp\": self.timestamp,\n",
    "            \"exception_type\": type(self.original_exception).__name__ if self.original_exception else None\n",
    "        }\n",
    "\n",
    "def safe_transform(func: Callable[[DataFrame], DataFrame], \n",
    "                  function_name: str,\n",
    "                  logger: FunctionalLogger = None) -> Callable[[DataFrame], Result[DataFrame, PipelineError]]:\n",
    "    \"\"\"\n",
    "    Higher-order function that wraps DataFrame transformations with error handling\n",
    "    Returns a Result type instead of raising exceptions\n",
    "    \"\"\"\n",
    "    def wrapper(df: DataFrame) -> Result[DataFrame, PipelineError]:\n",
    "        try:\n",
    "            if logger:\n",
    "                logger.info(f\"Starting transformation: {function_name}\", \n",
    "                          context={\"input_columns\": df.columns, \"input_rows\": df.count()},\n",
    "                          function_name=function_name)\n",
    "            \n",
    "            result_df = func(df)\n",
    "            \n",
    "            if logger:\n",
    "                logger.info(f\"Transformation completed successfully: {function_name}\",\n",
    "                          context={\"output_columns\": result_df.columns, \"output_rows\": result_df.count()},\n",
    "                          function_name=function_name)\n",
    "            \n",
    "            return Ok(result_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error = PipelineError(\n",
    "                error_type=type(e).__name__,\n",
    "                message=str(e),\n",
    "                context={\"input_columns\": df.columns if df else [], \"function\": function_name},\n",
    "                function_name=function_name,\n",
    "                timestamp=datetime.now().isoformat(),\n",
    "                original_exception=e\n",
    "            )\n",
    "            \n",
    "            if logger:\n",
    "                logger.error(f\"Transformation failed: {function_name}\",\n",
    "                           context=error.to_dict(),\n",
    "                           function_name=function_name)\n",
    "            \n",
    "            return Err(error)\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "# Example transformation functions wrapped with error handling\n",
    "print(\"=== Functional Error Handling Examples ===\")\n",
    "\n",
    "# Create sample data for testing\n",
    "sample_data = [\n",
    "    (1, \"Alice\", 25, 1000.0),\n",
    "    (2, \"Bob\", 30, 1500.0),\n",
    "    (3, \"Charlie\", 35, 2000.0),\n",
    "    (4, \"Diana\", -5, 800.0),  # Invalid age for testing\n",
    "    (5, \"Eve\", 40, None)      # Null amount for testing\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "test_df = spark.createDataFrame(sample_data, schema)\n",
    "\n",
    "# Pure transformation functions\n",
    "def validate_ages(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function to validate age ranges\"\"\"\n",
    "    return df.filter((F.col(\"age\") >= 0) & (F.col(\"age\") <= 120))\n",
    "\n",
    "def calculate_bonus(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function to calculate bonus (might fail with null amounts)\"\"\"\n",
    "    return df.withColumn(\"bonus\", F.col(\"amount\") * 0.1)\n",
    "\n",
    "def add_age_category(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function to add age categories\"\"\"\n",
    "    return df.withColumn(\"age_category\",\n",
    "                        F.when(F.col(\"age\") < 30, \"Young\")\n",
    "                         .when(F.col(\"age\") < 50, \"Middle\")\n",
    "                         .otherwise(\"Senior\"))\n",
    "\n",
    "# Wrap functions with error handling\n",
    "safe_validate_ages = safe_transform(validate_ages, \"validate_ages\", functional_logger)\n",
    "safe_calculate_bonus = safe_transform(calculate_bonus, \"calculate_bonus\", functional_logger)\n",
    "safe_add_age_category = safe_transform(add_age_category, \"add_age_category\", functional_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functional error handling pipeline\n",
    "\n",
    "print(\"=== Testing Safe Transformation Pipeline ===\")\n",
    "\n",
    "# Test successful transformations\n",
    "print(\"\\n1. Testing successful transformations:\")\n",
    "result1 = safe_validate_ages(test_df)\n",
    "if result1.is_ok():\n",
    "    print(f\"\u2705 Age validation successful: {result1.value.count()} records\")\n",
    "    \n",
    "    result2 = safe_add_age_category(result1.value)\n",
    "    if result2.is_ok():\n",
    "        print(f\"\u2705 Age categorization successful: {result2.value.count()} records\")\n",
    "        result2.value.show()\n",
    "    else:\n",
    "        print(f\"\u274c Age categorization failed: {result2.error.message}\")\n",
    "else:\n",
    "    print(f\"\u274c Age validation failed: {result1.error.message}\")\n",
    "\n",
    "# Test transformation that might fail\n",
    "print(\"\\n2. Testing transformation with null handling:\")\n",
    "result3 = safe_calculate_bonus(test_df)\n",
    "if result3.is_ok():\n",
    "    print(f\"\u2705 Bonus calculation successful: {result3.value.count()} records\")\n",
    "    result3.value.show()\n",
    "else:\n",
    "    print(f\"\u274c Bonus calculation failed: {result3.error.message}\")\n",
    "    print(f\"   Error details: {json.dumps(result3.error.to_dict(), indent=2)}\")\n",
    "\n",
    "# Composition of safe transformations\n",
    "def compose_safe_transformations(df: DataFrame) -> Result[DataFrame, List[PipelineError]]:\n",
    "    \"\"\"\n",
    "    Compose multiple safe transformations\n",
    "    Returns either the final result or all accumulated errors\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    current_df = df\n",
    "    \n",
    "    # Chain transformations, collecting errors\n",
    "    transformations = [\n",
    "        safe_validate_ages,\n",
    "        safe_add_age_category,\n",
    "        safe_calculate_bonus\n",
    "    ]\n",
    "    \n",
    "    for transform in transformations:\n",
    "        result = transform(current_df)\n",
    "        if result.is_ok():\n",
    "            current_df = result.value\n",
    "        else:\n",
    "            errors.append(result.error)\n",
    "            # Decide whether to continue or stop\n",
    "            if result.error.error_type in [\"AnalysisException\", \"PySparkException\"]:\n",
    "                # Stop on critical errors\n",
    "                return Err(errors)\n",
    "    \n",
    "    if errors:\n",
    "        return Err(errors)\n",
    "    else:\n",
    "        return Ok(current_df)\n",
    "\n",
    "print(\"\\n3. Testing composed safe transformations:\")\n",
    "composed_result = compose_safe_transformations(test_df)\n",
    "\n",
    "if composed_result.is_ok():\n",
    "    print(\"\u2705 All transformations completed successfully\")\n",
    "    composed_result.value.show()\n",
    "else:\n",
    "    print(f\"\u274c Pipeline failed with {len(composed_result.error)} errors:\")\n",
    "    for i, error in enumerate(composed_result.error, 1):\n",
    "        print(f\"   {i}. {error.function_name}: {error.message}\")\n",
    "\n",
    "print(\"\\n\ud83d\udd04 Pipeline execution completed with comprehensive logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PySpark Exception Handling and Debugging\n",
    "\n",
    "Let's explore how to handle specific PySpark exceptions and extract useful debugging information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PySpark exception handling and debugging\n",
    "\n",
    "try:\n",
    "    # Try to import PySpark-specific exceptions\n",
    "    from pyspark.errors import PySparkException, AnalysisException\n",
    "    PYSPARK_EXCEPTIONS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    # Fallback for older Spark versions\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "    PySparkException = Exception  # Fallback\n",
    "    PYSPARK_EXCEPTIONS_AVAILABLE = False\n",
    "\n",
    "def extract_spark_error_info(exception: Exception) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract structured information from Spark exceptions\n",
    "    \"\"\"\n",
    "    error_info = {\n",
    "        \"exception_type\": type(exception).__name__,\n",
    "        \"message\": str(exception),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Extract additional info for newer PySpark versions\n",
    "    if PYSPARK_EXCEPTIONS_AVAILABLE and hasattr(exception, 'getErrorClass'):\n",
    "        try:\n",
    "            error_info.update({\n",
    "                \"error_class\": exception.getErrorClass(),\n",
    "                \"sql_state\": exception.getSqlState() if hasattr(exception, 'getSqlState') else None,\n",
    "                \"message_parameters\": exception.getMessageParameters() if hasattr(exception, 'getMessageParameters') else {}\n",
    "            })\n",
    "        except:\n",
    "            pass  # Fallback gracefully\n",
    "    \n",
    "    return error_info\n",
    "\n",
    "def create_spark_error_demonstrations():\n",
    "    \"\"\"\n",
    "    Create various Spark errors for demonstration purposes\n",
    "    \"\"\"\n",
    "    demonstrations = []\n",
    "    \n",
    "    # 1. Table not found error\n",
    "    def demo_table_not_found():\n",
    "        try:\n",
    "            spark.sql(\"SELECT * FROM non_existent_table\").show()\n",
    "        except Exception as e:\n",
    "            return extract_spark_error_info(e)\n",
    "    \n",
    "    # 2. Column not found error\n",
    "    def demo_column_not_found():\n",
    "        try:\n",
    "            test_df.select(\"nonexistent_column\").show()\n",
    "        except Exception as e:\n",
    "            return extract_spark_error_info(e)\n",
    "    \n",
    "    # 3. Type mismatch error\n",
    "    def demo_type_mismatch():\n",
    "        try:\n",
    "            spark.sql(\"SELECT 'text' / 5\").show()\n",
    "        except Exception as e:\n",
    "            return extract_spark_error_info(e)\n",
    "    \n",
    "    # 4. Division by zero error\n",
    "    def demo_division_by_zero():\n",
    "        try:\n",
    "            spark.sql(\"SELECT 10 / 0\").show()\n",
    "        except Exception as e:\n",
    "            return extract_spark_error_info(e)\n",
    "    \n",
    "    return {\n",
    "        \"table_not_found\": demo_table_not_found,\n",
    "        \"column_not_found\": demo_column_not_found,\n",
    "        \"type_mismatch\": demo_type_mismatch,\n",
    "        \"division_by_zero\": demo_division_by_zero\n",
    "    }\n",
    "\n",
    "print(\"=== Spark Exception Handling Demonstrations ===\")\n",
    "\n",
    "demonstrations = create_spark_error_demonstrations()\n",
    "\n",
    "# Run demonstrations and log errors\n",
    "for demo_name, demo_func in demonstrations.items():\n",
    "    try:\n",
    "        print(f\"\\n--- {demo_name.replace('_', ' ').title()} Error ---\")\n",
    "        error_info = demo_func()\n",
    "        if error_info:\n",
    "            functional_logger.error(\n",
    "                f\"Demonstration error: {demo_name}\",\n",
    "                context=error_info,\n",
    "                function_name=\"error_demonstration\"\n",
    "            )\n",
    "            \n",
    "            print(f\"Error Type: {error_info['exception_type']}\")\n",
    "            print(f\"Message: {error_info['message'][:100]}...\")\n",
    "            \n",
    "            if 'error_class' in error_info and error_info['error_class']:\n",
    "                print(f\"Error Class: {error_info['error_class']}\")\n",
    "            if 'sql_state' in error_info and error_info['sql_state']:\n",
    "                print(f\"SQL State: {error_info['sql_state']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in demonstration: {e}\")\n",
    "\n",
    "# Advanced error handling with recovery strategies\n",
    "print(\"\\n=== Advanced Error Handling with Recovery ===\")\n",
    "\n",
    "def safe_sql_execution(sql_query: str, \n",
    "                      recovery_strategies: List[Callable[[], DataFrame]] = None) -> Result[DataFrame, PipelineError]:\n",
    "    \"\"\"\n",
    "    Execute SQL with recovery strategies for common errors\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result_df = spark.sql(sql_query)\n",
    "        \n",
    "        functional_logger.info(\n",
    "            \"SQL execution successful\",\n",
    "            context={\"query\": sql_query[:100], \"columns\": result_df.columns},\n",
    "            function_name=\"safe_sql_execution\"\n",
    "        )\n",
    "        \n",
    "        return Ok(result_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_info = extract_spark_error_info(e)\n",
    "        \n",
    "        functional_logger.warning(\n",
    "            \"SQL execution failed, attempting recovery\",\n",
    "            context=error_info,\n",
    "            function_name=\"safe_sql_execution\"\n",
    "        )\n",
    "        \n",
    "        # Try recovery strategies\n",
    "        if recovery_strategies:\n",
    "            for i, recovery_func in enumerate(recovery_strategies):\n",
    "                try:\n",
    "                    functional_logger.info(\n",
    "                        f\"Attempting recovery strategy {i+1}\",\n",
    "                        context={\"strategy_index\": i},\n",
    "                        function_name=\"safe_sql_execution\"\n",
    "                    )\n",
    "                    \n",
    "                    recovery_df = recovery_func()\n",
    "                    \n",
    "                    functional_logger.info(\n",
    "                        f\"Recovery strategy {i+1} successful\",\n",
    "                        context={\"strategy_index\": i, \"result_columns\": recovery_df.columns},\n",
    "                        function_name=\"safe_sql_execution\"\n",
    "                    )\n",
    "                    \n",
    "                    return Ok(recovery_df)\n",
    "                    \n",
    "                except Exception as recovery_error:\n",
    "                    functional_logger.warning(\n",
    "                        f\"Recovery strategy {i+1} failed\",\n",
    "                        context={\"strategy_index\": i, \"recovery_error\": str(recovery_error)},\n",
    "                        function_name=\"safe_sql_execution\"\n",
    "                    )\n",
    "                    continue\n",
    "        \n",
    "        # All recovery attempts failed\n",
    "        pipeline_error = PipelineError(\n",
    "            error_type=error_info[\"exception_type\"],\n",
    "            message=error_info[\"message\"],\n",
    "            context=error_info,\n",
    "            function_name=\"safe_sql_execution\",\n",
    "            timestamp=error_info[\"timestamp\"],\n",
    "            original_exception=e\n",
    "        )\n",
    "        \n",
    "        return Err(pipeline_error)\n",
    "\n",
    "# Test SQL execution with recovery\n",
    "print(\"\\nTesting SQL execution with recovery strategies...\")\n",
    "\n",
    "# Recovery strategies for table not found\n",
    "def create_fallback_table():\n",
    "    \"\"\"Recovery strategy: create a fallback empty DataFrame\"\"\"\n",
    "    return spark.createDataFrame([], StructType([StructField(\"fallback\", StringType(), True)]))\n",
    "\n",
    "def use_existing_table():\n",
    "    \"\"\"Recovery strategy: use an existing table\"\"\"\n",
    "    return test_df\n",
    "\n",
    "# Test with recovery\n",
    "sql_result = safe_sql_execution(\n",
    "    \"SELECT * FROM non_existent_table\",\n",
    "    recovery_strategies=[use_existing_table, create_fallback_table]\n",
    ")\n",
    "\n",
    "if sql_result.is_ok():\n",
    "    print(\"\u2705 SQL execution successful (possibly with recovery)\")\n",
    "    sql_result.value.show(3)\n",
    "else:\n",
    "    print(f\"\u274c SQL execution failed: {sql_result.error.message}\")\n",
    "\n",
    "print(\"\\n\ud83d\udd27 Advanced error handling patterns demonstrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Log Storage and Analysis Patterns\n",
    "\n",
    "Let's explore how to store logs in a structured way and analyze them for monitoring and debugging purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}