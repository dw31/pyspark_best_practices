{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5.3 Minimizing Data Shuffling and Addressing Data Skew\n",
        "\n",
        "This notebook explores strategies for minimizing expensive shuffle operations and handling data skew in PySpark applications while maintaining functional programming principles.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand how to:\n",
        "- Identify and understand shuffle operations in Spark\n",
        "- Minimize shuffles through strategic partitioning and join strategies\n",
        "- Detect and diagnose data skew in distributed processing\n",
        "- Apply skew remediation techniques (filtering, AQE, salting)\n",
        "- Use partitioning strategies (`repartition` vs `coalesce`) effectively\n",
        "- Optimize functional pipelines for distributed performance\n",
        "- Monitor and tune shuffle behavior with Spark UI\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Understanding of PySpark DataFrames and transformations\n",
        "- Knowledge of functional programming concepts\n",
        "- Familiarity with Spark's execution model\n",
        "- Basic understanding of distributed computing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import random\n",
        "import time\n",
        "from functools import reduce\n",
        "\n",
        "# Initialize Spark session\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"ShuffleOptimization\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "print(\"✅ Setup complete - Ready for shuffle optimization!\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"AQE Enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
        "print(f\"Skew Join Optimization: {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding Shuffle Operations\n",
        "\n",
        "**Shuffle** is the process of redistributing data across partitions, which may or may not involve moving data across executors. It's one of the most expensive operations in Spark.\n",
        "\n",
        "### When Shuffles Occur\n",
        "\n",
        "Shuffles happen during \"wide\" transformations:\n",
        "- **Joins**: `join()`, `crossJoin()`\n",
        "- **Aggregations**: `groupBy()`, `agg()`, `reduceByKey()`\n",
        "- **Window Functions**: Operations with `partitionBy`\n",
        "- **Repartitioning**: `repartition()`, `coalesce()` (sometimes)\n",
        "- **Sorting**: `orderBy()`, `sortBy()`\n",
        "- **Distinct**: `distinct()`, `dropDuplicates()`\n",
        "\n",
        "### Why Shuffles Are Expensive\n",
        "\n",
        "1. **Network I/O**: Data must be transferred between executors\n",
        "2. **Disk I/O**: Intermediate data written to disk during shuffle\n",
        "3. **Serialization/Deserialization**: Data must be serialized for network transfer\n",
        "4. **Memory Pressure**: Can cause spills to disk if memory insufficient\n",
        "5. **Synchronization**: All partitions must complete before next stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample datasets to demonstrate shuffle operations\n",
        "\n",
        "def create_orders_data(num_orders: int = 10000) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Pure function to create orders dataset.\n",
        "    Includes some skewed data for demonstration.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create skewed distribution - 70% of orders from top 3 customers\n",
        "    skewed_customer_ids = [1, 2, 3]  # Heavy customers\n",
        "    normal_customer_ids = list(range(4, 101))  # Normal customers\n",
        "    \n",
        "    data = []\n",
        "    for i in range(num_orders):\n",
        "        # 70% of orders go to top 3 customers (skewed)\n",
        "        if random.random() < 0.7:\n",
        "            customer_id = random.choice(skewed_customer_ids)\n",
        "        else:\n",
        "            customer_id = random.choice(normal_customer_ids)\n",
        "        \n",
        "        data.append((\n",
        "            i + 1,  # order_id\n",
        "            customer_id,\n",
        "            f\"Product_{random.randint(1, 50)}\",\n",
        "            random.randint(1, 10),  # quantity\n",
        "            round(random.uniform(10, 500), 2),  # price\n",
        "            f\"2024-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}\"\n",
        "        ))\n",
        "    \n",
        "    schema = StructType([\n",
        "        StructField(\"order_id\", IntegerType(), False),\n",
        "        StructField(\"customer_id\", IntegerType(), False),\n",
        "        StructField(\"product\", StringType(), False),\n",
        "        StructField(\"quantity\", IntegerType(), False),\n",
        "        StructField(\"price\", DoubleType(), False),\n",
        "        StructField(\"order_date\", StringType(), False)\n",
        "    ])\n",
        "    \n",
        "    return spark.createDataFrame(data, schema)\n",
        "\n",
        "def create_customers_data(num_customers: int = 100) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Pure function to create customers dataset.\n",
        "    \"\"\"\n",
        "    data = [\n",
        "        (\n",
        "            i,\n",
        "            f\"Customer_{i}\",\n",
        "            f\"customer{i}@example.com\",\n",
        "            random.choice([\"Bronze\", \"Silver\", \"Gold\", \"Platinum\"]),\n",
        "            random.choice([\"US\", \"CA\", \"UK\", \"DE\", \"FR\", \"JP\"])\n",
        "        )\n",
        "        for i in range(1, num_customers + 1)\n",
        "    ]\n",
        "    \n",
        "    schema = StructType([\n",
        "        StructField(\"customer_id\", IntegerType(), False),\n",
        "        StructField(\"name\", StringType(), False),\n",
        "        StructField(\"email\", StringType(), False),\n",
        "        StructField(\"tier\", StringType(), False),\n",
        "        StructField(\"country\", StringType(), False)\n",
        "    ])\n",
        "    \n",
        "    return spark.createDataFrame(data, schema)\n",
        "\n",
        "# Create datasets\n",
        "orders_df = create_orders_data(10000)\n",
        "customers_df = create_customers_data(100)\n",
        "\n",
        "print(f\"Orders: {orders_df.count():,} records, {orders_df.rdd.getNumPartitions()} partitions\")\n",
        "print(f\"Customers: {customers_df.count():,} records, {customers_df.rdd.getNumPartitions()} partitions\")\n",
        "\n",
        "print(\"\\nSample orders:\")\n",
        "orders_df.show(5)\n",
        "\n",
        "print(\"Sample customers:\")\n",
        "customers_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Visualizing Shuffle Behavior\n",
        "\n",
        "Let's examine how different operations cause shuffles and their impact on performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"OPERATIONS THAT CAUSE SHUFFLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Operation 1: GroupBy Aggregation (Wide Transformation - Shuffles)\n",
        "print(\"\\n1. GroupBy Aggregation (SHUFFLES):\")\n",
        "start = time.time()\n",
        "customer_totals = (\n",
        "    orders_df\n",
        "    .groupBy(\"customer_id\")\n",
        "    .agg(\n",
        "        F.sum(F.col(\"quantity\") * F.col(\"price\")).alias(\"total_spent\"),\n",
        "        F.count(\"*\").alias(\"order_count\")\n",
        "    )\n",
        ")\n",
        "result_count = customer_totals.count()\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"   Result: {result_count} customers\")\n",
        "print(f\"   Time: {elapsed:.2f}s\")\n",
        "print(f\"   Partitions: {customer_totals.rdd.getNumPartitions()}\")\n",
        "print(f\"   ⚠️  Shuffle required: Data redistributed by customer_id\")\n",
        "\n",
        "# Operation 2: Join (Wide Transformation - Shuffles)\n",
        "print(\"\\n2. Join Operation (SHUFFLES):\")\n",
        "start = time.time()\n",
        "enriched_orders = orders_df.join(customers_df, \"customer_id\")\n",
        "result_count = enriched_orders.count()\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"   Result: {result_count:,} enriched orders\")\n",
        "print(f\"   Time: {elapsed:.2f}s\")\n",
        "print(f\"   Partitions: {enriched_orders.rdd.getNumPartitions()}\")\n",
        "print(f\"   ⚠️  Shuffle required: Both sides may shuffle unless broadcasted\")\n",
        "\n",
        "# Operation 3: OrderBy (Wide Transformation - Shuffles)\n",
        "print(\"\\n3. OrderBy (SHUFFLES):\")\n",
        "start = time.time()\n",
        "sorted_orders = orders_df.orderBy(F.desc(\"price\"))\n",
        "result_count = sorted_orders.count()\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"   Result: {result_count:,} sorted orders\")\n",
        "print(f\"   Time: {elapsed:.2f}s\")\n",
        "print(f\"   Partitions: {sorted_orders.rdd.getNumPartitions()}\")\n",
        "print(f\"   ⚠️  Shuffle required: Data must be sorted globally\")\n",
        "\n",
        "# Operation 4: Filter (Narrow Transformation - No Shuffle)\n",
        "print(\"\\n4. Filter Operation (NO SHUFFLE):\")\n",
        "start = time.time()\n",
        "high_value_orders = orders_df.filter(F.col(\"price\") > 400)\n",
        "result_count = high_value_orders.count()\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"   Result: {result_count:,} high-value orders\")\n",
        "print(f\"   Time: {elapsed:.2f}s\")\n",
        "print(f\"   Partitions: {high_value_orders.rdd.getNumPartitions()}\")\n",
        "print(f\"   ✅ No shuffle: Narrow transformation, processed in-place\")\n",
        "\n",
        "# Operation 5: Select (Narrow Transformation - No Shuffle)\n",
        "print(\"\\n5. Select/WithColumn (NO SHUFFLE):\")\n",
        "start = time.time()\n",
        "transformed = orders_df.select(\n",
        "    \"order_id\",\n",
        "    \"customer_id\",\n",
        "    (F.col(\"quantity\") * F.col(\"price\")).alias(\"total\")\n",
        ")\n",
        "result_count = transformed.count()\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"   Result: {result_count:,} transformed orders\")\n",
        "print(f\"   Time: {elapsed:.2f}s\")\n",
        "print(f\"   Partitions: {transformed.rdd.getNumPartitions()}\")\n",
        "print(f\"   ✅ No shuffle: Narrow transformation, processed in-place\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"💡 Key Insight: Wide transformations (groupBy, join, orderBy) cause shuffles\")\n",
        "print(\"   Narrow transformations (filter, select) process data in-place\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Partitioning Strategies: `repartition` vs `coalesce`\n",
        "\n",
        "Understanding when to use `repartition()` vs `coalesce()` is critical for performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"REPARTITION vs COALESCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create a dataset with many partitions\n",
        "many_partitions_df = orders_df.repartition(50)\n",
        "print(f\"\\nStarting partitions: {many_partitions_df.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Strategy 1: Coalesce (Efficient for reducing partitions)\n",
        "print(\"\\n1. Using coalesce() to reduce partitions:\")\n",
        "start = time.time()\n",
        "coalesced_df = many_partitions_df.coalesce(5)\n",
        "coalesced_df.write.mode(\"overwrite\").format(\"noop\").save()  # Trigger action\n",
        "coalesce_time = time.time() - start\n",
        "\n",
        "print(f\"   Final partitions: {coalesced_df.rdd.getNumPartitions()}\")\n",
        "print(f\"   Time: {coalesce_time:.2f}s\")\n",
        "print(f\"   ✅ No full shuffle: Combines existing partitions efficiently\")\n",
        "print(f\"   Use case: Reducing partitions before writing to disk\")\n",
        "\n",
        "# Strategy 2: Repartition (Full shuffle for even distribution)\n",
        "print(\"\\n2. Using repartition() to reduce partitions:\")\n",
        "start = time.time()\n",
        "repartitioned_df = many_partitions_df.repartition(5)\n",
        "repartitioned_df.write.mode(\"overwrite\").format(\"noop\").save()  # Trigger action\n",
        "repartition_time = time.time() - start\n",
        "\n",
        "print(f\"   Final partitions: {repartitioned_df.rdd.getNumPartitions()}\")\n",
        "print(f\"   Time: {repartition_time:.2f}s\")\n",
        "print(f\"   ⚠️  Full shuffle: Evenly redistributes all data\")\n",
        "print(f\"   Use case: Balancing skewed data or increasing parallelism\")\n",
        "\n",
        "# Performance comparison\n",
        "print(f\"\\n📊 Performance Comparison:\")\n",
        "print(f\"   coalesce() time: {coalesce_time:.2f}s\")\n",
        "print(f\"   repartition() time: {repartition_time:.2f}s\")\n",
        "print(f\"   Speedup: {repartition_time/coalesce_time:.1f}x faster with coalesce()\")\n",
        "\n",
        "# Partition-based repartitioning (for skew handling)\n",
        "print(\"\\n3. Repartitioning by column (for co-location):\")\n",
        "partitioned_by_customer = orders_df.repartition(\"customer_id\")\n",
        "print(f\"   Partitions: {partitioned_by_customer.rdd.getNumPartitions()}\")\n",
        "print(f\"   ✅ Co-locates data by customer_id for efficient joins\")\n",
        "print(f\"   Use case: Pre-partitioning before joins or groupBy\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"💡 Decision Guide:\")\n",
        "print(\"   • coalesce(): Reducing partitions, no data balancing needed\")\n",
        "print(\"   • repartition(n): Need even distribution or increasing partitions\")\n",
        "print(\"   • repartition(col): Co-locate data for joins/aggregations\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Detecting Data Skew\n",
        "\n",
        "Data skew occurs when some partitions have significantly more data than others, leading to uneven workload distribution and slow tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"DATA SKEW DETECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Analyze data distribution by customer\n",
        "print(\"\\n1. Analyzing order distribution by customer:\")\n",
        "customer_order_counts = (\n",
        "    orders_df\n",
        "    .groupBy(\"customer_id\")\n",
        "    .agg(F.count(\"*\").alias(\"order_count\"))\n",
        "    .orderBy(F.desc(\"order_count\"))\n",
        ")\n",
        "\n",
        "# Get statistics\n",
        "stats = customer_order_counts.select(\n",
        "    F.min(\"order_count\").alias(\"min\"),\n",
        "    F.max(\"order_count\").alias(\"max\"),\n",
        "    F.avg(\"order_count\").alias(\"avg\"),\n",
        "    F.stddev(\"order_count\").alias(\"stddev\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"   Min orders per customer: {stats['min']}\")\n",
        "print(f\"   Max orders per customer: {stats['max']}\")\n",
        "print(f\"   Avg orders per customer: {stats['avg']:.1f}\")\n",
        "print(f\"   Stddev: {stats['stddev']:.1f}\")\n",
        "print(f\"   Skew ratio (max/avg): {stats['max']/stats['avg']:.1f}x\")\n",
        "\n",
        "if stats['max'] / stats['avg'] > 3:\n",
        "    print(f\"   ⚠️  SIGNIFICANT SKEW DETECTED!\")\n",
        "else:\n",
        "    print(f\"   ✅ Relatively balanced distribution\")\n",
        "\n",
        "# Show top skewed customers\n",
        "print(\"\\n   Top 10 customers by order count:\")\n",
        "customer_order_counts.show(10)\n",
        "\n",
        "# Partition size analysis\n",
        "print(\"\\n2. Analyzing partition sizes after groupBy:\")\n",
        "\n",
        "def analyze_partition_sizes(df: DataFrame, operation_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Pure function to analyze partition size distribution.\n",
        "    \"\"\"\n",
        "    # Get partition sizes\n",
        "    partition_sizes = df.rdd.mapPartitions(\n",
        "        lambda iterator: [sum(1 for _ in iterator)]\n",
        "    ).collect()\n",
        "    \n",
        "    if partition_sizes:\n",
        "        min_size = min(partition_sizes)\n",
        "        max_size = max(partition_sizes)\n",
        "        avg_size = sum(partition_sizes) / len(partition_sizes)\n",
        "        \n",
        "        print(f\"\\n   {operation_name}:\")\n",
        "        print(f\"   Total partitions: {len(partition_sizes)}\")\n",
        "        print(f\"   Min partition size: {min_size} records\")\n",
        "        print(f\"   Max partition size: {max_size} records\")\n",
        "        print(f\"   Avg partition size: {avg_size:.1f} records\")\n",
        "        print(f\"   Skew ratio (max/avg): {max_size/avg_size if avg_size > 0 else 0:.1f}x\")\n",
        "        \n",
        "        if max_size / avg_size > 2:\n",
        "            print(f\"   ⚠️  Partition skew detected!\")\n",
        "        else:\n",
        "            print(f\"   ✅ Balanced partitions\")\n",
        "\n",
        "# Analyze original data\n",
        "analyze_partition_sizes(orders_df, \"Original orders data\")\n",
        "\n",
        "# Analyze after groupBy\n",
        "grouped_df = orders_df.groupBy(\"customer_id\").agg(F.count(\"*\").alias(\"count\"))\n",
        "analyze_partition_sizes(grouped_df, \"After groupBy customer_id\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"💡 Skew Detection Techniques:\")\n",
        "print(\"   1. Analyze key distribution with groupBy + count\")\n",
        "print(\"   2. Monitor Spark UI for long-running tasks\")\n",
        "print(\"   3. Check partition size distribution\")\n",
        "print(\"   4. Look for skew ratio > 2-3x average\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Skew Remediation Strategies\n",
        "\n",
        "Multiple strategies exist for handling data skew, each with different trade-offs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SKEW REMEDIATION STRATEGIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Strategy 1: Filter out skewed values\n",
        "print(\"\\n1. Strategy: Filter Out Skewed Values\")\n",
        "print(\"   Use case: Null or special values causing skew\\n\")\n",
        "\n",
        "# Identify top skewed customers\n",
        "top_customers = (\n",
        "    orders_df\n",
        "    .groupBy(\"customer_id\")\n",
        "    .agg(F.count(\"*\").alias(\"order_count\"))\n",
        "    .filter(F.col(\"order_count\") > 1000)  # Threshold for \"heavy\" customers\n",
        "    .select(\"customer_id\")\n",
        ").collect()\n",
        "\n",
        "skewed_customer_ids = [row.customer_id for row in top_customers]\n",
        "print(f\"   Identified {len(skewed_customer_ids)} skewed customers: {skewed_customer_ids}\")\n",
        "\n",
        "# Process normal and skewed separately\n",
        "normal_orders = orders_df.filter(~F.col(\"customer_id\").isin(skewed_customer_ids))\n",
        "skewed_orders = orders_df.filter(F.col(\"customer_id\").isin(skewed_customer_ids))\n",
        "\n",
        "print(f\"   Normal orders: {normal_orders.count():,}\")\n",
        "print(f\"   Skewed orders: {skewed_orders.count():,}\")\n",
        "print(f\"   ✅ Process separately with different strategies\")\n",
        "\n",
        "# Strategy 2: Salting (Advanced technique)\n",
        "print(\"\\n2. Strategy: Salting Keys\")\n",
        "print(\"   Use case: Cannot filter skewed values, need even distribution\\n\")\n",
        "\n",
        "def salt_dataframe(df: DataFrame, key_col: str, salt_range: int = 10) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Pure function to add salt to skewed keys.\n",
        "    Distributes skewed keys across multiple partitions.\n",
        "    \"\"\"\n",
        "    return df.withColumn(\n",
        "        \"salted_key\",\n",
        "        F.concat(\n",
        "            F.col(key_col).cast(\"string\"),\n",
        "            F.lit(\"_\"),\n",
        "            (F.rand() * salt_range).cast(\"int\").cast(\"string\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Apply salting to skewed data\n",
        "salted_df = salt_dataframe(skewed_orders, \"customer_id\", salt_range=10)\n",
        "print(f\"   Original customer_id: 1\")\n",
        "print(f\"   Salted keys: 1_0, 1_1, 1_2, ..., 1_9\")\n",
        "print(f\"   ✅ Distributes one hot key across 10 partitions\")\n",
        "\n",
        "# For joins, need to explode the other side\n",
        "def explode_for_salt_join(df: DataFrame, key_col: str, salt_range: int = 10) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Pure function to explode DataFrame for salted join.\n",
        "    Creates copies with all possible salt values.\n",
        "    \"\"\"\n",
        "    salt_values = list(range(salt_range))\n",
        "    return df.withColumn(\n",
        "        \"salt\",\n",
        "        F.explode(F.array(*[F.lit(i) for i in salt_values]))\n",
        "    ).withColumn(\n",
        "        \"salted_key\",\n",
        "        F.concat(\n",
        "            F.col(key_col).cast(\"string\"),\n",
        "            F.lit(\"_\"),\n",
        "            F.col(\"salt\").cast(\"string\")\n",
        "        )\n",
        "    ).drop(\"salt\")\n",
        "\n",
        "print(f\"\\n   For joins with salted data:\")\n",
        "print(f\"   • Salt the large skewed table\")\n",
        "print(f\"   • Explode the smaller table with all salt values\")\n",
        "print(f\"   • Join on salted_key\")\n",
        "print(f\"   ⚠️  Trade-off: Increases data size but distributes load\")\n",
        "\n",
        "# Strategy 3: Adaptive Query Execution (AQE)\n",
        "print(\"\\n3. Strategy: Adaptive Query Execution (AQE)\")\n",
        "print(\"   Use case: Automatic skew handling (Spark 3.0+)\\n\")\n",
        "\n",
        "print(f\"   AQE Status: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
        "print(f\"   Skew Join: {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")\n",
        "print(f\"   ✅ Automatically splits skewed partitions during joins\")\n",
        "print(f\"   No code changes required - handled by Spark\")\n",
        "\n",
        "# Strategy 4: Increase Parallelism\n",
        "print(\"\\n4. Strategy: Increase Parallelism\")\n",
        "print(\"   Use case: Skew is moderate, just need more tasks\\n\")\n",
        "\n",
        "current_partitions = int(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
        "print(f\"   Current shuffle partitions: {current_partitions}\")\n",
        "print(f\"   Recommendation: Increase to 2x-4x for skewed workloads\")\n",
        "print(f\"   Example: spark.conf.set('spark.sql.shuffle.partitions', '400')\")\n",
        "print(f\"   ✅ More partitions = smaller chunks, less impact from skew\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"💡 Strategy Selection Guide:\")\n",
        "print(\"   1. AQE (Default): Enable and let Spark handle it automatically\")\n",
        "print(\"   2. Filter: Remove skewed values if they can be processed separately\")\n",
        "print(\"   3. Increase Partitions: Simple fix for moderate skew\")\n",
        "print(\"   4. Salting: Last resort for severe skew that can't be filtered\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Join Optimization Strategies\n",
        "\n",
        "Different join strategies have very different shuffle characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"JOIN OPTIMIZATION STRATEGIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Strategy 1: Sort-Merge Join (Default for large tables)\n",
        "print(\"\\n1. Sort-Merge Join (Default):\")\n",
        "print(\"   Both sides shuffled and sorted by join key\\n\")\n",
        "\n",
        "start = time.time()\n",
        "sort_merge_join = orders_df.join(customers_df, \"customer_id\")\n",
        "result_count = sort_merge_join.count()\n",
        "sort_merge_time = time.time() - start\n",
        "\n",
        "print(f\"   Result: {result_count:,} records\")\n",
        "print(f\"   Time: {sort_merge_time:.2f}s\")\n",
        "print(f\"   Shuffle: Both sides\")\n",
        "print(f\"   Use case: Both tables large, can't broadcast\")\n",
        "\n",
        "# Strategy 2: Broadcast Join (No shuffle for large table)\n",
        "print(\"\\n2. Broadcast Hash Join:\")\n",
        "print(\"   Small table broadcasted, large table not shuffled\\n\")\n",
        "\n",
        "start = time.time()\n",
        "broadcast_join = orders_df.join(F.broadcast(customers_df), \"customer_id\")\n",
        "result_count = broadcast_join.count()\n",
        "broadcast_time = time.time() - start\n",
        "\n",
        "print(f\"   Result: {result_count:,} records\")\n",
        "print(f\"   Time: {broadcast_time:.2f}s\")\n",
        "print(f\"   Speedup: {sort_merge_time/broadcast_time:.1f}x faster\")\n",
        "print(f\"   Shuffle: Only small table (broadcast)\")\n",
        "print(f\"   Use case: One table < 10MB (configurable)\")\n",
        "\n",
        "# Strategy 3: Bucketed Join (Pre-partitioned tables)\n",
        "print(\"\\n3. Bucketed Join (Delta Lake):\")\n",
        "print(\"   Pre-partitioned tables, no shuffle needed\\n\")\n",
        "\n",
        "print(\"   Setup:\")\n",
        "print(\"   • Write orders bucketed by customer_id\")\n",
        "print(\"   • Write customers bucketed by customer_id\")\n",
        "print(\"   • Join on customer_id = no shuffle!\")\n",
        "print(\"   \")\n",
        "print(\"   Example:\")\n",
        "print(\"   orders_df.write.format('delta')\")\n",
        "print(\"       .bucketBy(20, 'customer_id')\")\n",
        "print(\"       .save('/delta/orders')\")\n",
        "print(\"   \")\n",
        "print(\"   ✅ Zero shuffle for joins on bucket column\")\n",
        "print(\"   Use case: Repeated joins on same key\")\n",
        "\n",
        "# Strategy 4: Partitioned Join (Co-located data)\n",
        "print(\"\\n4. Partitioned Join:\")\n",
        "print(\"   Repartition both sides by join key\\n\")\n",
        "\n",
        "# Pre-partition both DataFrames\n",
        "orders_partitioned = orders_df.repartition(\"customer_id\")\n",
        "customers_partitioned = customers_df.repartition(\"customer_id\")\n",
        "\n",
        "start = time.time()\n",
        "partitioned_join = orders_partitioned.join(customers_partitioned, \"customer_id\")\n",
        "result_count = partitioned_join.count()\n",
        "partitioned_time = time.time() - start\n",
        "\n",
        "print(f\"   Result: {result_count:,} records\")\n",
        "print(f\"   Time: {partitioned_time:.2f}s\")\n",
        "print(f\"   Shuffle: Both sides (once, during repartition)\")\n",
        "print(f\"   Use case: Multiple operations on same key\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"💡 Join Strategy Decision Tree:\")\n",
        "print(\"   1. Small table (<10MB)? → Broadcast Join\")\n",
        "print(\"   2. Bucketed tables? → Bucketed Join\")\n",
        "print(\"   3. Multiple operations on same key? → Repartition first\")\n",
        "print(\"   4. Otherwise → Let AQE choose (usually sort-merge)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Functional Patterns for Shuffle Optimization\n",
        "\n",
        "Let's create reusable functional utilities for shuffle optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Callable\n",
        "\n",
        "@dataclass\n",
        "class ShuffleMetrics:\n",
        "    \"\"\"Immutable metrics for shuffle analysis\"\"\"\n",
        "    operation: str\n",
        "    input_partitions: int\n",
        "    output_partitions: int\n",
        "    execution_time: float\n",
        "    record_count: int\n",
        "    shuffle_occurred: bool\n",
        "    \n",
        "    def __str__(self) -> str:\n",
        "        shuffle_status = \"✅ No Shuffle\" if not self.shuffle_occurred else \"⚠️  Shuffle\"\n",
        "        return (\n",
        "            f\"{self.operation}:\\n\"\n",
        "            f\"  Records: {self.record_count:,}\\n\"\n",
        "            f\"  Partitions: {self.input_partitions} → {self.output_partitions}\\n\"\n",
        "            f\"  Time: {self.execution_time:.2f}s\\n\"\n",
        "            f\"  {shuffle_status}\"\n",
        "        )\n",
        "\n",
        "def measure_operation(\n",
        "    df: DataFrame,\n",
        "    operation: Callable[[DataFrame], DataFrame],\n",
        "    operation_name: str\n",
        ") -> Tuple[DataFrame, ShuffleMetrics]:\n",
        "    \"\"\"\n",
        "    Pure function to measure shuffle behavior of an operation.\n",
        "    \"\"\"\n",
        "    input_partitions = df.rdd.getNumPartitions()\n",
        "    \n",
        "    start = time.time()\n",
        "    result_df = operation(df)\n",
        "    record_count = result_df.count()  # Trigger execution\n",
        "    execution_time = time.time() - start\n",
        "    \n",
        "    output_partitions = result_df.rdd.getNumPartitions()\n",
        "    \n",
        "    # Heuristic: shuffle likely occurred if partition count changed\n",
        "    shuffle_occurred = input_partitions != output_partitions\n",
        "    \n",
        "    metrics = ShuffleMetrics(\n",
        "        operation=operation_name,\n",
        "        input_partitions=input_partitions,\n",
        "        output_partitions=output_partitions,\n",
        "        execution_time=execution_time,\n",
        "        record_count=record_count,\n",
        "        shuffle_occurred=shuffle_occurred\n",
        "    )\n",
        "    \n",
        "    return result_df, metrics\n",
        "\n",
        "def optimize_partitioning(\n",
        "    df: DataFrame,\n",
        "    target_partition_size_mb: int = 128,\n",
        "    avg_row_size_bytes: int = 500\n",
        ") -> DataFrame:\n",
        "    \"\"\"\n",
        "    Pure function to calculate optimal partition count.\n",
        "    Returns optimally repartitioned DataFrame.\n",
        "    \"\"\"\n",
        "    row_count = df.count()\n",
        "    estimated_size_mb = (row_count * avg_row_size_bytes) / (1024 * 1024)\n",
        "    \n",
        "    target_partitions = max(1, int(estimated_size_mb / target_partition_size_mb))\n",
        "    current_partitions = df.rdd.getNumPartitions()\n",
        "    \n",
        "    print(f\"Partition Optimization Analysis:\")\n",
        "    print(f\"  Estimated size: {estimated_size_mb:.1f} MB\")\n",
        "    print(f\"  Current partitions: {current_partitions}\")\n",
        "    print(f\"  Target partitions: {target_partitions}\")\n",
        "    print(f\"  Target size per partition: {target_partition_size_mb} MB\")\n",
        "    \n",
        "    if target_partitions < current_partitions:\n",
        "        print(f\"  → Using coalesce() to reduce partitions\")\n",
        "        return df.coalesce(target_partitions)\n",
        "    elif target_partitions > current_partitions:\n",
        "        print(f\"  → Using repartition() to increase partitions\")\n",
        "        return df.repartition(target_partitions)\n",
        "    else:\n",
        "        print(f\"  → Current partitioning is optimal\")\n",
        "        return df\n",
        "\n",
        "# Test the utilities\n",
        "print(\"=\"*80)\n",
        "print(\"FUNCTIONAL SHUFFLE OPTIMIZATION UTILITIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test 1: Measure groupBy operation\n",
        "print(\"\\n1. Measuring groupBy operation:\")\n",
        "result_df, metrics = measure_operation(\n",
        "    orders_df,\n",
        "    lambda df: df.groupBy(\"customer_id\").agg(F.sum(\"quantity\").alias(\"total_qty\")),\n",
        "    \"GroupBy Aggregation\"\n",
        ")\n",
        "print(metrics)\n",
        "\n",
        "# Test 2: Optimize partitioning\n",
        "print(\"\\n2. Optimizing partition count:\")\n",
        "optimized_df = optimize_partitioning(orders_df, target_partition_size_mb=128)\n",
        "\n",
        "# Test 3: Compare operations\n",
        "print(\"\\n3. Comparing narrow vs wide transformations:\")\n",
        "\n",
        "filter_df, filter_metrics = measure_operation(\n",
        "    orders_df,\n",
        "    lambda df: df.filter(F.col(\"price\") > 100),\n",
        "    \"Filter (Narrow)\"\n",
        ")\n",
        "print(filter_metrics)\n",
        "\n",
        "print()\n",
        "\n",
        "sort_df, sort_metrics = measure_operation(\n",
        "    orders_df,\n",
        "    lambda df: df.orderBy(\"price\"),\n",
        "    \"OrderBy (Wide)\"\n",
        ")\n",
        "print(sort_metrics)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Best Practices and Anti-Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"BEST PRACTICES FOR SHUFFLE OPTIMIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "✅ BEST PRACTICES:\n",
        "\n",
        "1. Minimize Wide Transformations\n",
        "   • Filter data early to reduce shuffle volume\n",
        "   • Use broadcast joins for small tables (<10MB)\n",
        "   • Combine multiple aggregations in single groupBy\n",
        "\n",
        "2. Optimize Partitioning\n",
        "   • Target 128-200MB per partition\n",
        "   • Use coalesce() when reducing partitions\n",
        "   • Use repartition() for even distribution or increasing partitions\n",
        "   • Repartition by join key before multiple operations\n",
        "\n",
        "3. Handle Data Skew\n",
        "   • Enable AQE (default in Spark 3.0+)\n",
        "   • Monitor Spark UI for long-running tasks\n",
        "   • Filter skewed values and process separately\n",
        "   • Use salting as last resort for severe skew\n",
        "\n",
        "4. Join Strategy Selection\n",
        "   • Broadcast join: One table <10MB\n",
        "   • Bucketed join: Repeated joins on same key\n",
        "   • Sort-merge join: Both tables large\n",
        "   • Let AQE optimize automatically when possible\n",
        "\n",
        "5. Configuration Tuning\n",
        "   • spark.sql.shuffle.partitions: Adjust based on data size\n",
        "   • spark.sql.autoBroadcastJoinThreshold: Increase if safe\n",
        "   • spark.sql.adaptive.enabled: Always enable (Spark 3.0+)\n",
        "   • Let AQE auto-tune when possible\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ANTI-PATTERNS TO AVOID\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "❌ ANTI-PATTERN 1: Unnecessary Shuffles\n",
        "\n",
        "Bad:\n",
        "df.repartition(100).filter(condition)  # Shuffle before filter\n",
        "\n",
        "Good:\n",
        "df.filter(condition).repartition(100)  # Filter first, reduce data\n",
        "\n",
        "---\n",
        "\n",
        "❌ ANTI-PATTERN 2: Using repartition() When coalesce() Suffices\n",
        "\n",
        "Bad:\n",
        "df.repartition(5)  # Full shuffle to reduce partitions\n",
        "\n",
        "Good:\n",
        "df.coalesce(5)  # Efficient partition combining\n",
        "\n",
        "---\n",
        "\n",
        "❌ ANTI-PATTERN 3: Multiple Shuffles on Same Key\n",
        "\n",
        "Bad:\n",
        "df.groupBy(\"key\").agg(...)  # Shuffle 1\n",
        "result.join(other_df, \"key\")  # Shuffle 2\n",
        "\n",
        "Good:\n",
        "df_partitioned = df.repartition(\"key\")  # Shuffle once\n",
        "agg_result = df_partitioned.groupBy(\"key\").agg(...)  # No shuffle\n",
        "joined = agg_result.join(other_df.repartition(\"key\"), \"key\")  # No shuffle\n",
        "\n",
        "---\n",
        "\n",
        "❌ ANTI-PATTERN 4: Ignoring Data Skew\n",
        "\n",
        "Bad:\n",
        "# Let one task process 90% of data\n",
        "skewed_df.groupBy(\"skewed_key\").agg(...)\n",
        "\n",
        "Good:\n",
        "# Enable AQE or filter skewed values\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "# or\n",
        "normal_data = df.filter(~is_skewed)\n",
        "skewed_data = df.filter(is_skewed)\n",
        "# Process separately\n",
        "\n",
        "---\n",
        "\n",
        "❌ ANTI-PATTERN 5: collect() on Large Datasets\n",
        "\n",
        "Bad:\n",
        "data = df.collect()  # Shuffle all data to driver - OOM risk!\n",
        "for row in data:\n",
        "    process(row)\n",
        "\n",
        "Good:\n",
        "df.foreach(lambda row: process(row))  # Process on executors\n",
        "# or\n",
        "df.write.format(...).save(...)  # Write directly\n",
        "\n",
        "---\n",
        "\n",
        "❌ ANTI-PATTERN 6: Too Many Small Partitions\n",
        "\n",
        "Bad:\n",
        "df.repartition(10000)  # 10MB per partition - too many tasks!\n",
        "\n",
        "Good:\n",
        "# Target 128-200MB per partition\n",
        "optimal_partitions = data_size_mb / 128\n",
        "df.repartition(optimal_partitions)\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we explored strategies for minimizing data shuffling and handling data skew in PySpark:\n",
        "\n",
        "### Key Concepts Covered\n",
        "\n",
        "1. **Understanding Shuffles**\n",
        "   - Wide vs narrow transformations\n",
        "   - Performance impact of network and disk I/O\n",
        "   - Identifying shuffle operations in code\n",
        "\n",
        "2. **Partitioning Strategies**\n",
        "   - `repartition()` for even distribution and increasing partitions\n",
        "   - `coalesce()` for efficient partition reduction\n",
        "   - Repartitioning by column for co-location\n",
        "   - Optimal partition sizing (128-200MB)\n",
        "\n",
        "3. **Data Skew Detection**\n",
        "   - Analyzing key distribution with groupBy\n",
        "   - Monitoring Spark UI for long-running tasks\n",
        "   - Partition size distribution analysis\n",
        "   - Skew ratio calculation\n",
        "\n",
        "4. **Skew Remediation**\n",
        "   - Adaptive Query Execution (AQE) automatic handling\n",
        "   - Filtering skewed values for separate processing\n",
        "   - Increasing parallelism with more partitions\n",
        "   - Salting technique for severe skew\n",
        "\n",
        "5. **Join Optimization**\n",
        "   - Broadcast joins for small tables\n",
        "   - Bucketed joins for pre-partitioned data\n",
        "   - Partitioned joins for multiple operations\n",
        "   - Sort-merge joins with AQE optimization\n",
        "\n",
        "### Functional Programming Integration\n",
        "\n",
        "- Pure functions for metrics collection and analysis\n",
        "- Immutable data structures for shuffle metrics\n",
        "- Composable optimization utilities\n",
        "- Declarative partition sizing functions\n",
        "\n",
        "### Performance Principles\n",
        "\n",
        "- **Minimize Shuffles**: Filter early, broadcast small tables, combine operations\n",
        "- **Optimize Partitions**: Target 128-200MB per partition\n",
        "- **Handle Skew**: Enable AQE, filter outliers, use salting as last resort\n",
        "- **Monitor Performance**: Use Spark UI and custom metrics\n",
        "- **Let AQE Help**: Leverage automatic optimizations in Spark 3.0+\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Monitor your Spark applications in Spark UI\n",
        "- Profile partition distributions in production workloads\n",
        "- Experiment with different join strategies\n",
        "- Enable and tune AQE for automatic optimization\n",
        "- Build custom monitoring for skew detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "Practice implementing shuffle optimization techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EXERCISES: Practice Shuffle Optimization\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "Exercise 1: Identify Shuffle Operations\n",
        "----------------------------------------\n",
        "Analyze the following pipeline and identify all shuffle operations:\n",
        "\n",
        "result = (df\n",
        "    .filter(F.col(\"status\") == \"active\")\n",
        "    .select(\"user_id\", \"amount\", \"date\")\n",
        "    .groupBy(\"user_id\")\n",
        "    .agg(F.sum(\"amount\").alias(\"total\"))\n",
        "    .join(users_df, \"user_id\")\n",
        "    .orderBy(F.desc(\"total\"))\n",
        ")\n",
        "\n",
        "List the shuffle operations and suggest optimizations.\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 2: Optimize Partition Count\n",
        "-------------------------------------\n",
        "You have a 50GB dataset with 1000 partitions.\n",
        "\n",
        "Questions:\n",
        "1. What is the average partition size?\n",
        "2. Is this optimal? (Target: 128-200MB per partition)\n",
        "3. What operation would you use to optimize?\n",
        "4. How many partitions should you target?\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 3: Detect and Handle Skew\n",
        "-----------------------------------\n",
        "Create a function that:\n",
        "1. Detects skew in a DataFrame by a given key\n",
        "2. Returns separate DataFrames for skewed and normal data\n",
        "3. Recommends a remediation strategy\n",
        "\n",
        "def detect_and_split_skew(\n",
        "    df: DataFrame,\n",
        "    key_col: str,\n",
        "    skew_threshold: float = 3.0\n",
        ") -> Tuple[DataFrame, DataFrame, str]:\n",
        "    # Your implementation\n",
        "    pass\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 4: Optimize Join Strategy\n",
        "-----------------------------------\n",
        "Given:\n",
        "- orders_df: 100GB, 1M unique customer_ids\n",
        "- customers_df: 5MB, 100K customers\n",
        "- products_df: 50MB, 10K products\n",
        "\n",
        "Task: Optimize this pipeline:\n",
        "\n",
        "result = (orders_df\n",
        "    .join(customers_df, \"customer_id\")\n",
        "    .join(products_df, \"product_id\")\n",
        "    .groupBy(\"customer_tier\", \"product_category\")\n",
        "    .agg(F.sum(\"amount\"))\n",
        ")\n",
        "\n",
        "Questions:\n",
        "1. Which joins should be broadcast?\n",
        "2. Should you repartition? On which column?\n",
        "3. Rewrite the optimized version\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 5: Implement Salting\n",
        "------------------------------\n",
        "Implement a complete salted join for skewed data:\n",
        "\n",
        "def salted_join(\n",
        "    large_df: DataFrame,\n",
        "    small_df: DataFrame,\n",
        "    join_key: str,\n",
        "    salt_range: int = 10\n",
        ") -> DataFrame:\n",
        "    \"\"\"\n",
        "    Perform a salted join to handle skew.\n",
        "    \n",
        "    Steps:\n",
        "    1. Add salt to large_df's join key\n",
        "    2. Explode small_df with all salt values\n",
        "    3. Join on salted key\n",
        "    4. Clean up salt columns\n",
        "    \"\"\"\n",
        "    # Your implementation\n",
        "    pass\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 6: Performance Monitoring\n",
        "-----------------------------------\n",
        "Create a decorator that measures and reports shuffle metrics:\n",
        "\n",
        "def monitor_shuffles(func):\n",
        "    \"\"\"\n",
        "    Decorator to monitor shuffle behavior of DataFrame operations.\n",
        "    Should report:\n",
        "    - Execution time\n",
        "    - Input/output partition counts\n",
        "    - Estimated shuffle occurred\n",
        "    \"\"\"\n",
        "    # Your implementation\n",
        "    pass\n",
        "\n",
        "@monitor_shuffles\n",
        "def my_transformation(df: DataFrame) -> DataFrame:\n",
        "    return df.groupBy(\"key\").agg(F.sum(\"value\"))\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n📝 Complete these exercises to master shuffle optimization!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
