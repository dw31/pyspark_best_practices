{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5.4 Advanced Cluster Configuration and Tuning\n",
        "\n",
        "This notebook explores platform-level optimizations for PySpark workloads, focusing on cluster configuration, Spark tuning parameters, and Databricks-specific features that enhance functional programming performance.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will understand how to:\n",
        "- Optimize Spark configuration parameters for functional workloads\n",
        "- Leverage Adaptive Query Execution (AQE) for automatic optimization\n",
        "- Enable and utilize Photon acceleration\n",
        "- Size clusters appropriately for workload characteristics\n",
        "- Configure memory and resource allocation\n",
        "- Monitor and tune performance with Spark UI\n",
        "- Apply autoscaling strategies for cost optimization\n",
        "- Clean up legacy Spark configurations\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Understanding of PySpark fundamentals\n",
        "- Familiarity with Spark execution model\n",
        "- Knowledge of functional programming patterns\n",
        "- Basic understanding of distributed computing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/10/27 22:32:13 WARN Utils: Your hostname, Tarski.local, resolves to a loopback address: 127.0.0.1; using 10.0.0.28 instead (on interface en0)\n",
            "25/10/27 22:32:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/10/27 22:32:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created new Spark session\n",
            "\n",
            "Spark Version: 4.0.0\n",
            "Spark Deployment Mode: local[*]\n",
            "Application Name: ClusterConfiguration\n"
          ]
        }
      ],
      "source": [
        "# Essential imports\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "import time\n",
        "\n",
        "# Initialize Spark session\n",
        "try:\n",
        "    spark\n",
        "    print(\"✅ Using existing Spark session\")\n",
        "except NameError:\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"ClusterConfiguration\") \\\n",
        "        .getOrCreate()\n",
        "    print(\"✅ Created new Spark session\")\n",
        "\n",
        "print(f\"\\nSpark Version: {spark.version}\")\n",
        "print(f\"Spark Deployment Mode: {spark.sparkContext.master}\")\n",
        "print(f\"Application Name: {spark.sparkContext.appName}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding Spark Configuration Hierarchy\n",
        "\n",
        "Spark configurations can be set at multiple levels with different precedence:\n",
        "\n",
        "### Configuration Precedence (Highest to Lowest)\n",
        "\n",
        "1. **Runtime Configuration** (via `spark.conf.set()`)\n",
        "   - Set programmatically in notebooks or applications\n",
        "   - Overrides all other settings\n",
        "   - Can be changed during session (for some configs)\n",
        "\n",
        "2. **Cluster Configuration** (Databricks cluster UI)\n",
        "   - Set when creating/configuring cluster\n",
        "   - Applies to all jobs on that cluster\n",
        "   - Persists across restarts\n",
        "\n",
        "3. **Application Configuration** (via `spark-submit` or SparkSession builder)\n",
        "   - Set when creating SparkSession\n",
        "   - Job-specific settings\n",
        "\n",
        "4. **Spark Defaults** (spark-defaults.conf)\n",
        "   - System-wide defaults\n",
        "   - Lowest precedence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SparkConfig:\n",
        "    \"\"\"Immutable Spark configuration snapshot\"\"\"\n",
        "    name: str\n",
        "    value: str\n",
        "    category: str\n",
        "    is_default: bool\n",
        "    description: str = \"\"\n",
        "\n",
        "def get_current_configurations(category_filter: Optional[str] = None) -> List[SparkConfig]:\n",
        "    \"\"\"\n",
        "    Pure function to retrieve current Spark configurations.\n",
        "    Returns immutable list of configuration objects.\n",
        "    \"\"\"\n",
        "    all_configs = spark.sparkContext.getConf().getAll()\n",
        "    \n",
        "    configs = []\n",
        "    for key, value in all_configs:\n",
        "        # Categorize configuration\n",
        "        if \"memory\" in key.lower():\n",
        "            category = \"Memory\"\n",
        "        elif \"shuffle\" in key.lower():\n",
        "            category = \"Shuffle\"\n",
        "        elif \"adaptive\" in key.lower() or \"aqe\" in key.lower():\n",
        "            category = \"AQE\"\n",
        "        elif \"executor\" in key.lower():\n",
        "            category = \"Executor\"\n",
        "        elif \"driver\" in key.lower():\n",
        "            category = \"Driver\"\n",
        "        elif \"sql\" in key.lower():\n",
        "            category = \"SQL\"\n",
        "        else:\n",
        "            category = \"Other\"\n",
        "        \n",
        "        if category_filter is None or category == category_filter:\n",
        "            configs.append(SparkConfig(\n",
        "                name=key,\n",
        "                value=value,\n",
        "                category=category,\n",
        "                is_default=False  # Would need to compare with defaults\n",
        "            ))\n",
        "    \n",
        "    return sorted(configs, key=lambda c: (c.category, c.name))\n",
        "\n",
        "# Display current configurations by category\n",
        "print(\"=\"*80)\n",
        "print(\"CURRENT SPARK CONFIGURATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for category in [\"Memory\", \"Executor\", \"AQE\", \"Shuffle\"]:\n",
        "    configs = get_current_configurations(category)\n",
        "    if configs:\n",
        "        print(f\"\\n{category} Configurations ({len(configs)}):\")\n",
        "        for config in configs[:5]:  # Show first 5\n",
        "            print(f\"  {config.name}: {config.value}\")\n",
        "        if len(configs) > 5:\n",
        "            print(f\"  ... and {len(configs) - 5} more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Critical Spark Configuration Parameters\n",
        "\n",
        "Let's explore the most important Spark configurations for functional PySpark workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CRITICAL SPARK CONFIGURATION PARAMETERS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "AQE Configuration\n",
            "================================================================================\n",
            "\n",
            "✅ spark.sql.adaptive.enabled\n",
            "   Current: true\n",
            "   Recommended: true\n",
            "   Purpose: Enable Adaptive Query Execution for automatic optimization\n",
            "   Impact: Automatic partition tuning, join strategy optimization, skew handling\n",
            "\n",
            "\n",
            "✅ spark.sql.adaptive.coalescePartitions.enabled\n",
            "   Current: true\n",
            "   Recommended: true\n",
            "   Purpose: Automatically reduce partition count after shuffle\n",
            "   Impact: Prevents too many small tasks, improves performance\n",
            "\n",
            "\n",
            "✅ spark.sql.adaptive.skewJoin.enabled\n",
            "   Current: true\n",
            "   Recommended: true\n",
            "   Purpose: Automatically handle skewed joins\n",
            "   Impact: Splits skewed partitions, balances workload\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Shuffle Configuration\n",
            "================================================================================\n",
            "\n",
            "⚠️ spark.sql.shuffle.partitions\n",
            "   Current: 200\n",
            "   Recommended: 200-400\n",
            "   Purpose: Number of partitions for shuffle operations\n",
            "   Impact: Balance between parallelism and overhead (AQE auto-tunes this)\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Join Configuration\n",
            "================================================================================\n",
            "\n",
            "⚠️ spark.sql.autoBroadcastJoinThreshold\n",
            "   Current: 10485760b\n",
            "   Recommended: 10485760 (10MB)\n",
            "   Purpose: Maximum table size for broadcast joins\n",
            "   Impact: Larger = more broadcast joins (faster), but OOM risk if too large\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Python Configuration\n",
            "================================================================================\n",
            "\n",
            "⚠️ spark.sql.execution.arrow.pyspark.enabled\n",
            "   Current: false\n",
            "   Recommended: true\n",
            "   Purpose: Enable Apache Arrow for Pandas UDFs and conversions\n",
            "   Impact: 10-100x faster Pandas UDF execution and toPandas() operations\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Memory Configuration\n",
            "================================================================================\n",
            "\n",
            "⚠️ spark.executor.memory\n",
            "   Current: Not Set\n",
            "   Recommended: Based on workload\n",
            "   Purpose: Memory allocated per executor\n",
            "   Impact: Reduce spills, but too much wastes resources\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Executor Configuration\n",
            "================================================================================\n",
            "\n",
            "⚠️ spark.executor.cores\n",
            "   Current: Not Set\n",
            "   Recommended: 4-8\n",
            "   Purpose: CPU cores per executor\n",
            "   Impact: Balance parallelism with resource sharing\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Configuration Health: 3/8 optimal\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CRITICAL SPARK CONFIGURATION PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "@dataclass\n",
        "class ConfigRecommendation:\n",
        "    \"\"\"Configuration parameter with recommendations\"\"\"\n",
        "    parameter: str\n",
        "    current_value: str\n",
        "    recommended_value: str\n",
        "    purpose: str\n",
        "    impact: str\n",
        "    category: str\n",
        "    \n",
        "    def is_optimal(self) -> bool:\n",
        "        return self.current_value == self.recommended_value\n",
        "    \n",
        "    def __str__(self) -> str:\n",
        "        status = \"✅\" if self.is_optimal() else \"⚠️\"\n",
        "        return (\n",
        "            f\"{status} {self.parameter}\\n\"\n",
        "            f\"   Current: {self.current_value}\\n\"\n",
        "            f\"   Recommended: {self.recommended_value}\\n\"\n",
        "            f\"   Purpose: {self.purpose}\\n\"\n",
        "            f\"   Impact: {self.impact}\"\n",
        "        )\n",
        "\n",
        "# Define critical configurations with recommendations\n",
        "critical_configs = [\n",
        "    {\n",
        "        \"parameter\": \"spark.sql.adaptive.enabled\",\n",
        "        \"recommended\": \"true\",\n",
        "        \"purpose\": \"Enable Adaptive Query Execution for automatic optimization\",\n",
        "        \"impact\": \"Automatic partition tuning, join strategy optimization, skew handling\",\n",
        "        \"category\": \"AQE\"\n",
        "    },\n",
        "    {\n",
        "        \"parameter\": \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
        "        \"recommended\": \"true\",\n",
        "        \"purpose\": \"Automatically reduce partition count after shuffle\",\n",
        "        \"impact\": \"Prevents too many small tasks, improves performance\",\n",
        "        \"category\": \"AQE\"\n",
        "    },\n",
        "    {\n",
        "        \"parameter\": \"spark.sql.adaptive.skewJoin.enabled\",\n",
        "        \"recommended\": \"true\",\n",
        "        \"purpose\": \"Automatically handle skewed joins\",\n",
        "        \"impact\": \"Splits skewed partitions, balances workload\",\n",
        "        \"category\": \"AQE\"\n",
        "    },\n",
        "    {\n",
        "        \"parameter\": \"spark.sql.shuffle.partitions\",\n",
        "        \"recommended\": \"200-400\",\n",
        "        \"purpose\": \"Number of partitions for shuffle operations\",\n",
        "        \"impact\": \"Balance between parallelism and overhead (AQE auto-tunes this)\",\n",
        "        \"category\": \"Shuffle\"\n",
        "    },\n",
        "    {\n",
        "        \"parameter\": \"spark.sql.autoBroadcastJoinThreshold\",\n",
        "        \"recommended\": \"10485760 (10MB)\",\n",
        "        \"purpose\": \"Maximum table size for broadcast joins\",\n",
        "        \"impact\": \"Larger = more broadcast joins (faster), but OOM risk if too large\",\n",
        "        \"category\": \"Join\"\n",
        "    },\n",
        "    {\n",
        "        \"parameter\": \"spark.sql.execution.arrow.pyspark.enabled\",\n",
        "        \"recommended\": \"true\",\n",
        "        \"purpose\": \"Enable Apache Arrow for Pandas UDFs and conversions\",\n",
        "        \"impact\": \"10-100x faster Pandas UDF execution and toPandas() operations\",\n",
        "        \"category\": \"Python\"\n",
        "    },\n",
        "    {\n",
        "        \"parameter\": \"spark.executor.memory\",\n",
        "        \"recommended\": \"Based on workload\",\n",
        "        \"purpose\": \"Memory allocated per executor\",\n",
        "        \"impact\": \"Reduce spills, but too much wastes resources\",\n",
        "        \"category\": \"Memory\"\n",
        "    },\n",
        "    {\n",
        "        \"parameter\": \"spark.executor.cores\",\n",
        "        \"recommended\": \"4-8\",\n",
        "        \"purpose\": \"CPU cores per executor\",\n",
        "        \"impact\": \"Balance parallelism with resource sharing\",\n",
        "        \"category\": \"Executor\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Check current values and create recommendations\n",
        "recommendations = []\n",
        "for config_def in critical_configs:\n",
        "    try:\n",
        "        current = spark.conf.get(config_def[\"parameter\"])\n",
        "    except:\n",
        "        current = \"Not Set\"\n",
        "    \n",
        "    recommendations.append(ConfigRecommendation(\n",
        "        parameter=config_def[\"parameter\"],\n",
        "        current_value=current,\n",
        "        recommended_value=config_def[\"recommended\"],\n",
        "        purpose=config_def[\"purpose\"],\n",
        "        impact=config_def[\"impact\"],\n",
        "        category=config_def[\"category\"]\n",
        "    ))\n",
        "\n",
        "# Display recommendations by category\n",
        "for category in [\"AQE\", \"Shuffle\", \"Join\", \"Python\", \"Memory\", \"Executor\"]:\n",
        "    category_recs = [r for r in recommendations if r.category == category]\n",
        "    if category_recs:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"{category} Configuration\")\n",
        "        print(f\"{'='*80}\")\n",
        "        for rec in category_recs:\n",
        "            print(f\"\\n{rec}\")\n",
        "            print()\n",
        "\n",
        "# Summary\n",
        "optimal_count = sum(1 for r in recommendations if r.is_optimal())\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Configuration Health: {optimal_count}/{len(recommendations)} optimal\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Adaptive Query Execution (AQE) Deep Dive\n",
        "\n",
        "AQE is a runtime optimization framework that dynamically adjusts query plans based on actual runtime statistics.\n",
        "\n",
        "AQE provides three major optimizations:\n",
        "\n",
        "### 1. 🔧 DYNAMIC PARTITION COALESCING\n",
        "\n",
        "- Automatically reduces number of partitions after shuffle\n",
        "- Prevents too many small tasks\n",
        "- Adjusts based on actual data size\n",
        "   \n",
        "- Configuration:\n",
        "    ```\n",
        "    spark.sql.adaptive.coalescePartitions.enabled = true\n",
        "    spark.sql.adaptive.advisoryPartitionSizeInBytes = 64MB (target size)\n",
        "    ```\n",
        "\n",
        "### 2. 🔀 DYNAMIC JOIN STRATEGY SWITCHING\n",
        "\n",
        "- Converts sort-merge joins to broadcast joins at runtime\n",
        "- Based on actual table sizes (not estimates)\n",
        "- Avoids expensive shuffles when possible\n",
        "   \n",
        "- Configuration:\n",
        "    ```\n",
        "    spark.sql.adaptive.autoBroadcastJoinThreshold = 10MB\n",
        "    ```\n",
        "\n",
        "### 3. ⚖️ SKEWED JOIN OPTIMIZATION\n",
        "\n",
        "- Detects skewed partitions during shuffle\n",
        "- Splits large partitions into smaller chunks\n",
        "- Duplicates smaller table data for split partitions\n",
        "   \n",
        "- Configuration:\n",
        "    ```\n",
        "    spark.sql.adaptive.skewJoin.enabled = true\n",
        "    spark.sql.adaptive.skewJoin.skewedPartitionFactor = 5\n",
        "    spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes = 256MB\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Current AQE Configuration:\n",
            "================================================================================\n",
            "⚠️ enabled: false\n",
            "⚠️ coalesce_partitions: false\n",
            "⚠️ skew_join: false\n",
            "✅ advisory_partition_size: 64MB\n",
            "✅ broadcast_threshold: 10MB\n",
            "\n",
            "================================================================================\n",
            "💡 Recommendation: Always enable AQE for functional PySpark workloads\n",
            "   It provides automatic optimization without code changes!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Check AQE status\n",
        "def check_aqe_status() -> Dict[str, Any]:\n",
        "    \"\"\"Pure function to check AQE configuration status\"\"\"\n",
        "    aqe_configs = {\n",
        "        \"enabled\": spark.conf.get(\"spark.sql.adaptive.enabled\", \"false\"),\n",
        "        \"coalesce_partitions\": spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\"),\n",
        "        \"skew_join\": spark.conf.get(\"spark.sql.adaptive.skewJoin.enabled\", \"false\"),\n",
        "        \"advisory_partition_size\": spark.conf.get(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\"),\n",
        "        \"broadcast_threshold\": spark.conf.get(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"10MB\"),\n",
        "    }\n",
        "    \n",
        "    return aqe_configs\n",
        "\n",
        "print(\"\\nCurrent AQE Configuration:\")\n",
        "print(\"=\"*80)\n",
        "aqe_status = check_aqe_status()\n",
        "for key, value in aqe_status.items():\n",
        "    status = \"✅\" if value in [\"true\", \"True\"] or \"MB\" in str(value) else \"⚠️\"\n",
        "    print(f\"{status} {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"💡 Recommendation: Always enable AQE for functional PySpark workloads\")\n",
        "print(\"   It provides automatic optimization without code changes!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Photon Acceleration\n",
        "\n",
        "Photon is Databricks' vectorized query engine written in C++, providing significant performance improvements for SQL operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📊 PERFORMANCE BENEFITS:\n",
        "- 2-10x faster for SQL aggregations and joins\n",
        "- 3-5x faster for Delta Lake operations\n",
        "- Improved resource utilization\n",
        "- Better performance on built-in functions\n",
        "\n",
        "✅ WHAT PHOTON ACCELERATES:\n",
        "- Built-in SQL functions (F.col, F.sum, F.avg, etc.)\n",
        "- Aggregations (groupBy, agg)\n",
        "- Joins (all types)\n",
        "- Window functions\n",
        "- Delta Lake reads and writes\n",
        "- Parquet operations\n",
        "- Filters and projections\n",
        "\n",
        "❌ WHAT PHOTON DOES NOT ACCELERATE:\n",
        "- Python UDFs (use Pandas UDFs with Arrow instead)\n",
        "- RDD operations\n",
        "- Non-SQL operations\n",
        "- Third-party formats (without Delta/Parquet)\n",
        "\n",
        "🎯 BEST PRACTICES:\n",
        "1. Enable Photon for all production workloads\n",
        "2. Use built-in functions instead of UDFs\n",
        "3. Write to Delta Lake format for best performance\n",
        "4. Monitor Photon usage in Spark UI\n",
        "\n",
        "🔧 ENABLING PHOTON:\n",
        "- Databricks Runtime 9.1 LTS or higher\n",
        "- Select \"Photon Acceleration\" when creating cluster\n",
        "- Available on AWS, Azure, and GCP\n",
        "- No code changes required\n",
        "\n",
        "💰 COST CONSIDERATIONS:\n",
        "- ~20% higher DBU cost\n",
        "- But 2-10x faster execution\n",
        "- Net result: Lower total cost for most workloads\n",
        "- Faster results = better productivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Photon Status:\n",
            "================================================================================\n",
            "photon_enabled: unknown\n",
            "runtime_version: unknown\n",
            "status: Not Enabled\n",
            "\n",
            "================================================================================\n",
            "💡 Key Insight: Photon + Functional Programming = Maximum Performance\n",
            "   Built-in functions are dramatically faster with Photon!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Check if Photon is available/enabled\n",
        "def check_photon_status() -> Dict[str, str]:\n",
        "    \"\"\"Check Photon availability and status\"\"\"\n",
        "    try:\n",
        "        # Photon-specific configs (may not be present in all runtimes)\n",
        "        photon_enabled = spark.conf.get(\"spark.databricks.photon.enabled\", \"unknown\")\n",
        "        runtime_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\", \"unknown\")\n",
        "        \n",
        "        return {\n",
        "            \"photon_enabled\": photon_enabled,\n",
        "            \"runtime_version\": runtime_version,\n",
        "            \"status\": \"Available\" if photon_enabled == \"true\" else \"Not Enabled\"\n",
        "        }\n",
        "    except:\n",
        "        return {\n",
        "            \"photon_enabled\": \"unknown\",\n",
        "            \"runtime_version\": \"unknown\",\n",
        "            \"status\": \"Cannot determine (may not be in Databricks environment)\"\n",
        "        }\n",
        "\n",
        "print(\"\\nPhoton Status:\")\n",
        "print(\"=\"*80)\n",
        "photon_status = check_photon_status()\n",
        "for key, value in photon_status.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"💡 Key Insight: Photon + Functional Programming = Maximum Performance\")\n",
        "print(\"   Built-in functions are dramatically faster with Photon!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Cluster Sizing and Resource Allocation\n",
        "\n",
        "Proper cluster sizing is critical for both performance and cost optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CLUSTER SIZING GUIDELINES\n",
            "================================================================================\n",
            "\n",
            "Workload: Interactive Development\n",
            "  Data Size: 10GB\n",
            "  Instance Type: Standard_DS3_v2 (AWS: m5.xlarge)\n",
            "  Workers: 1-4 (autoscaling)\n",
            "  Executor Memory: 8g\n",
            "  Executor Cores: 4\n",
            "  Rationale: Small cluster for development, fast startup, autoscale for larger queries\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Workload: ETL Pipeline (Medium)\n",
            "  Data Size: 500GB\n",
            "  Instance Type: Memory Optimized: Standard_E8s_v3 (AWS: r5.2xlarge)\n",
            "  Workers: 4-16 (autoscaling)\n",
            "  Executor Memory: 16g\n",
            "  Executor Cores: 4\n",
            "  Rationale: Balance memory and compute, autoscale based on load\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Workload: ETL Pipeline (Large)\n",
            "  Data Size: 5000GB\n",
            "  Instance Type: Compute Optimized: Standard_F16s_v2 (AWS: c5.4xlarge)\n",
            "  Workers: 8-32 (autoscaling)\n",
            "  Executor Memory: 16g\n",
            "  Executor Cores: 8\n",
            "  Rationale: High parallelism for large data processing\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Workload: Streaming\n",
            "  Data Size: 100GB\n",
            "  Instance Type: Memory Optimized: Standard_E16s_v3 (AWS: r5.4xlarge)\n",
            "  Workers: 4-8 (autoscaling)\n",
            "  Executor Memory: 32g\n",
            "  Executor Cores: 4\n",
            "  Rationale: Stable worker count, memory for state management\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Workload: Machine Learning\n",
            "  Data Size: 1000GB\n",
            "  Instance Type: GPU Optimized: Standard_NC6s_v3 (AWS: p3.2xlarge)\n",
            "  Workers: 2-8 (autoscaling)\n",
            "  Executor Memory: 32g\n",
            "  Executor Cores: 6\n",
            "  Rationale: GPU acceleration for ML, memory for feature caching\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "CLUSTER SIZING PRINCIPLES\n",
            "================================================================================\n",
            "\n",
            "1. 🎯 MATCH INSTANCE TYPE TO WORKLOAD:\n",
            "   • Compute Optimized: CPU-intensive transformations, joins\n",
            "   • Memory Optimized: Large aggregations, caching, ML\n",
            "   • Storage Optimized: Heavy I/O, data loading\n",
            "   • GPU Optimized: Deep learning, image processing\n",
            "\n",
            "2. 📊 SIZE FOR DATA VOLUME:\n",
            "   • Rule of thumb: 1 core per 1-2GB of data\n",
            "   • Target: 128-200MB per partition\n",
            "   • Memory: 3-4x input data size for complex operations\n",
            "\n",
            "3. ⚡ LEVERAGE AUTOSCALING:\n",
            "   • Set min for baseline performance\n",
            "   • Set max for cost control\n",
            "   • Scale up quickly, scale down slowly\n",
            "   • Monitor utilization to adjust\n",
            "\n",
            "4. 💰 COST OPTIMIZATION:\n",
            "   • Larger clusters often cheaper per-job (faster completion)\n",
            "   • Use spot/preemptible instances for fault-tolerant workloads\n",
            "   • Terminate idle clusters automatically\n",
            "   • Use cluster pools for faster startup\n",
            "\n",
            "5. 🔧 EXECUTOR CONFIGURATION:\n",
            "   • Executor cores: 4-8 (sweet spot for most workloads)\n",
            "   • Executor memory: Leave 10% overhead for Spark\n",
            "   • Number of executors: (total_cores / executor_cores) - 1 (driver)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CLUSTER SIZING GUIDELINES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "@dataclass\n",
        "class ClusterRecommendation:\n",
        "    \"\"\"Cluster sizing recommendation\"\"\"\n",
        "    workload_type: str\n",
        "    data_size_gb: float\n",
        "    instance_type: str\n",
        "    min_workers: int\n",
        "    max_workers: int\n",
        "    executor_memory: str\n",
        "    executor_cores: int\n",
        "    rationale: str\n",
        "    \n",
        "    def __str__(self) -> str:\n",
        "        return (\n",
        "            f\"Workload: {self.workload_type}\\n\"\n",
        "            f\"  Data Size: {self.data_size_gb}GB\\n\"\n",
        "            f\"  Instance Type: {self.instance_type}\\n\"\n",
        "            f\"  Workers: {self.min_workers}-{self.max_workers} (autoscaling)\\n\"\n",
        "            f\"  Executor Memory: {self.executor_memory}\\n\"\n",
        "            f\"  Executor Cores: {self.executor_cores}\\n\"\n",
        "            f\"  Rationale: {self.rationale}\"\n",
        "        )\n",
        "\n",
        "recommendations = [\n",
        "    ClusterRecommendation(\n",
        "        workload_type=\"Interactive Development\",\n",
        "        data_size_gb=10,\n",
        "        instance_type=\"Standard_DS3_v2 (AWS: m5.xlarge)\",\n",
        "        min_workers=1,\n",
        "        max_workers=4,\n",
        "        executor_memory=\"8g\",\n",
        "        executor_cores=4,\n",
        "        rationale=\"Small cluster for development, fast startup, autoscale for larger queries\"\n",
        "    ),\n",
        "    ClusterRecommendation(\n",
        "        workload_type=\"ETL Pipeline (Medium)\",\n",
        "        data_size_gb=500,\n",
        "        instance_type=\"Memory Optimized: Standard_E8s_v3 (AWS: r5.2xlarge)\",\n",
        "        min_workers=4,\n",
        "        max_workers=16,\n",
        "        executor_memory=\"16g\",\n",
        "        executor_cores=4,\n",
        "        rationale=\"Balance memory and compute, autoscale based on load\"\n",
        "    ),\n",
        "    ClusterRecommendation(\n",
        "        workload_type=\"ETL Pipeline (Large)\",\n",
        "        data_size_gb=5000,\n",
        "        instance_type=\"Compute Optimized: Standard_F16s_v2 (AWS: c5.4xlarge)\",\n",
        "        min_workers=8,\n",
        "        max_workers=32,\n",
        "        executor_memory=\"16g\",\n",
        "        executor_cores=8,\n",
        "        rationale=\"High parallelism for large data processing\"\n",
        "    ),\n",
        "    ClusterRecommendation(\n",
        "        workload_type=\"Streaming\",\n",
        "        data_size_gb=100,\n",
        "        instance_type=\"Memory Optimized: Standard_E16s_v3 (AWS: r5.4xlarge)\",\n",
        "        min_workers=4,\n",
        "        max_workers=8,\n",
        "        executor_memory=\"32g\",\n",
        "        executor_cores=4,\n",
        "        rationale=\"Stable worker count, memory for state management\"\n",
        "    ),\n",
        "    ClusterRecommendation(\n",
        "        workload_type=\"Machine Learning\",\n",
        "        data_size_gb=1000,\n",
        "        instance_type=\"GPU Optimized: Standard_NC6s_v3 (AWS: p3.2xlarge)\",\n",
        "        min_workers=2,\n",
        "        max_workers=8,\n",
        "        executor_memory=\"32g\",\n",
        "        executor_cores=6,\n",
        "        rationale=\"GPU acceleration for ML, memory for feature caching\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "for rec in recommendations:\n",
        "    print(f\"\\n{rec}\")\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLUSTER SIZING PRINCIPLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "1. 🎯 MATCH INSTANCE TYPE TO WORKLOAD:\n",
        "   • Compute Optimized: CPU-intensive transformations, joins\n",
        "   • Memory Optimized: Large aggregations, caching, ML\n",
        "   • Storage Optimized: Heavy I/O, data loading\n",
        "   • GPU Optimized: Deep learning, image processing\n",
        "\n",
        "2. 📊 SIZE FOR DATA VOLUME:\n",
        "   • Rule of thumb: 1 core per 1-2GB of data\n",
        "   • Target: 128-200MB per partition\n",
        "   • Memory: 3-4x input data size for complex operations\n",
        "\n",
        "3. ⚡ LEVERAGE AUTOSCALING:\n",
        "   • Set min for baseline performance\n",
        "   • Set max for cost control\n",
        "   • Scale up quickly, scale down slowly\n",
        "   • Monitor utilization to adjust\n",
        "\n",
        "4. 💰 COST OPTIMIZATION:\n",
        "   • Larger clusters often cheaper per-job (faster completion)\n",
        "   • Use spot/preemptible instances for fault-tolerant workloads\n",
        "   • Terminate idle clusters automatically\n",
        "   • Use cluster pools for faster startup\n",
        "\n",
        "5. 🔧 EXECUTOR CONFIGURATION:\n",
        "   • Executor cores: 4-8 (sweet spot for most workloads)\n",
        "   • Executor memory: Leave 10% overhead for Spark\n",
        "   • Number of executors: (total_cores / executor_cores) - 1 (driver)\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Memory Configuration and Tuning\n",
        "\n",
        "Understanding Spark's memory model is crucial for avoiding OOM errors and optimizing performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SPARK MEMORY MODEL\n",
            "================================================================================\n",
            "\n",
            "Spark divides executor memory into several regions:\n",
            "\n",
            "┌─────────────────────────────────────────────────────────────┐\n",
            "│                    Executor Memory (100%)                    │\n",
            "├─────────────────────────────────────────────────────────────┤\n",
            "│  Reserved Memory (300MB)                                     │\n",
            "│  • Fixed overhead for Spark internals                        │\n",
            "├─────────────────────────────────────────────────────────────┤\n",
            "│  User Memory (40% by default)                                │\n",
            "│  • User data structures                                      │\n",
            "│  • UDFs and custom objects                                   │\n",
            "│  • RDD transformations                                       │\n",
            "├─────────────────────────────────────────────────────────────┤\n",
            "│  Unified Memory (60% by default)                             │\n",
            "│  ┌───────────────────────────────────────────────────────┐  │\n",
            "│  │ Execution Memory (Dynamic, ~50%)                      │  │\n",
            "│  │ • Shuffles, joins, sorts, aggregations                │  │\n",
            "│  │ • Spills to disk if exceeded                          │  │\n",
            "│  ├───────────────────────────────────────────────────────┤  │\n",
            "│  │ Storage Memory (Dynamic, ~50%)                        │  │\n",
            "│  │ • Cached DataFrames                                   │  │\n",
            "│  │ • Broadcast variables                                 │  │\n",
            "│  │ • Can be evicted if execution needs memory            │  │\n",
            "│  └───────────────────────────────────────────────────────┘  │\n",
            "└─────────────────────────────────────────────────────────────┘\n",
            "\n",
            "KEY CONFIGURATIONS:\n",
            "\n",
            "1. spark.executor.memory\n",
            "   • Total memory per executor\n",
            "   • Example: \"16g\" for 16GB\n",
            "\n",
            "2. spark.memory.fraction (default: 0.6)\n",
            "   • Fraction for unified memory (execution + storage)\n",
            "   • Remaining goes to user memory\n",
            "\n",
            "3. spark.memory.storageFraction (default: 0.5)\n",
            "   • Fraction of unified memory protected for storage\n",
            "   • Execution can borrow if storage not using it\n",
            "\n",
            "4. spark.executor.memoryOverhead\n",
            "   • Off-heap memory for JVM overhead\n",
            "   • Default: max(executorMemory * 0.10, 384MB)\n",
            "\n",
            "\n",
            "\n",
            "MEMORY BREAKDOWN EXAMPLES:\n",
            "================================================================================\n",
            "\n",
            "Executor Memory: 8GB\n",
            "  Reserved:  0.29GB\n",
            "  User:      3.08GB (UDFs, custom objects)\n",
            "  Execution: 2.31GB (shuffles, joins, sorts)\n",
            "  Storage:   2.31GB (cache, broadcast)\n",
            "  Overhead:  0.80GB (JVM overhead)\n",
            "\n",
            "Executor Memory: 16GB\n",
            "  Reserved:  0.29GB\n",
            "  User:      6.28GB (UDFs, custom objects)\n",
            "  Execution: 4.71GB (shuffles, joins, sorts)\n",
            "  Storage:   4.71GB (cache, broadcast)\n",
            "  Overhead:  1.60GB (JVM overhead)\n",
            "\n",
            "Executor Memory: 32GB\n",
            "  Reserved:  0.29GB\n",
            "  User:      12.68GB (UDFs, custom objects)\n",
            "  Execution: 9.51GB (shuffles, joins, sorts)\n",
            "  Storage:   9.51GB (cache, broadcast)\n",
            "  Overhead:  3.20GB (JVM overhead)\n",
            "\n",
            "Executor Memory: 64GB\n",
            "  Reserved:  0.29GB\n",
            "  User:      25.48GB (UDFs, custom objects)\n",
            "  Execution: 19.11GB (shuffles, joins, sorts)\n",
            "  Storage:   19.11GB (cache, broadcast)\n",
            "  Overhead:  6.40GB (JVM overhead)\n",
            "\n",
            "================================================================================\n",
            "💡 Memory Tuning Tips:\n",
            "   • Start with defaults, only tune if seeing spills/OOM\n",
            "   • Monitor Spark UI Storage and Executors tabs\n",
            "   • For cache-heavy workloads: increase storageFraction\n",
            "   • For shuffle-heavy workloads: defaults usually optimal\n",
            "   • Use larger executors (16-32GB) for complex operations\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SPARK MEMORY MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "Spark divides executor memory into several regions:\n",
        "\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│                    Executor Memory (100%)                    │\n",
        "├─────────────────────────────────────────────────────────────┤\n",
        "│  Reserved Memory (300MB)                                     │\n",
        "│  • Fixed overhead for Spark internals                        │\n",
        "├─────────────────────────────────────────────────────────────┤\n",
        "│  User Memory (40% by default)                                │\n",
        "│  • User data structures                                      │\n",
        "│  • UDFs and custom objects                                   │\n",
        "│  • RDD transformations                                       │\n",
        "├─────────────────────────────────────────────────────────────┤\n",
        "│  Unified Memory (60% by default)                             │\n",
        "│  ┌───────────────────────────────────────────────────────┐  │\n",
        "│  │ Execution Memory (Dynamic, ~50%)                      │  │\n",
        "│  │ • Shuffles, joins, sorts, aggregations                │  │\n",
        "│  │ • Spills to disk if exceeded                          │  │\n",
        "│  ├───────────────────────────────────────────────────────┤  │\n",
        "│  │ Storage Memory (Dynamic, ~50%)                        │  │\n",
        "│  │ • Cached DataFrames                                   │  │\n",
        "│  │ • Broadcast variables                                 │  │\n",
        "│  │ • Can be evicted if execution needs memory            │  │\n",
        "│  └───────────────────────────────────────────────────────┘  │\n",
        "└─────────────────────────────────────────────────────────────┘\n",
        "\n",
        "KEY CONFIGURATIONS:\n",
        "\n",
        "1. spark.executor.memory\n",
        "   • Total memory per executor\n",
        "   • Example: \"16g\" for 16GB\n",
        "\n",
        "2. spark.memory.fraction (default: 0.6)\n",
        "   • Fraction for unified memory (execution + storage)\n",
        "   • Remaining goes to user memory\n",
        "\n",
        "3. spark.memory.storageFraction (default: 0.5)\n",
        "   • Fraction of unified memory protected for storage\n",
        "   • Execution can borrow if storage not using it\n",
        "\n",
        "4. spark.executor.memoryOverhead\n",
        "   • Off-heap memory for JVM overhead\n",
        "   • Default: max(executorMemory * 0.10, 384MB)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "def calculate_memory_breakdown(executor_memory_gb: float) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Pure function to calculate Spark memory allocation.\n",
        "    Returns memory breakdown in GB.\n",
        "    \"\"\"\n",
        "    reserved_mb = 300\n",
        "    memory_fraction = 0.6\n",
        "    storage_fraction = 0.5\n",
        "    \n",
        "    total_mb = executor_memory_gb * 1024\n",
        "    usable_mb = total_mb - reserved_mb\n",
        "    \n",
        "    unified_mb = usable_mb * memory_fraction\n",
        "    user_mb = usable_mb * (1 - memory_fraction)\n",
        "    \n",
        "    storage_mb = unified_mb * storage_fraction\n",
        "    execution_mb = unified_mb * (1 - storage_fraction)\n",
        "    \n",
        "    overhead_mb = max(total_mb * 0.10, 384)\n",
        "    \n",
        "    return {\n",
        "        \"total_gb\": executor_memory_gb,\n",
        "        \"reserved_gb\": reserved_mb / 1024,\n",
        "        \"user_memory_gb\": user_mb / 1024,\n",
        "        \"execution_memory_gb\": execution_mb / 1024,\n",
        "        \"storage_memory_gb\": storage_mb / 1024,\n",
        "        \"overhead_gb\": overhead_mb / 1024,\n",
        "    }\n",
        "\n",
        "# Example calculations\n",
        "print(\"\\nMEMORY BREAKDOWN EXAMPLES:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for memory_size in [8, 16, 32, 64]:\n",
        "    breakdown = calculate_memory_breakdown(memory_size)\n",
        "    print(f\"\\nExecutor Memory: {memory_size}GB\")\n",
        "    print(f\"  Reserved:  {breakdown['reserved_gb']:.2f}GB\")\n",
        "    print(f\"  User:      {breakdown['user_memory_gb']:.2f}GB (UDFs, custom objects)\")\n",
        "    print(f\"  Execution: {breakdown['execution_memory_gb']:.2f}GB (shuffles, joins, sorts)\")\n",
        "    print(f\"  Storage:   {breakdown['storage_memory_gb']:.2f}GB (cache, broadcast)\")\n",
        "    print(f\"  Overhead:  {breakdown['overhead_gb']:.2f}GB (JVM overhead)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"💡 Memory Tuning Tips:\")\n",
        "print(\"   • Start with defaults, only tune if seeing spills/OOM\")\n",
        "print(\"   • Monitor Spark UI Storage and Executors tabs\")\n",
        "print(\"   • For cache-heavy workloads: increase storageFraction\")\n",
        "print(\"   • For shuffle-heavy workloads: defaults usually optimal\")\n",
        "print(\"   • Use larger executors (16-32GB) for complex operations\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Configuration Management Best Practices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CONFIGURATION MANAGEMENT BEST PRACTICES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "1. 🧹 CLEAN UP LEGACY CONFIGURATIONS\n",
        "\n",
        "Problem: Old configurations can cause performance issues\n",
        "Solution: Regularly audit and remove outdated settings\n",
        "\n",
        "# Identify potentially problematic configs\n",
        "spark.conf.getAll().forEach { case (k, v) =>\n",
        "  if (k.contains(\"deprecated\") || k.contains(\"legacy\")) {\n",
        "    println(s\"Review: $k = $v\")\n",
        "  }\n",
        "}\n",
        "\n",
        "2. 📝 DOCUMENT CONFIGURATION CHANGES\n",
        "\n",
        "Good Practice:\n",
        "• Maintain a configuration changelog\n",
        "• Document why each non-default setting was changed\n",
        "• Include performance test results\n",
        "\n",
        "Example:\n",
        "# configs/production.conf\n",
        "# 2024-01-15: Increased shuffle partitions for 10TB daily load\n",
        "# Performance improvement: 40% faster aggregations\n",
        "spark.sql.shuffle.partitions=400\n",
        "\n",
        "3. 🎯 USE ENVIRONMENT-SPECIFIC CONFIGURATIONS\n",
        "\n",
        "Development:\n",
        "• Small shuffle partitions (50-100)\n",
        "• Verbose logging\n",
        "• Smaller memory allocations\n",
        "\n",
        "Production:\n",
        "• Optimal shuffle partitions (200-400)\n",
        "• Minimal logging\n",
        "• Full resource allocation\n",
        "• AQE enabled\n",
        "\n",
        "4. 🔄 VERSION CONTROL YOUR CONFIGURATIONS\n",
        "\n",
        "# cluster-configs/\n",
        "#   dev-cluster.json\n",
        "#   staging-cluster.json\n",
        "#   prod-cluster.json\n",
        "\n",
        "Track changes with git\n",
        "Review configuration changes in PRs\n",
        "Automate deployment with Terraform/ARM templates\n",
        "\n",
        "5. ⚡ START WITH DEFAULTS, TUNE INCREMENTALLY\n",
        "\n",
        "Approach:\n",
        "1. Start with Spark/Databricks defaults\n",
        "2. Enable AQE (should be default in Spark 3.0+)\n",
        "3. Monitor performance and resource usage\n",
        "4. Tune one configuration at a time\n",
        "5. Measure impact before moving to next\n",
        "\n",
        "6. 📊 MONITOR CONFIGURATION EFFECTIVENESS\n",
        "\n",
        "Metrics to track:\n",
        "• Job duration\n",
        "• Shuffle read/write bytes\n",
        "• Spill to disk (memory/disk)\n",
        "• GC time\n",
        "• Task distribution\n",
        "\n",
        "7. 🛡️ AVOID ANTI-PATTERNS\n",
        "\n",
        "❌ Don't:\n",
        "• Copy configurations blindly from StackOverflow\n",
        "• Set every configuration \"just in case\"\n",
        "• Ignore default recommendations\n",
        "• Change multiple configs simultaneously\n",
        "• Use extremely large/small values without testing\n",
        "\n",
        "✅ Do:\n",
        "• Understand what each configuration does\n",
        "• Measure before and after changes\n",
        "• Document rationale for changes\n",
        "• Use AQE to auto-tune when possible\n",
        "• Review Databricks best practices regularly\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Utility to export current configurations\n",
        "def export_spark_configurations(filename: str = \"spark_config.json\") -> None:\n",
        "    \"\"\"\n",
        "    Export current Spark configurations to JSON file.\n",
        "    Useful for documentation and version control.\n",
        "    \"\"\"\n",
        "    configs = dict(spark.sparkContext.getConf().getAll())\n",
        "    \n",
        "    config_export = {\n",
        "        \"spark_version\": spark.version,\n",
        "        \"export_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"configurations\": configs\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nConfiguration export (first 10):\")\n",
        "    for i, (key, value) in enumerate(list(configs.items())[:10]):\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(f\"  ... and {len(configs) - 10} more\")\n",
        "    \n",
        "    return config_export\n",
        "\n",
        "# Example usage\n",
        "config_export = export_spark_configurations()\n",
        "print(f\"\\n✅ Configuration exported ({len(config_export['configurations'])} parameters)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we explored advanced cluster configuration and tuning for PySpark workloads:\n",
        "\n",
        "### Key Concepts Covered\n",
        "\n",
        "1. **Spark Configuration Hierarchy**\n",
        "   - Runtime vs cluster vs application configurations\n",
        "   - Configuration precedence and override behavior\n",
        "   - Critical parameters for functional workloads\n",
        "\n",
        "2. **Adaptive Query Execution (AQE)**\n",
        "   - Dynamic partition coalescing\n",
        "   - Runtime join strategy optimization\n",
        "   - Automatic skew handling\n",
        "   - Always-on recommendation for Spark 3.0+\n",
        "\n",
        "3. **Photon Acceleration**\n",
        "   - 2-10x performance improvements for SQL operations\n",
        "   - Optimizations for built-in functions\n",
        "   - Integration with functional programming patterns\n",
        "   - Cost-benefit analysis\n",
        "\n",
        "4. **Cluster Sizing Strategies**\n",
        "   - Instance type selection for workload characteristics\n",
        "   - Executor and core allocation\n",
        "   - Autoscaling configuration\n",
        "   - Cost optimization techniques\n",
        "\n",
        "5. **Memory Management**\n",
        "   - Spark memory model (execution, storage, user)\n",
        "   - Memory fraction configuration\n",
        "   - Avoiding OOM errors and spills\n",
        "   - Overhead calculations\n",
        "\n",
        "6. **Configuration Best Practices**\n",
        "   - Clean up legacy configurations\n",
        "   - Environment-specific settings\n",
        "   - Version control and documentation\n",
        "   - Incremental tuning approach\n",
        "\n",
        "### Platform-Level Optimizations\n",
        "\n",
        "These configurations enhance functional programming by:\n",
        "- **Reducing manual tuning**: AQE automates many optimizations\n",
        "- **Improving built-in function performance**: Photon accelerates declarative operations\n",
        "- **Enabling larger datasets**: Proper memory configuration supports complex transformations\n",
        "- **Cost efficiency**: Faster execution = lower total cost\n",
        "\n",
        "### Critical Configurations for Functional PySpark\n",
        "\n",
        "**Always Enable:**\n",
        "```python\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "```\n",
        "\n",
        "**Tune Based on Workload:**\n",
        "- `spark.sql.shuffle.partitions`: 200-400 (AQE auto-tunes)\n",
        "- `spark.executor.memory`: Based on data volume and operations\n",
        "- `spark.executor.cores`: 4-8 for most workloads\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Enable AQE on all production clusters\n",
        "- Evaluate Photon for your workloads\n",
        "- Audit and clean legacy configurations\n",
        "- Implement configuration version control\n",
        "- Monitor Spark UI for optimization opportunities\n",
        "- Establish baseline metrics before tuning\n",
        "\n",
        "### Key Takeaway\n",
        "\n",
        "Modern Spark (3.0+) with AQE and Photon provides excellent performance with minimal configuration tuning. Focus on:\n",
        "1. **Writing functional, declarative code** with built-in functions\n",
        "2. **Enabling platform optimizations** (AQE, Photon)\n",
        "3. **Proper cluster sizing** for your workload\n",
        "4. **Monitoring and incremental tuning** based on evidence\n",
        "\n",
        "The platform handles most low-level optimizations automatically, allowing you to focus on business logic and functional design patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "Practice advanced cluster configuration and tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EXERCISES: Advanced Configuration and Tuning\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "Exercise 1: Configuration Audit\n",
        "--------------------------------\n",
        "Audit your current Spark configuration:\n",
        "\n",
        "1. List all non-default configurations\n",
        "2. Identify deprecated or legacy settings\n",
        "3. Check if AQE is fully enabled\n",
        "4. Verify Arrow is enabled for Pandas UDFs\n",
        "5. Document the purpose of each custom setting\n",
        "\n",
        "def audit_spark_config() -> Dict[str, List[str]]:\n",
        "    # Your implementation\n",
        "    pass\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 2: Memory Calculation\n",
        "-------------------------------\n",
        "Given a workload that processes 500GB of data with complex aggregations:\n",
        "\n",
        "Questions:\n",
        "1. How much executor memory do you need?\n",
        "2. How many executors should you use?\n",
        "3. What should spark.executor.cores be?\n",
        "4. Calculate the memory breakdown (execution, storage, user)\n",
        "\n",
        "Assumptions:\n",
        "• Target: 128MB per partition\n",
        "• Executor cores: 4\n",
        "• Need to cache 100GB intermediate results\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 3: AQE Configuration\n",
        "------------------------------\n",
        "Design optimal AQE configuration for:\n",
        "\n",
        "Scenario A: Streaming workload with small batches\n",
        "Scenario B: Large batch ETL with multiple joins\n",
        "Scenario C: Interactive analytics with ad-hoc queries\n",
        "\n",
        "For each scenario, specify:\n",
        "• AQE enabled configurations\n",
        "• Advisory partition size\n",
        "• Broadcast threshold\n",
        "• Skew join settings\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 4: Cluster Sizing\n",
        "---------------------------\n",
        "Design a cluster for each workload:\n",
        "\n",
        "Workload 1: Daily ETL processing 2TB data\n",
        "• SLA: Complete in < 2 hours\n",
        "• Operations: Joins, aggregations, window functions\n",
        "• Budget: Optimize for cost\n",
        "\n",
        "Workload 2: Real-time streaming dashboard\n",
        "• Latency: < 1 minute end-to-end\n",
        "• Throughput: 10K events/sec\n",
        "• State: 50GB\n",
        "\n",
        "Specify:\n",
        "• Instance type\n",
        "• Number of workers (min/max)\n",
        "• Executor memory and cores\n",
        "• Key Spark configurations\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 5: Configuration Migration\n",
        "------------------------------------\n",
        "You're migrating from Spark 2.4 to Spark 3.2. Update this configuration:\n",
        "\n",
        "Old (Spark 2.4):\n",
        "spark.sql.shuffle.partitions=200\n",
        "spark.sql.autoBroadcastJoinThreshold=10485760\n",
        "# Manual partition tuning for each job\n",
        "# No skew handling\n",
        "\n",
        "Questions:\n",
        "1. What new AQE configurations should you enable?\n",
        "2. Can you remove any manual tuning?\n",
        "3. What performance improvements do you expect?\n",
        "4. What new features can optimize your code?\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 6: Performance Monitoring\n",
        "-----------------------------------\n",
        "Create a monitoring dashboard that tracks:\n",
        "\n",
        "def create_performance_metrics() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Collect key performance metrics:\n",
        "    • Job duration\n",
        "    • Shuffle read/write\n",
        "    • Spill metrics\n",
        "    • GC time\n",
        "    • Task skew\n",
        "    • AQE optimizations applied\n",
        "    \"\"\"\n",
        "    # Your implementation\n",
        "    pass\n",
        "\n",
        "---\n",
        "\n",
        "Exercise 7: Cost Optimization\n",
        "------------------------------\n",
        "Your current cluster costs $100/hour and jobs run 8 hours/day.\n",
        "\n",
        "Options:\n",
        "A) Double cluster size (+100% cost)\n",
        "B) Enable Photon (+20% cost)\n",
        "C) Optimize code with AQE (no cost increase)\n",
        "\n",
        "Expected speedups:\n",
        "A) 1.8x faster\n",
        "B) 3x faster\n",
        "C) 1.5x faster\n",
        "\n",
        "Questions:\n",
        "1. Calculate total daily cost for each option\n",
        "2. Which option provides best ROI?\n",
        "3. Can you combine options? What's the impact?\n",
        "4. Factor in developer time savings\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n📝 Complete these exercises to master cluster configuration!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spark-sdp-lakehouse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
