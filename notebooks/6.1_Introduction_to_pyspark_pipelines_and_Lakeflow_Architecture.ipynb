{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Introduction to pyspark.pipelines and Databricks Lakeflow Architecture\n",
    "\n",
    "This notebook introduces Apache Spark 4.1's declarative pipelines framework (`pyspark.pipelines`) and Databricks Lakeflow Declarative Pipelines, the evolution of Delta Live Tables (DLT). We'll explore the migration path from legacy `dlt` to the new `pyspark.pipelines` module, understanding how this aligns with functional programming principles.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- The evolution from Delta Live Tables (`dlt`) to Lakeflow Declarative Pipelines (`pyspark.pipelines`)\n",
    "- Apache Spark 4.1's open-source declarative pipelines vs Databricks Lakeflow extensions\n",
    "- Core architectural concepts: flows, tables, views, sinks\n",
    "- Why declarative pipelines align with functional programming\n",
    "- Migration strategies from legacy DLT code\n",
    "- The Lakeflow platform ecosystem\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of PySpark DataFrames and transformations\n",
    "- Familiarity with functional programming concepts (pure functions, immutability)\n",
    "- Knowledge of Delta Lake basics\n",
    "- Experience with data pipeline patterns\n",
    "\n",
    "## Important Note\n",
    "\n",
    "**Platform Requirements**: This notebook demonstrates concepts that require:\n",
    "- Apache Spark 4.1+ (for open-source `pyspark.pipelines`)\n",
    "- Databricks Runtime (for full Lakeflow features)\n",
    "- Pipeline execution mode (not standard interactive notebook mode)\n",
    "\n",
    "Code examples show both the declarative syntax and functional simulation patterns for educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Evolution: From DLT to Lakeflow Declarative Pipelines\n",
    "\n",
    "### Historical Context\n",
    "\n",
    "**Delta Live Tables (Legacy)**:\n",
    "```python\n",
    "import dlt  # Legacy module\n",
    "\n",
    "@dlt.table\n",
    "def customers():\n",
    "    return spark.read.table(\"source_customers\")\n",
    "```\n",
    "\n",
    "**Lakeflow Declarative Pipelines (Modern)**:\n",
    "```python\n",
    "from pyspark import pipelines as dp  # New module in Spark 4.1+\n",
    "\n",
    "@dp.table\n",
    "def customers():\n",
    "    return spark.read.table(\"source_customers\")\n",
    "```\n",
    "\n",
    "### Key Changes and Timeline\n",
    "\n",
    "| Milestone | Description | Significance |\n",
    "|-----------|-------------|---------------|\n",
    "| **2021** | Delta Live Tables (DLT) introduced | Proprietary Databricks framework for declarative pipelines |\n",
    "| **2024** | Apache Spark 4.1 released | Declarative pipelines (`pyspark.pipelines`) become open source |\n",
    "| **2025** | Databricks Data + AI Summit | DLT rebranded as \"Lakeflow Declarative Pipelines\" |\n",
    "| **Current** | Unified approach | Single API works across open-source Spark and Databricks |\n",
    "\n",
    "### Why the Change?\n",
    "\n",
    "1. **Open Source Alignment**: Makes declarative pipelines available to the entire Spark community\n",
    "2. **Standardization**: `pyspark.pipelines` becomes the standard, not a Databricks-specific API\n",
    "3. **Portability**: Code written with `pyspark.pipelines` runs on any Spark 4.1+ environment\n",
    "4. **Innovation**: Databricks contribution accelerates data engineering innovation ecosystem-wide\n",
    "5. **Future-Proofing**: Aligns with Apache Spark's long-term evolution\n",
    "\n",
    "### Backward Compatibility\n",
    "\n",
    "**Good News**: Existing `dlt` code continues to work!\n",
    "```python\n",
    "# Both imports work, but dp is recommended for new code\n",
    "import dlt                          # Legacy: still supported\n",
    "from pyspark import pipelines as dp  # Modern: recommended\n",
    "\n",
    "# Old decorator names still work\n",
    "@dlt.table              # ✓ Supported (legacy)\n",
    "@dp.table               # ✓ Recommended (modern)\n",
    "```\n",
    "\n",
    "**Migration Recommendation**: \n",
    "- Existing pipelines: No immediate action required\n",
    "- New development: Use `from pyspark import pipelines as dp`\n",
    "- Gradual migration: Update imports as you enhance pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Open Source vs Databricks Extensions\n",
    "\n",
    "Understanding what's in open-source Spark vs Databricks-specific extensions:\n",
    "\n",
    "### Open Source Apache Spark 4.1 (`pyspark.pipelines`)\n",
    "\n",
    "**Core Capabilities**:\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "# ✓ Available in open-source Spark 4.1+\n",
    "@dp.table                    # Define materialized tables\n",
    "@dp.streaming_table          # Define streaming tables\n",
    "@dp.materialized_view        # Define materialized views\n",
    "@dp.temporary_view           # Define temporary views\n",
    "\n",
    "# Quality expectations\n",
    "@dp.expect()                 # Monitor quality (WARN)\n",
    "@dp.expect_or_drop()         # Filter violations (DROP)\n",
    "@dp.expect_or_fail()         # Stop on violations (FAIL)\n",
    "```\n",
    "\n",
    "**What You Get**:\n",
    "- Declarative table definitions\n",
    "- Automatic dependency resolution\n",
    "- Built-in data quality expectations\n",
    "- Streaming and batch processing\n",
    "- Optimized execution planning\n",
    "\n",
    "**Runs On**:\n",
    "- Any Apache Spark 4.1+ cluster\n",
    "- Local development environments\n",
    "- Kubernetes-based Spark\n",
    "- Cloud Spark services (EMR, HDInsight, Dataproc)\n",
    "- Databricks (extended features)\n",
    "\n",
    "### Databricks Lakeflow Extensions\n",
    "\n",
    "**Additional Features** (Databricks-specific):\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "# ⚡ Databricks-specific enhancements\n",
    "dp.create_auto_cdc_flow()              # Automatic CDC pattern\n",
    "dp.create_auto_cdc_from_snapshot_flow() # CDC from snapshots\n",
    "dp.create_sink()                        # Advanced sink configuration\n",
    "dp.append_flow()                        # Incremental append patterns\n",
    "\n",
    "# Unity Catalog integration\n",
    "# Photon acceleration\n",
    "# Serverless compute\n",
    "# Advanced monitoring and lineage\n",
    "```\n",
    "\n",
    "**Databricks Platform Integration**:\n",
    "- **Unity Catalog**: Fine-grained access control, data lineage, audit logs\n",
    "- **Photon Engine**: 2-10x performance improvements for pipeline operations\n",
    "- **Serverless Compute**: Auto-scaling, instant start, pay-per-use\n",
    "- **Lakeflow UI**: Visual pipeline designer, monitoring dashboards, alerting\n",
    "- **Lakeflow Connect**: High-throughput connectors for 100+ data sources\n",
    "- **Lakeflow Jobs**: Workflow orchestration, scheduling, error handling\n",
    "\n",
    "### Code Portability Strategy\n",
    "\n",
    "**Pattern 1: Pure Open Source** (maximum portability)\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "@dp.table\n",
    "def bronze_events():\n",
    "    return spark.readStream.format(\"delta\").table(\"raw.events\")\n",
    "\n",
    "# ✓ Runs anywhere with Spark 4.1+\n",
    "```\n",
    "\n",
    "**Pattern 2: Databricks-Enhanced** (optimized for Databricks)\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "# Use Databricks-specific features when available\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"bronze.customers\",\n",
    "    target=\"silver.customers\",\n",
    "    keys=[\"customer_id\"]\n",
    ")\n",
    "\n",
    "# ⚡ Leverages Databricks optimizations\n",
    "```\n",
    "\n",
    "**Recommendation**: Use open-source APIs for core logic, Databricks extensions for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Architecture: Declarative vs Imperative Pipelines\n",
    "\n",
    "### The Paradigm Shift\n",
    "\n",
    "**Imperative Approach** (Traditional PySpark):\n",
    "```python\n",
    "# ❌ You define HOW to build the pipeline\n",
    "# Step 1: Read raw data\n",
    "raw_df = spark.read.format(\"json\").load(\"/data/raw/events/\")\n",
    "raw_df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/bronze/events\")\n",
    "\n",
    "# Step 2: Transform to silver\n",
    "bronze_df = spark.read.format(\"delta\").load(\"/data/bronze/events\")\n",
    "silver_df = bronze_df.filter(col(\"event_type\").isNotNull())\n",
    "silver_df.write.format(\"delta\").mode(\"append\").save(\"/data/silver/events\")\n",
    "\n",
    "# Step 3: Aggregate to gold\n",
    "silver_df = spark.read.format(\"delta\").load(\"/data/silver/events\")\n",
    "gold_df = silver_df.groupBy(\"date\").agg(count(\"*\").alias(\"event_count\"))\n",
    "gold_df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/gold/daily_events\")\n",
    "\n",
    "# Issues:\n",
    "# - Must manually orchestrate execution order\n",
    "# - Need custom dependency management\n",
    "# - Error handling is manual\n",
    "# - Retry logic is custom\n",
    "# - Monitoring requires separate implementation\n",
    "```\n",
    "\n",
    "**Declarative Approach** (`pyspark.pipelines`):\n",
    "```python\n",
    "# ✅ You define WHAT tables you want, Spark figures out HOW\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dp.table\n",
    "def bronze_events():\n",
    "    \"\"\"Raw events from source system\"\"\"\n",
    "    return spark.read.format(\"json\").load(\"/data/raw/events/\")\n",
    "\n",
    "@dp.table\n",
    "@dp.expect_or_drop(\"valid_event_type\", \"event_type IS NOT NULL\")\n",
    "def silver_events():\n",
    "    \"\"\"Cleaned events with quality filters\"\"\"\n",
    "    return dp.read(\"bronze_events\")\n",
    "\n",
    "@dp.table\n",
    "def gold_daily_events():\n",
    "    \"\"\"Daily event aggregations\"\"\"\n",
    "    return (\n",
    "        dp.read(\"silver_events\")\n",
    "        .groupBy(\"date\")\n",
    "        .agg(F.count(\"*\").alias(\"event_count\"))\n",
    "    )\n",
    "\n",
    "# Benefits:\n",
    "# ✓ Automatic dependency resolution (gold depends on silver depends on bronze)\n",
    "# ✓ Built-in error handling and retries\n",
    "# ✓ Automatic quality monitoring\n",
    "# ✓ Intelligent parallelization\n",
    "# ✓ Incremental processing\n",
    "```\n",
    "\n",
    "### Functional Programming Alignment\n",
    "\n",
    "Declarative pipelines naturally embody functional programming principles:\n",
    "\n",
    "1. **Pure Functions**: Pipeline definitions are pure functions\n",
    "   ```python\n",
    "   @dp.table\n",
    "   def customers():  # Pure function: input → output\n",
    "       return spark.table(\"raw.customers\")  # No side effects\n",
    "   ```\n",
    "\n",
    "2. **Immutability**: Tables are immutable, transformations create new tables\n",
    "   ```python\n",
    "   # bronze_events is immutable\n",
    "   # silver_events is a NEW table, not a mutation\n",
    "   ```\n",
    "\n",
    "3. **Composition**: Tables compose naturally through dependencies\n",
    "   ```python\n",
    "   @dp.table\n",
    "   def final_result():\n",
    "       return dp.read(\"intermediate1\").join(dp.read(\"intermediate2\"))\n",
    "   ```\n",
    "\n",
    "4. **Declarative**: Focus on WHAT, not HOW\n",
    "   - Catalyst optimizer handles execution strategy\n",
    "   - Pipeline engine handles orchestration\n",
    "   - Platform handles infrastructure\n",
    "\n",
    "5. **No Side Effects in Definitions**: Actions (write, collect) are prohibited\n",
    "   ```python\n",
    "   @dp.table\n",
    "   def valid_table():\n",
    "       return spark.table(\"source\")  # ✓ Returns DataFrame\n",
    "   \n",
    "   @dp.table\n",
    "   def invalid_table():\n",
    "       df = spark.table(\"source\")\n",
    "       df.write.save(\"somewhere\")    # ❌ Side effect forbidden!\n",
    "       return df\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Concepts: Tables, Views, and Flows\n",
    "\n",
    "### Table Types in Lakeflow\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "# 1. Materialized Table (@dp.table)\n",
    "@dp.table(comment=\"Customer master table\")\n",
    "def customers():\n",
    "    \"\"\"\n",
    "    Fully materialized Delta table.\n",
    "    - Data is physically stored\n",
    "    - Can be queried independently\n",
    "    - Batch-processed by default\n",
    "    - Best for: Reference data, slowly changing dimensions\n",
    "    \"\"\"\n",
    "    return spark.read.table(\"raw.customers\")\n",
    "\n",
    "# 2. Streaming Table (@dp.streaming_table)\n",
    "@dp.streaming_table(comment=\"Real-time events stream\")\n",
    "def events_stream():\n",
    "    \"\"\"\n",
    "    Streaming Delta table with incremental processing.\n",
    "    - Continuously processes new data\n",
    "    - Maintains checkpoints\n",
    "    - Exactly-once guarantees\n",
    "    - Best for: Real-time data, event streams, CDC\n",
    "    \"\"\"\n",
    "    return spark.readStream.format(\"delta\").table(\"raw.events\")\n",
    "\n",
    "# 3. Materialized View (@dp.materialized_view)\n",
    "@dp.materialized_view(comment=\"Customer order summary\")\n",
    "def customer_orders():\n",
    "    \"\"\"\n",
    "    Pre-computed aggregate or join.\n",
    "    - Stored like a table\n",
    "    - Refreshed on pipeline run\n",
    "    - Optimized for read performance\n",
    "    - Best for: Complex aggregations, frequently-queried joins\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read(\"customers\")\n",
    "        .join(dp.read(\"orders\"), \"customer_id\")\n",
    "        .groupBy(\"customer_id\")\n",
    "        .agg(F.sum(\"order_total\").alias(\"total_spent\"))\n",
    "    )\n",
    "\n",
    "# 4. Temporary View (@dp.temporary_view)\n",
    "@dp.temporary_view(comment=\"Filtered active customers\")\n",
    "def active_customers():\n",
    "    \"\"\"\n",
    "    Logical view, not materialized.\n",
    "    - No physical storage\n",
    "    - Computed on-demand when referenced\n",
    "    - Saves storage for intermediate transformations\n",
    "    - Best for: Intermediate filters, reusable subqueries\n",
    "    \"\"\"\n",
    "    return dp.read(\"customers\").filter(F.col(\"status\") == \"active\")\n",
    "```\n",
    "\n",
    "### Decision Matrix: Which Type to Use?\n",
    "\n",
    "| Criteria | @dp.table | @dp.streaming_table | @dp.materialized_view | @dp.temporary_view |\n",
    "|----------|-----------|---------------------|----------------------|--------------------|\n",
    "| **Storage** | Materialized | Materialized | Materialized | None |\n",
    "| **Processing** | Batch | Streaming | Batch | On-demand |\n",
    "| **Use Case** | Source/dimension tables | Real-time data | Complex aggregations | Intermediate logic |\n",
    "| **Query Performance** | Fast | Fast | Fast | Depends on complexity |\n",
    "| **Storage Cost** | Moderate | Moderate | Moderate | None |\n",
    "| **Refresh Pattern** | Full or incremental | Continuous | On pipeline run | Every query |\n",
    "| **Best For** | Static/slow-changing | Events, CDC | Pre-computed joins | Filters, subqueries |\n",
    "\n",
    "### Flows: Coordinating Multiple Tables\n",
    "\n",
    "**Concept**: A \"flow\" represents a logical grouping of tables with a specific processing pattern.\n",
    "\n",
    "```python\n",
    "# Example: Append Flow (for growing datasets)\n",
    "dp.append_flow(\n",
    "    source=dp.read(\"bronze_logs\"),\n",
    "    target=\"silver_logs\",\n",
    "    target_columns=[\"timestamp\", \"user_id\", \"action\", \"processed_at\"]\n",
    ")\n",
    "# Only new records from bronze are appended to silver\n",
    "\n",
    "# Example: CDC Flow (change data capture)\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"bronze.customer_changes\",\n",
    "    target=\"silver.customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"update_timestamp\"\n",
    ")\n",
    "# Automatically applies inserts, updates, deletes\n",
    "```\n",
    "\n",
    "### Dependency Resolution\n",
    "\n",
    "```python\n",
    "# Lakeflow automatically resolves dependencies\n",
    "@dp.table\n",
    "def a():\n",
    "    return spark.table(\"source_a\")\n",
    "\n",
    "@dp.table\n",
    "def b():\n",
    "    return dp.read(\"a\")  # Depends on table 'a'\n",
    "\n",
    "@dp.table\n",
    "def c():\n",
    "    return dp.read(\"a\").union(dp.read(\"b\"))  # Depends on both 'a' and 'b'\n",
    "\n",
    "# Execution order: a → b → c (automatically determined)\n",
    "# Parallelization: a runs first, then b, then c (DAG optimization)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Lakeflow Platform Ecosystem\n",
    "\n",
    "### Three Pillars of Lakeflow\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                     LAKEFLOW PLATFORM                            │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────┐  │\n",
    "│  │  Lakeflow        │  │  Lakeflow        │  │  Lakeflow    │  │\n",
    "│  │  Connect         │  │  Declarative     │  │  Jobs        │  │\n",
    "│  │                  │  │  Pipelines       │  │              │  │\n",
    "│  │  [Ingestion]     │→ │  [Transform]     │→ │  [Orchestrate]│  │\n",
    "│  └──────────────────┘  └──────────────────┘  └──────────────┘  │\n",
    "│                                                                  │\n",
    "│  • 100+ connectors   │  • pyspark.pipelines │  • DAG workflows │\n",
    "│  • Change tracking   │  • Data quality      │  • Scheduling    │\n",
    "│  • Schema detection  │  • Lineage tracking  │  • Monitoring    │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 1. Lakeflow Connect (Ingestion Layer)\n",
    "\n",
    "**Purpose**: High-throughput data ingestion from diverse sources\n",
    "\n",
    "**Key Features**:\n",
    "- 100+ pre-built connectors (Salesforce, SAP, Oracle, MySQL, etc.)\n",
    "- Auto-schema detection and evolution\n",
    "- Change data capture (CDC) built-in\n",
    "- Incremental loading strategies\n",
    "- Error handling and dead-letter queues\n",
    "\n",
    "**Example Integration**:\n",
    "```python\n",
    "# Lakeflow Connect ingests data to bronze layer\n",
    "# Then Lakeflow Declarative Pipelines transforms it\n",
    "\n",
    "@dp.streaming_table\n",
    "def bronze_salesforce():\n",
    "    # Data ingested by Lakeflow Connect\n",
    "    return spark.readStream.table(\"connect.salesforce_accounts\")\n",
    "```\n",
    "\n",
    "### 2. Lakeflow Declarative Pipelines (Transformation Layer)\n",
    "\n",
    "**Purpose**: Declarative data transformations with built-in quality\n",
    "\n",
    "**What We're Learning in This Section**:\n",
    "- Table definitions with `@dp.table`, `@dp.streaming_table`, etc.\n",
    "- Data quality expectations\n",
    "- Dependency management\n",
    "- Streaming and batch processing\n",
    "\n",
    "### 3. Lakeflow Jobs (Orchestration Layer)\n",
    "\n",
    "**Purpose**: Workflow orchestration and scheduling\n",
    "\n",
    "**Key Features**:\n",
    "- Pipeline scheduling (cron, event-driven)\n",
    "- Cross-pipeline dependencies\n",
    "- Retry policies and error handling\n",
    "- Monitoring and alerting\n",
    "- Resource management\n",
    "\n",
    "**Example Workflow**:\n",
    "```python\n",
    "# Lakeflow Jobs orchestrates:\n",
    "# 1. Run Lakeflow Connect ingestion → bronze tables\n",
    "# 2. Trigger Lakeflow Declarative Pipeline → silver/gold tables\n",
    "# 3. Send notifications on completion/failure\n",
    "# 4. Trigger downstream ML or BI workloads\n",
    "```\n",
    "\n",
    "### Integration Pattern\n",
    "\n",
    "```python\n",
    "# Full Lakeflow workflow example\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "# Step 1: Lakeflow Connect ingests to bronze (configured via UI)\n",
    "\n",
    "# Step 2: Lakeflow Declarative Pipeline transforms\n",
    "@dp.streaming_table\n",
    "def bronze_customers():\n",
    "    \"\"\"Ingested by Lakeflow Connect\"\"\"\n",
    "    return spark.readStream.table(\"connect.customers\")\n",
    "\n",
    "@dp.table\n",
    "@dp.expect_or_drop(\"valid_email\", \"email IS NOT NULL\")\n",
    "def silver_customers():\n",
    "    \"\"\"Cleaned and validated\"\"\"\n",
    "    return dp.read(\"bronze_customers\").filter(F.col(\"country\") == \"US\")\n",
    "\n",
    "@dp.materialized_view\n",
    "def gold_customer_metrics():\n",
    "    \"\"\"Business aggregates\"\"\"\n",
    "    return (\n",
    "        dp.read(\"silver_customers\")\n",
    "        .groupBy(\"segment\")\n",
    "        .agg(F.count(\"*\").alias(\"customer_count\"))\n",
    "    )\n",
    "\n",
    "# Step 3: Lakeflow Jobs schedules and monitors execution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Migration Guide: From dlt to pyspark.pipelines\n",
    "\n",
    "### Import Changes\n",
    "\n",
    "```python\n",
    "# OLD (Legacy DLT)\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# NEW (Lakeflow Declarative Pipelines)\n",
    "from pyspark import pipelines as dp  # Changed import\n",
    "from pyspark.sql import functions as F  # No change\n",
    "```\n",
    "\n",
    "### Decorator Migration\n",
    "\n",
    "| Legacy DLT | Modern Lakeflow | Notes |\n",
    "|------------|-----------------|-------|\n",
    "| `@dlt.table` | `@dp.table` | Direct 1:1 replacement |\n",
    "| `@dlt.view` | `@dp.materialized_view` | Name clarified |\n",
    "| `dlt.read(\"table\")` | `dp.read(\"table\")` | Function call replacement |\n",
    "| `dlt.read_stream(\"table\")` | `dp.read_stream(\"table\")` | Streaming reads |\n",
    "| `@dlt.expect()` | `@dp.expect()` | Direct replacement |\n",
    "| `@dlt.expect_or_drop()` | `@dp.expect_or_drop()` | Direct replacement |\n",
    "| `@dlt.expect_or_fail()` | `@dp.expect_or_fail()` | Direct replacement |\n",
    "\n",
    "### Complete Migration Example\n",
    "\n",
    "**Before (Legacy DLT)**:\n",
    "```python\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"customers_bronze\",\n",
    "    comment=\"Raw customer data\"\n",
    ")\n",
    "def customers_bronze():\n",
    "    return spark.read.table(\"raw.customers\")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"customers_silver\",\n",
    "    comment=\"Cleaned customer data\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_email\", \"email IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")\n",
    "def customers_silver():\n",
    "    return dlt.read(\"customers_bronze\")\n",
    "\n",
    "@dlt.view(\n",
    "    name=\"customer_summary\",\n",
    "    comment=\"Customer metrics by country\"\n",
    ")\n",
    "def customer_summary():\n",
    "    return (\n",
    "        dlt.read(\"customers_silver\")\n",
    "        .groupBy(\"country\")\n",
    "        .agg(F.count(\"*\").alias(\"customer_count\"))\n",
    "    )\n",
    "```\n",
    "\n",
    "**After (Modern Lakeflow)**:\n",
    "```python\n",
    "from pyspark import pipelines as dp  # Changed: new import\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dp.table(  # Changed: dlt → dp\n",
    "    name=\"customers_bronze\",\n",
    "    comment=\"Raw customer data\"\n",
    ")\n",
    "def customers_bronze():\n",
    "    return spark.read.table(\"raw.customers\")\n",
    "\n",
    "@dp.table(  # Changed: dlt → dp\n",
    "    name=\"customers_silver\",\n",
    "    comment=\"Cleaned customer data\"\n",
    ")\n",
    "@dp.expect_or_drop(\"valid_email\", \"email IS NOT NULL\")  # Changed: dlt → dp\n",
    "@dp.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")  # Changed: dlt → dp\n",
    "def customers_silver():\n",
    "    return dp.read(\"customers_bronze\")  # Changed: dlt → dp\n",
    "\n",
    "@dp.materialized_view(  # Changed: @dlt.view → @dp.materialized_view\n",
    "    name=\"customer_summary\",\n",
    "    comment=\"Customer metrics by country\"\n",
    ")\n",
    "def customer_summary():\n",
    "    return (\n",
    "        dp.read(\"customers_silver\")  # Changed: dlt → dp\n",
    "        .groupBy(\"country\")\n",
    "        .agg(F.count(\"*\").alias(\"customer_count\"))\n",
    "    )\n",
    "```\n",
    "\n",
    "### Migration Automation\n",
    "\n",
    "**Simple Find-and-Replace Strategy**:\n",
    "```bash\n",
    "# In your pipeline notebooks/files:\n",
    "1. Replace: import dlt → from pyspark import pipelines as dp\n",
    "2. Replace: @dlt. → @dp.\n",
    "3. Replace: dlt.read → dp.read\n",
    "4. Replace: dlt.read_stream → dp.read_stream\n",
    "5. Replace: @dlt.view → @dp.materialized_view\n",
    "```\n",
    "\n",
    "**Testing After Migration**:\n",
    "```python\n",
    "# Both versions should produce identical results\n",
    "# 1. Run legacy pipeline in test mode\n",
    "# 2. Run migrated pipeline in test mode\n",
    "# 3. Compare output tables (should be identical)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Reference: API Cheat Sheet\n",
    "\n",
    "### Essential Imports\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "```\n",
    "\n",
    "### Table Definition Decorators\n",
    "```python\n",
    "@dp.table                    # Materialized batch table\n",
    "@dp.streaming_table          # Streaming table with checkpoints\n",
    "@dp.materialized_view        # Pre-computed view (refreshed on run)\n",
    "@dp.temporary_view           # Logical view (not materialized)\n",
    "```\n",
    "\n",
    "### Reading Tables\n",
    "```python\n",
    "dp.read(\"table_name\")        # Read batch table\n",
    "dp.read_stream(\"table_name\") # Read streaming table\n",
    "```\n",
    "\n",
    "### Data Quality Expectations\n",
    "```python\n",
    "@dp.expect(\"name\", \"constraint\")           # WARN: Monitor violations\n",
    "@dp.expect_or_drop(\"name\", \"constraint\")   # DROP: Remove violating records\n",
    "@dp.expect_or_fail(\"name\", \"constraint\")   # FAIL: Stop pipeline on violation\n",
    "```\n",
    "\n",
    "### Flow Creation (Databricks-specific)\n",
    "```python\n",
    "dp.append_flow()                           # Incremental append pattern\n",
    "dp.create_auto_cdc_flow()                  # Automatic CDC\n",
    "dp.create_sink()                           # Custom sink configuration\n",
    "```\n",
    "\n",
    "### Prohibited Operations in Definitions\n",
    "```python\n",
    "# ❌ NEVER use these in @dp.table functions:\n",
    "df.collect()          # Action: triggers computation\n",
    "df.count()            # Action: triggers computation\n",
    "df.toPandas()         # Action: collects to driver\n",
    "df.write.save()       # Side effect: writes data\n",
    "df.write.saveAsTable()  # Side effect: creates table\n",
    "df.writeStream.start()  # Side effect: starts stream\n",
    "\n",
    "# ✅ Only return DataFrames:\n",
    "return df  # Correct: returns DataFrame for Lakeflow to materialize\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored the evolution from Delta Live Tables to Lakeflow Declarative Pipelines:\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Evolution Path**\n",
    "   - Legacy: `import dlt` (Databricks proprietary)\n",
    "   - Modern: `from pyspark import pipelines as dp` (Apache Spark 4.1+)\n",
    "   - Backward compatible: Both work, new code should use `dp`\n",
    "\n",
    "2. **Open Source vs Databricks**\n",
    "   - Core pipelines API: Open source in Spark 4.1+\n",
    "   - Databricks extensions: CDC flows, Unity Catalog, Photon, serverless\n",
    "   - Code portability: Core API works anywhere\n",
    "\n",
    "3. **Declarative Paradigm**\n",
    "   - Define WHAT tables you want, not HOW to build them\n",
    "   - Automatic dependency resolution\n",
    "   - Built-in error handling and retries\n",
    "   - Aligns with functional programming principles\n",
    "\n",
    "4. **Table Types**\n",
    "   - `@dp.table`: Materialized batch tables\n",
    "   - `@dp.streaming_table`: Real-time streaming tables\n",
    "   - `@dp.materialized_view`: Pre-computed aggregations\n",
    "   - `@dp.temporary_view`: Logical views (no storage)\n",
    "\n",
    "5. **Lakeflow Platform**\n",
    "   - Lakeflow Connect: Data ingestion\n",
    "   - Lakeflow Declarative Pipelines: Transformations\n",
    "   - Lakeflow Jobs: Orchestration and scheduling\n",
    "\n",
    "6. **Migration Strategy**\n",
    "   - Simple find-and-replace: `dlt` → `dp`\n",
    "   - No breaking changes: Legacy code continues to work\n",
    "   - Gradual adoption recommended\n",
    "\n",
    "### Functional Programming Benefits\n",
    "\n",
    "- **Pure Functions**: Table definitions are pure (no side effects)\n",
    "- **Immutability**: Tables are immutable, transformations create new tables\n",
    "- **Composition**: Tables compose through dependency graph\n",
    "- **Declarative**: Focus on outcome, not implementation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In upcoming notebooks, we'll dive deeper into:\n",
    "- **6.2**: Defining tables, views, and sinks in detail\n",
    "- **6.3**: Streaming tables and real-time processing patterns\n",
    "- **6.4**: Data quality expectations and validation\n",
    "- **6.5**: Advanced flows and CDC patterns\n",
    "- **6.6**: Best practices and anti-patterns\n",
    "\n",
    "Lakeflow Declarative Pipelines represents a significant evolution in data engineering, bringing declarative, functional paradigms to large-scale data processing with Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Practice understanding the evolution from DLT to Lakeflow:\n",
    "\n",
    "**Exercise 1: Identify Migration Patterns**\n",
    "- Review your existing DLT code (if available)\n",
    "- Identify which decorators need to change\n",
    "- Create a migration checklist\n",
    "\n",
    "**Exercise 2: Table Type Selection**\n",
    "- For each scenario below, choose the appropriate decorator:\n",
    "  - Real-time clickstream events\n",
    "  - Daily customer dimension table\n",
    "  - Complex 5-table join used in multiple places\n",
    "  - Intermediate filter step (active users only)\n",
    "\n",
    "**Exercise 3: Declarative vs Imperative**\n",
    "- Take an existing imperative PySpark pipeline\n",
    "- Redesign it using `@dp` decorators\n",
    "- Compare code readability and maintainability\n",
    "\n",
    "**Exercise 4: Dependency Mapping**\n",
    "- Draw the dependency graph for a multi-table pipeline\n",
    "- Identify which tables can run in parallel\n",
    "- Understand how Lakeflow resolves execution order\n",
    "\n",
    "**Exercise 5: Migration Planning**\n",
    "- Estimate the effort to migrate an existing DLT pipeline\n",
    "- Identify any Databricks-specific features used\n",
    "- Plan a phased migration approach\n",
    "\n",
    "In the next notebook, we'll implement actual Lakeflow pipelines with hands-on code examples!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
