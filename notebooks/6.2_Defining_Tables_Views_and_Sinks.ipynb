{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Defining Tables, Views, and Sinks in Lakeflow\n",
    "\n",
    "This notebook provides hands-on implementation guidance for defining tables, materialized views, temporary views, and sinks using `pyspark.pipelines`. We'll explore configuration options, functional patterns, and best practices for each table type.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Define batch tables with `@dp.table` and configuration options\n",
    "- Create materialized views for pre-computed aggregations\n",
    "- Use temporary views for intermediate transformations\n",
    "- Configure table properties (partitioning, clustering, schema enforcement)\n",
    "- Define custom sinks for specialized output patterns\n",
    "- Apply functional programming principles to table definitions\n",
    "- Structure table definitions for testability and reusability\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Notebook 6.1 (Introduction to pyspark.pipelines)\n",
    "- Understanding of PySpark DataFrames\n",
    "- Knowledge of Delta Lake table properties\n",
    "- Familiarity with functional programming concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform setup detection\n",
    "# In Databricks: Keep commented\n",
    "# In Local: Uncomment this line\n",
    "# %run 00_Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for Lakeflow pipelines\n",
    "# NOTE: Actual pyspark.pipelines requires Spark 4.1+ and pipeline execution mode\n",
    "# This notebook demonstrates concepts with educational examples\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# In a real Lakeflow pipeline:\n",
    "# from pyspark import pipelines as dp\n",
    "\n",
    "print(\"✅ Imports complete - Ready for table definition demonstration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining Batch Tables with @dp.table\n",
    "\n",
    "### Basic Table Definition\n",
    "\n",
    "The `@dp.table` decorator defines a materialized Delta table that is fully refreshed on each pipeline run.\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "@dp.table(\n",
    "    name=\"customers\",                    # Table name (optional, defaults to function name)\n",
    "    comment=\"Customer master table\"      # Documentation\n",
    ")\n",
    "def customers():\n",
    "    \"\"\"\n",
    "    Pure function returning a DataFrame.\n",
    "    No side effects - Lakeflow handles materialization.\n",
    "    \"\"\"\n",
    "    return spark.read.table(\"raw.customers\")\n",
    "```\n",
    "\n",
    "### Table Definition Best Practices\n",
    "\n",
    "**✅ DO: Keep table functions pure**\n",
    "```python\n",
    "@dp.table\n",
    "def clean_customers():\n",
    "    \"\"\"Pure function - only returns DataFrame\"\"\"\n",
    "    return (\n",
    "        spark.table(\"raw.customers\")\n",
    "        .filter(F.col(\"status\") == \"active\")\n",
    "        .select(\"customer_id\", \"name\", \"email\", \"country\")\n",
    "    )\n",
    "    # ✓ No .write(), no .collect(), no side effects\n",
    "```\n",
    "\n",
    "**❌ DON'T: Include side effects in table functions**\n",
    "```python\n",
    "@dp.table\n",
    "def bad_customers():\n",
    "    df = spark.table(\"raw.customers\")\n",
    "    \n",
    "    # ❌ WRONG: These are prohibited in Lakeflow\n",
    "    df.write.save(\"/some/path\")           # Side effect\n",
    "    count = df.count()                    # Action (triggers computation)\n",
    "    print(f\"Processing {count} records\") # Side effect\n",
    "    df.cache()                            # State mutation\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "### Configuration Options\n",
    "\n",
    "```python\n",
    "@dp.table(\n",
    "    name=\"customers\",\n",
    "    comment=\"Customer master table with comprehensive metadata\",\n",
    "    \n",
    "    # Table properties\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\",\n",
    "        \"delta.targetFileSize\": \"128mb\"\n",
    "    },\n",
    "    \n",
    "    # Partitioning (for large tables)\n",
    "    partition_cols=[\"country\", \"signup_year\"],\n",
    "    \n",
    "    # Path specification (optional)\n",
    "    path=\"/mnt/delta/customers\"\n",
    ")\n",
    "def customers():\n",
    "    return (\n",
    "        spark.table(\"raw.customers\")\n",
    "        .withColumn(\"signup_year\", F.year(\"signup_date\"))\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Materialized Views with @dp.materialized_view\n",
    "\n",
    "### When to Use Materialized Views\n",
    "\n",
    "Materialized views are ideal for:\n",
    "- Complex joins across multiple tables\n",
    "- Expensive aggregations used by multiple downstream consumers\n",
    "- Pre-computed metrics for dashboards and reporting\n",
    "- Denormalized tables for query performance\n",
    "\n",
    "### Basic Materialized View\n",
    "\n",
    "```python\n",
    "@dp.materialized_view(\n",
    "    name=\"customer_order_summary\",\n",
    "    comment=\"Pre-computed customer order metrics\"\n",
    ")\n",
    "def customer_order_summary():\n",
    "    \"\"\"\n",
    "    Expensive join and aggregation computed once per pipeline run.\n",
    "    Results are materialized for fast query performance.\n",
    "    \"\"\"\n",
    "    customers = dp.read(\"customers\")\n",
    "    orders = dp.read(\"orders\")\n",
    "    \n",
    "    return (\n",
    "        orders\n",
    "        .join(customers, \"customer_id\")\n",
    "        .groupBy(\n",
    "            \"customer_id\",\n",
    "            \"customer_name\",\n",
    "            \"country\"\n",
    "        )\n",
    "        .agg(\n",
    "            F.count(\"order_id\").alias(\"total_orders\"),\n",
    "            F.sum(\"order_total\").alias(\"total_spent\"),\n",
    "            F.avg(\"order_total\").alias(\"avg_order_value\"),\n",
    "            F.max(\"order_date\").alias(\"last_order_date\")\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "### Functional Composition with Materialized Views\n",
    "\n",
    "```python\n",
    "# Pure transformation functions\n",
    "def enrich_with_customer_tier(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function: Classify customers by spending\"\"\"\n",
    "    return df.withColumn(\n",
    "        \"tier\",\n",
    "        F.when(F.col(\"total_spent\") >= 10000, \"Platinum\")\n",
    "         .when(F.col(\"total_spent\") >= 5000, \"Gold\")\n",
    "         .when(F.col(\"total_spent\") >= 1000, \"Silver\")\n",
    "         .otherwise(\"Bronze\")\n",
    "    )\n",
    "\n",
    "def calculate_customer_health_score(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function: Calculate engagement score\"\"\"\n",
    "    return df.withColumn(\n",
    "        \"health_score\",\n",
    "        (\n",
    "            (F.col(\"total_orders\") * 10) +\n",
    "            (F.col(\"total_spent\") / 100) +\n",
    "            (F.datediff(F.current_date(), F.col(\"last_order_date\")) * -1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Compose pure functions in materialized view\n",
    "@dp.materialized_view(name=\"customer_analytics\")\n",
    "def customer_analytics():\n",
    "    \"\"\"Composed analytics from pure functions\"\"\"\n",
    "    return (\n",
    "        dp.read(\"customer_order_summary\")\n",
    "        .transform(enrich_with_customer_tier)\n",
    "        .transform(calculate_customer_health_score)\n",
    "    )\n",
    "```\n",
    "\n",
    "### Materialized View vs Table: Decision Matrix\n",
    "\n",
    "| Criteria | @dp.table | @dp.materialized_view |\n",
    "|----------|-----------|------------------------|\n",
    "| **Purpose** | Primary data storage | Pre-computed aggregations |\n",
    "| **Source** | Raw/external data | Derived from other tables |\n",
    "| **Complexity** | Simple transformations | Complex joins/aggregations |\n",
    "| **Consumers** | Few to many | Many consumers |\n",
    "| **Update Pattern** | Replace or append | Refresh on pipeline run |\n",
    "| **Best For** | Source tables, dimensions | Analytics, reporting, metrics |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporary Views with @dp.temporary_view\n",
    "\n",
    "### Purpose and Benefits\n",
    "\n",
    "Temporary views are **logical views** that:\n",
    "- Are NOT materialized to storage\n",
    "- Are computed on-demand when referenced\n",
    "- Save storage costs for intermediate transformations\n",
    "- Enable reusable subquery logic\n",
    "\n",
    "### When to Use Temporary Views\n",
    "\n",
    "**✅ Use temporary views for:**\n",
    "- Intermediate filter steps\n",
    "- Reusable subqueries used by multiple tables\n",
    "- Logic that's referenced only within the same pipeline\n",
    "- Simple transformations that are cheap to recompute\n",
    "\n",
    "**❌ Don't use temporary views for:**\n",
    "- Expensive computations (joins, aggregations)\n",
    "- Tables queried by external systems\n",
    "- Results that need to be persisted\n",
    "- Complex transformations computed multiple times\n",
    "\n",
    "### Basic Temporary View\n",
    "\n",
    "```python\n",
    "@dp.temporary_view(\n",
    "    name=\"active_customers\",\n",
    "    comment=\"Logical view of active customers (not materialized)\"\n",
    ")\n",
    "def active_customers():\n",
    "    \"\"\"Filter to active customers - no storage cost\"\"\"\n",
    "    return (\n",
    "        dp.read(\"customers\")\n",
    "        .filter(F.col(\"status\") == \"active\")\n",
    "        .filter(F.col(\"email_verified\") == True)\n",
    "    )\n",
    "\n",
    "# Multiple tables can reference this view\n",
    "@dp.table\n",
    "def active_customer_orders():\n",
    "    return (\n",
    "        dp.read(\"active_customers\")  # Computed on-demand\n",
    "        .join(dp.read(\"orders\"), \"customer_id\")\n",
    "    )\n",
    "\n",
    "@dp.table\n",
    "def active_customer_subscriptions():\n",
    "    return (\n",
    "        dp.read(\"active_customers\")  # Recomputed (not cached)\n",
    "        .join(dp.read(\"subscriptions\"), \"customer_id\")\n",
    "    )\n",
    "```\n",
    "\n",
    "### Functional Reusability Pattern\n",
    "\n",
    "```python\n",
    "# Define reusable filters as temporary views\n",
    "@dp.temporary_view\n",
    "def high_value_customers():\n",
    "    \"\"\"Customers with significant lifetime value\"\"\"\n",
    "    return (\n",
    "        dp.read(\"customer_order_summary\")\n",
    "        .filter(F.col(\"total_spent\") >= 5000)\n",
    "        .filter(F.col(\"total_orders\") >= 10)\n",
    "    )\n",
    "\n",
    "@dp.temporary_view\n",
    "def recent_customers():\n",
    "    \"\"\"Customers who ordered in last 30 days\"\"\"\n",
    "    return (\n",
    "        dp.read(\"customer_order_summary\")\n",
    "        .filter(\n",
    "            F.datediff(F.current_date(), F.col(\"last_order_date\")) <= 30\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Combine views for specialized segments\n",
    "@dp.table\n",
    "def vip_active_customers():\n",
    "    \"\"\"High-value customers who are recently active\"\"\"\n",
    "    return (\n",
    "        dp.read(\"high_value_customers\")\n",
    "        .join(\n",
    "            dp.read(\"recent_customers\"),\n",
    "            \"customer_id\",\n",
    "            \"inner\"\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "### Performance Consideration\n",
    "\n",
    "```python\n",
    "# ⚠️ CAUTION: Expensive view used multiple times\n",
    "@dp.temporary_view\n",
    "def expensive_aggregation():\n",
    "    \"\"\"Complex computation - consider materializing!\"\"\"\n",
    "    return (\n",
    "        dp.read(\"large_table\")\n",
    "        .join(dp.read(\"another_large_table\"), \"key\")  # Expensive\n",
    "        .groupBy(\"dimension\").agg(F.sum(\"metric\"))     # Expensive\n",
    "    )\n",
    "\n",
    "# This will recompute the expensive aggregation THREE times:\n",
    "@dp.table\n",
    "def report_1():\n",
    "    return dp.read(\"expensive_aggregation\").filter(...)  # Compute #1\n",
    "\n",
    "@dp.table\n",
    "def report_2():\n",
    "    return dp.read(\"expensive_aggregation\").filter(...)  # Compute #2\n",
    "\n",
    "@dp.table\n",
    "def report_3():\n",
    "    return dp.read(\"expensive_aggregation\").filter(...)  # Compute #3\n",
    "\n",
    "# ✅ BETTER: Materialize expensive computation\n",
    "@dp.materialized_view  # Changed from temporary_view\n",
    "def expensive_aggregation():\n",
    "    \"\"\"Computed once, reused three times\"\"\"\n",
    "    return (...)  # Same logic, now materialized\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Table Properties and Configuration\n",
    "\n",
    "### Delta Lake Optimization Properties\n",
    "\n",
    "```python\n",
    "@dp.table(\n",
    "    name=\"optimized_events\",\n",
    "    table_properties={\n",
    "        # Auto-optimization\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\",\n",
    "        \n",
    "        # File sizing\n",
    "        \"delta.targetFileSize\": \"128mb\",\n",
    "        \n",
    "        # Data skipping\n",
    "        \"delta.dataSkippingNumIndexedCols\": \"32\",\n",
    "        \n",
    "        # Retention\n",
    "        \"delta.deletedFileRetentionDuration\": \"interval 7 days\",\n",
    "        \"delta.logRetentionDuration\": \"interval 30 days\"\n",
    "    }\n",
    ")\n",
    "def optimized_events():\n",
    "    return spark.table(\"raw.events\")\n",
    "```\n",
    "\n",
    "### Partitioning Strategies\n",
    "\n",
    "```python\n",
    "# Time-based partitioning (common for event data)\n",
    "@dp.table(\n",
    "    name=\"events_partitioned\",\n",
    "    partition_cols=[\"event_date\"],  # Partition by date\n",
    "    comment=\"Events partitioned by date for efficient time-range queries\"\n",
    ")\n",
    "def events_partitioned():\n",
    "    return (\n",
    "        spark.table(\"raw.events\")\n",
    "        .withColumn(\"event_date\", F.to_date(\"event_timestamp\"))\n",
    "    )\n",
    "\n",
    "# Multi-level partitioning (for very large tables)\n",
    "@dp.table(\n",
    "    name=\"transactions_partitioned\",\n",
    "    partition_cols=[\"year\", \"month\", \"country\"],\n",
    "    comment=\"Hierarchical partitioning for global transaction data\"\n",
    ")\n",
    "def transactions_partitioned():\n",
    "    return (\n",
    "        spark.table(\"raw.transactions\")\n",
    "        .withColumn(\"year\", F.year(\"transaction_date\"))\n",
    "        .withColumn(\"month\", F.month(\"transaction_date\"))\n",
    "    )\n",
    "```\n",
    "\n",
    "### Liquid Clustering (Databricks-specific)\n",
    "\n",
    "```python\n",
    "@dp.table(\n",
    "    name=\"customers_clustered\",\n",
    "    table_properties={\n",
    "        \"delta.enableChangeDataFeed\": \"true\",\n",
    "        \"delta.feature.clustering\": \"supported\"\n",
    "    },\n",
    "    cluster_cols=[\"country\", \"customer_tier\"],  # Databricks-specific\n",
    "    comment=\"Customers with liquid clustering for flexible query patterns\"\n",
    ")\n",
    "def customers_clustered():\n",
    "    return spark.table(\"raw.customers\")\n",
    "# Benefits: Better than static partitioning for evolving query patterns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Patterns: Dependency Management\n",
    "\n",
    "### Reading from Other Pipeline Tables\n",
    "\n",
    "```python\n",
    "# Method 1: Using dp.read() (recommended)\n",
    "@dp.table\n",
    "def silver_customers():\n",
    "    return dp.read(\"bronze_customers\").filter(...)\n",
    "    # ✓ Lakeflow tracks dependency: bronze → silver\n",
    "\n",
    "# Method 2: Using spark.table() for external tables\n",
    "@dp.table\n",
    "def external_data():\n",
    "    return spark.table(\"external_catalog.schema.table\")\n",
    "    # ✓ For tables outside the pipeline\n",
    "```\n",
    "\n",
    "### Complex Dependency Graphs\n",
    "\n",
    "```python\n",
    "# Bronze layer: Raw data ingestion\n",
    "@dp.table\n",
    "def bronze_orders():\n",
    "    return spark.read.table(\"raw.orders\")\n",
    "\n",
    "@dp.table\n",
    "def bronze_customers():\n",
    "    return spark.read.table(\"raw.customers\")\n",
    "\n",
    "@dp.table\n",
    "def bronze_products():\n",
    "    return spark.read.table(\"raw.products\")\n",
    "\n",
    "# Silver layer: Cleaned and joined data\n",
    "@dp.table\n",
    "def silver_orders_enriched():\n",
    "    \"\"\"Orders enriched with customer and product data\"\"\"\n",
    "    orders = dp.read(\"bronze_orders\")\n",
    "    customers = dp.read(\"bronze_customers\")\n",
    "    products = dp.read(\"bronze_products\")\n",
    "    \n",
    "    return (\n",
    "        orders\n",
    "        .join(customers, \"customer_id\")\n",
    "        .join(products, \"product_id\")\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"order_date\",\n",
    "            \"customer_id\",\n",
    "            customers[\"customer_name\"],\n",
    "            \"product_id\",\n",
    "            products[\"product_name\"],\n",
    "            \"quantity\",\n",
    "            \"unit_price\",\n",
    "            (F.col(\"quantity\") * F.col(\"unit_price\")).alias(\"total\")\n",
    "        )\n",
    "    )\n",
    "# Dependencies: bronze_orders, bronze_customers, bronze_products → silver_orders_enriched\n",
    "\n",
    "# Gold layer: Business aggregations\n",
    "@dp.materialized_view\n",
    "def gold_daily_revenue():\n",
    "    \"\"\"Daily revenue aggregation\"\"\"\n",
    "    return (\n",
    "        dp.read(\"silver_orders_enriched\")\n",
    "        .groupBy(F.to_date(\"order_date\").alias(\"date\"))\n",
    "        .agg(\n",
    "            F.sum(\"total\").alias(\"revenue\"),\n",
    "            F.count(\"order_id\").alias(\"order_count\"),\n",
    "            F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "        )\n",
    "    )\n",
    "# Dependency: silver_orders_enriched → gold_daily_revenue\n",
    "\n",
    "# Lakeflow execution order (automatically determined):\n",
    "# 1. bronze_orders, bronze_customers, bronze_products (parallel)\n",
    "# 2. silver_orders_enriched (waits for all bronze)\n",
    "# 3. gold_daily_revenue (waits for silver)\n",
    "```\n",
    "\n",
    "### Cross-Pipeline References\n",
    "\n",
    "```python\n",
    "# Reading from another pipeline's output\n",
    "@dp.table\n",
    "def my_table():\n",
    "    # Reference table from a different pipeline\n",
    "    other_pipeline_table = spark.table(\"catalog.schema.other_pipeline_output\")\n",
    "    return other_pipeline_table.filter(...)\n",
    "    # Note: Lakeflow Jobs can orchestrate cross-pipeline dependencies\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Sinks with dp.create_sink()\n",
    "\n",
    "### What Are Sinks?\n",
    "\n",
    "Sinks allow you to write pipeline data to custom destinations beyond Delta tables.\n",
    "\n",
    "**Common Use Cases:**\n",
    "- Write to external databases (PostgreSQL, MySQL, etc.)\n",
    "- Export to cloud storage (S3, Azure Blob, GCS)\n",
    "- Send to messaging systems (Kafka, Event Hubs)\n",
    "- Push to APIs or web services\n",
    "\n",
    "### Basic Sink Definition (Databricks-specific)\n",
    "\n",
    "```python\n",
    "# Create a custom sink for a table\n",
    "dp.create_sink(\n",
    "    source=dp.read(\"gold_customer_metrics\"),\n",
    "    destination=\"external_analytics\",\n",
    "    mode=\"overwrite\",\n",
    "    options={\n",
    "        \"url\": \"jdbc:postgresql://host:5432/analytics\",\n",
    "        \"dbtable\": \"customer_metrics\",\n",
    "        \"user\": \"etl_user\",\n",
    "        \"password\": dbutils.secrets.get(\"db\", \"password\")\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### Sink Patterns\n",
    "\n",
    "```python\n",
    "# Pattern 1: Export to Parquet for external systems\n",
    "dp.create_sink(\n",
    "    source=dp.read(\"gold_daily_revenue\"),\n",
    "    destination=\"/mnt/exports/revenue\",\n",
    "    format=\"parquet\",\n",
    "    mode=\"overwrite\",\n",
    "    partition_by=[\"year\", \"month\"]\n",
    ")\n",
    "\n",
    "# Pattern 2: Incremental export (append mode)\n",
    "dp.create_sink(\n",
    "    source=dp.read(\"silver_events\"),\n",
    "    destination=\"/mnt/exports/events\",\n",
    "    format=\"delta\",\n",
    "    mode=\"append\"\n",
    ")\n",
    "\n",
    "# Pattern 3: Export to external warehouse\n",
    "dp.create_sink(\n",
    "    source=dp.read(\"gold_customer_analytics\"),\n",
    "    destination=\"analytics_warehouse\",\n",
    "    options={\n",
    "        \"url\": \"jdbc:snowflake://account.snowflakecomputing.com\",\n",
    "        \"dbtable\": \"analytics.customer_metrics\",\n",
    "        \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "        \"sfDatabase\": \"ANALYTICS\",\n",
    "        \"sfSchema\": \"PUBLIC\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### Sink vs Table: When to Use Each\n",
    "\n",
    "| Feature | @dp.table | dp.create_sink() |\n",
    "|---------|-----------|------------------|\n",
    "| **Storage** | Delta Lake (lakehouse) | Custom destination |\n",
    "| **Primary Use** | Pipeline transformations | External system integration |\n",
    "| **Queryable in Pipeline** | Yes (via dp.read()) | No |\n",
    "| **Unity Catalog** | Integrated | External |\n",
    "| **Best For** | Internal pipeline data | Exporting to external systems |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testable and Reusable Table Definitions\n",
    "\n",
    "### Functional Composition for Testability\n",
    "\n",
    "```python\n",
    "# Pure transformation functions (testable in isolation)\n",
    "def filter_active_status(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function: Filter to active records\"\"\"\n",
    "    return df.filter(F.col(\"status\") == \"active\")\n",
    "\n",
    "def enrich_with_age_group(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function: Add age group classification\"\"\"\n",
    "    return df.withColumn(\n",
    "        \"age_group\",\n",
    "        F.when(F.col(\"age\") < 18, \"Under 18\")\n",
    "         .when(F.col(\"age\") < 35, \"18-34\")\n",
    "         .when(F.col(\"age\") < 55, \"35-54\")\n",
    "         .otherwise(\"55+\")\n",
    "    )\n",
    "\n",
    "def standardize_country_codes(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function: Normalize country codes to ISO 3166-1 alpha-2\"\"\"\n",
    "    return df.withColumn(\n",
    "        \"country_code\",\n",
    "        F.upper(F.trim(F.col(\"country\")))\n",
    "    )\n",
    "\n",
    "# Compose pure functions in table definition\n",
    "@dp.table(\n",
    "    name=\"customers_processed\",\n",
    "    comment=\"Customers with derived attributes\"\n",
    ")\n",
    "def customers_processed():\n",
    "    \"\"\"Table definition composed from testable pure functions\"\"\"\n",
    "    return (\n",
    "        dp.read(\"bronze_customers\")\n",
    "        .transform(filter_active_status)\n",
    "        .transform(enrich_with_age_group)\n",
    "        .transform(standardize_country_codes)\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            \"name\",\n",
    "            \"email\",\n",
    "            \"age\",\n",
    "            \"age_group\",\n",
    "            \"country_code\",\n",
    "            \"signup_date\"\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "### Testing Pure Transformation Functions\n",
    "\n",
    "```python\n",
    "# test_transformations.py\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from chispa.dataframe_comparer import assert_df_equality\n",
    "\n",
    "def test_filter_active_status(spark: SparkSession):\n",
    "    \"\"\"Test active status filter in isolation\"\"\"\n",
    "    # Input data\n",
    "    input_df = spark.createDataFrame([\n",
    "        (1, \"active\"),\n",
    "        (2, \"inactive\"),\n",
    "        (3, \"active\"),\n",
    "    ], [\"id\", \"status\"])\n",
    "    \n",
    "    # Expected output\n",
    "    expected_df = spark.createDataFrame([\n",
    "        (1, \"active\"),\n",
    "        (3, \"active\"),\n",
    "    ], [\"id\", \"status\"])\n",
    "    \n",
    "    # Test transformation\n",
    "    result_df = filter_active_status(input_df)\n",
    "    \n",
    "    # Assert equality\n",
    "    assert_df_equality(result_df, expected_df)\n",
    "\n",
    "def test_enrich_with_age_group(spark: SparkSession):\n",
    "    \"\"\"Test age group enrichment\"\"\"\n",
    "    input_df = spark.createDataFrame([\n",
    "        (1, 15),\n",
    "        (2, 25),\n",
    "        (3, 45),\n",
    "        (4, 65),\n",
    "    ], [\"id\", \"age\"])\n",
    "    \n",
    "    result_df = enrich_with_age_group(input_df)\n",
    "    \n",
    "    age_groups = [row.age_group for row in result_df.collect()]\n",
    "    assert age_groups == [\"Under 18\", \"18-34\", \"35-54\", \"55+\"]\n",
    "```\n",
    "\n",
    "### Configuration-Driven Table Definitions\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TableConfig:\n",
    "    \"\"\"Immutable configuration for table definitions\"\"\"\n",
    "    name: str\n",
    "    source_table: str\n",
    "    partition_cols: List[str]\n",
    "    table_properties: Dict[str, str]\n",
    "    comment: str\n",
    "\n",
    "# Configuration instances\n",
    "CUSTOMER_CONFIG = TableConfig(\n",
    "    name=\"customers_silver\",\n",
    "    source_table=\"bronze.customers\",\n",
    "    partition_cols=[\"country\"],\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.targetFileSize\": \"128mb\"\n",
    "    },\n",
    "    comment=\"Silver layer customer data\"\n",
    ")\n",
    "\n",
    "# Use configuration in table definition\n",
    "@dp.table(\n",
    "    name=CUSTOMER_CONFIG.name,\n",
    "    partition_cols=CUSTOMER_CONFIG.partition_cols,\n",
    "    table_properties=CUSTOMER_CONFIG.table_properties,\n",
    "    comment=CUSTOMER_CONFIG.comment\n",
    ")\n",
    "def customers_silver():\n",
    "    return spark.table(CUSTOMER_CONFIG.source_table).filter(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored how to define tables, views, and sinks in Lakeflow Declarative Pipelines:\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Batch Tables (`@dp.table`)**\n",
    "   - Materialized Delta tables for primary data storage\n",
    "   - Configuration options: partitioning, clustering, table properties\n",
    "   - Pure function definitions (no side effects)\n",
    "\n",
    "2. **Materialized Views (`@dp.materialized_view`)**\n",
    "   - Pre-computed aggregations and complex joins\n",
    "   - Refreshed on each pipeline run\n",
    "   - Ideal for analytics and reporting workloads\n",
    "\n",
    "3. **Temporary Views (`@dp.temporary_view`)**\n",
    "   - Logical views without physical storage\n",
    "   - Best for intermediate transformations\n",
    "   - Reusable subquery patterns\n",
    "\n",
    "4. **Table Properties**\n",
    "   - Delta Lake optimization settings\n",
    "   - Partitioning strategies\n",
    "   - Liquid clustering (Databricks-specific)\n",
    "\n",
    "5. **Dependency Management**\n",
    "   - Using `dp.read()` for pipeline dependencies\n",
    "   - Automatic dependency resolution\n",
    "   - Complex multi-layer architectures\n",
    "\n",
    "6. **Custom Sinks**\n",
    "   - Exporting to external systems\n",
    "   - JDBC, cloud storage, messaging systems\n",
    "   - Complementing Delta Lake tables\n",
    "\n",
    "7. **Testability**\n",
    "   - Pure transformation functions\n",
    "   - Functional composition with `.transform()`\n",
    "   - Configuration-driven definitions\n",
    "\n",
    "### Functional Programming Benefits\n",
    "\n",
    "- **Pure Functions**: Table definitions return DataFrames without side effects\n",
    "- **Composition**: Build complex pipelines from simple, testable functions\n",
    "- **Immutability**: Tables are immutable, transformations create new tables\n",
    "- **Declarative**: Focus on what tables contain, not how to build them\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "✅ Keep table functions pure (no `.write()`, `.collect()`, `.count()`)\n",
    "✅ Use appropriate decorator for use case (table vs view vs temporary view)\n",
    "✅ Extract transformation logic into testable pure functions\n",
    "✅ Configure table properties for optimal performance\n",
    "✅ Use temporary views for cheap intermediate steps\n",
    "✅ Materialize expensive computations used multiple times\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **6.3**: Streaming tables and real-time processing\n",
    "- **6.4**: Data quality with expectations\n",
    "- **6.5**: Advanced flows and CDC patterns\n",
    "- **6.6**: Best practices and anti-patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Practice defining tables and views:\n",
    "\n",
    "**Exercise 1: Table Type Selection**\n",
    "For each scenario, choose `@dp.table`, `@dp.materialized_view`, or `@dp.temporary_view`:\n",
    "- Product catalog updated nightly\n",
    "- Complex 4-table join for dashboard\n",
    "- Filter step: \"orders from last 30 days\"\n",
    "- Daily sales aggregation by region\n",
    "\n",
    "**Exercise 2: Pure Function Extraction**\n",
    "- Take an existing table definition\n",
    "- Extract transformation logic into pure functions\n",
    "- Compose using `.transform()`\n",
    "- Write unit tests for each function\n",
    "\n",
    "**Exercise 3: Configuration Design**\n",
    "- Create a `TableConfig` dataclass for your domain\n",
    "- Define table properties for optimal performance\n",
    "- Choose appropriate partitioning strategy\n",
    "- Document configuration decisions\n",
    "\n",
    "**Exercise 4: Dependency Graph**\n",
    "- Design a 3-layer pipeline (bronze/silver/gold)\n",
    "- Define 5-7 tables with dependencies\n",
    "- Include temporary views for reusable logic\n",
    "- Add materialized view for complex aggregation\n",
    "\n",
    "**Exercise 5: Sink Configuration**\n",
    "- Identify a table that should be exported\n",
    "- Design sink configuration for external system\n",
    "- Choose appropriate export format and mode\n",
    "- Document integration requirements\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
