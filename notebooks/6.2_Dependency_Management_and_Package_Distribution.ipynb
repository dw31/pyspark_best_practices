{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Dependency Management and Package Distribution\n",
    "\n",
    "This notebook demonstrates best practices for managing Python dependencies and distributing PySpark code in Databricks environments.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand dependency management strategies in Databricks\n",
    "- Learn to package PySpark modules for distribution\n",
    "- Master library installation patterns\n",
    "- Create wheel packages for functional modules\n",
    "- Implement version control for dependencies\n",
    "- Build reusable, distributable PySpark libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local development: Uncomment the next line\n",
    "# %run 00_Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Dependency Management Matters\n",
    "\n",
    "Proper dependency management is critical for production PySpark applications:\n",
    "\n",
    "**Key Challenges**:\n",
    "1. **Reproducibility**: Ensure code runs identically across environments\n",
    "2. **Version Conflicts**: Manage conflicting library versions\n",
    "3. **Distribution**: Deploy code across Spark cluster nodes\n",
    "4. **Governance**: Control library usage in production\n",
    "5. **Performance**: Minimize overhead from dependency loading\n",
    "\n",
    "**Functional Programming Alignment**:\n",
    "- Well-managed dependencies enable pure function libraries to be shared\n",
    "- Versioned packages ensure deterministic behavior\n",
    "- Proper distribution ensures transformations execute consistently across nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Library Installation Patterns\n",
    "\n",
    "Databricks supports multiple methods for installing Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Databricks Library Installation Methods ===\")\n",
    "\n",
    "installation_methods = \"\"\"\n",
    "1. CLUSTER-SCOPED LIBRARIES\n",
    "   - Installed on entire cluster\n",
    "   - Available to all notebooks and jobs on that cluster\n",
    "   - Requires cluster restart\n",
    "   - Best for: Shared dependencies across team\n",
    "   \n",
    "   Installation:\n",
    "   - UI: Cluster > Libraries > Install New\n",
    "   - API: Databricks CLI or REST API\n",
    "   \n",
    "   Use Cases:\n",
    "   - Common data science libraries (pandas, numpy, scikit-learn)\n",
    "   - Stable, production dependencies\n",
    "   - Team-wide standards\n",
    "\n",
    "2. NOTEBOOK-SCOPED LIBRARIES (%pip, %conda)\n",
    "   - Installed in specific notebook session only\n",
    "   - No cluster restart required\n",
    "   - Isolated from other notebooks\n",
    "   - Best for: Experimentation, notebook-specific needs\n",
    "   \n",
    "   Installation:\n",
    "   %pip install <package-name>\n",
    "   %pip install -r requirements.txt\n",
    "   \n",
    "   Use Cases:\n",
    "   - Exploratory analysis\n",
    "   - Testing new libraries\n",
    "   - Temporary dependencies\n",
    "\n",
    "3. WORKSPACE FILES\n",
    "   - Upload Python files (.py, .whl) to workspace\n",
    "   - Import as modules in notebooks\n",
    "   - Version controlled through workspace\n",
    "   - Best for: Internal utility modules\n",
    "   \n",
    "   Installation:\n",
    "   - Upload via UI or Databricks CLI\n",
    "   - Import: sys.path.append('/Workspace/...')\n",
    "   \n",
    "   Use Cases:\n",
    "   - Custom utility functions\n",
    "   - Internal libraries\n",
    "   - Shared code modules\n",
    "\n",
    "4. UNITY CATALOG VOLUMES (Recommended for Production)\n",
    "   - Centralized library storage with governance\n",
    "   - Access control and auditing\n",
    "   - Version management\n",
    "   - Best for: Production dependencies\n",
    "   \n",
    "   Installation:\n",
    "   %pip install /Volumes/catalog/schema/volume/package.whl\n",
    "   \n",
    "   Use Cases:\n",
    "   - Production-grade libraries\n",
    "   - Governance requirements\n",
    "   - Centralized dependency management\n",
    "\n",
    "5. PYPI (Public Python Package Index)\n",
    "   - Install from PyPI repository\n",
    "   - Largest ecosystem of Python packages\n",
    "   - Best for: Public, well-maintained libraries\n",
    "   \n",
    "   Installation:\n",
    "   %pip install package-name==version\n",
    "   \n",
    "   Use Cases:\n",
    "   - Standard libraries (requests, boto3, etc.)\n",
    "   - Popular data science tools\n",
    "   - Open-source dependencies\n",
    "\"\"\"\n",
    "\n",
    "print(installation_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook-Scoped Library Installation\n",
    "\n",
    "Demonstrate notebook-scoped installation with %pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ BEST PRACTICE: Notebook-scoped installation for isolated dependencies\n",
    "\n",
    "# Example: Install specific version for reproducibility\n",
    "# %pip install pandas==2.0.0\n",
    "\n",
    "# Example: Install from requirements file\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "# Example: Install multiple packages\n",
    "# %pip install requests boto3 pyyaml\n",
    "\n",
    "# Example: Install from Unity Catalog volume\n",
    "# %pip install /Volumes/main/libraries/python/my_package-1.0.0-py3-none-any.whl\n",
    "\n",
    "print(\"=== Notebook-Scoped Installation Best Practices ===\")\n",
    "print(\"\\n✅ DO:\")\n",
    "print(\"  - Pin exact versions (package==1.2.3) for reproducibility\")\n",
    "print(\"  - Use requirements.txt for complex dependencies\")\n",
    "print(\"  - Install at the beginning of notebook\")\n",
    "print(\"  - Document why each library is needed\")\n",
    "print(\"  - Use %pip (not !pip) in Databricks\")\n",
    "\n",
    "print(\"\\n❌ DON'T:\")\n",
    "print(\"  - Install in the middle of notebook execution\")\n",
    "print(\"  - Use unpinned versions (package without version)\")\n",
    "print(\"  - Install unnecessary dependencies\")\n",
    "print(\"  - Mix %pip and !pip commands\")\n",
    "print(\"  - Install different versions in different cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Reusable Python Modules\n",
    "\n",
    "Package functional PySpark code into reusable modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Creating Reusable PySpark Modules ===\")\n",
    "\n",
    "# Example: Module structure for reusable transformations\n",
    "module_example = '''\n",
    "# File: pyspark_transformations/cleaning.py\n",
    "\"\"\"\n",
    "Data cleaning transformations for PySpark.\n",
    "All functions are pure - DataFrame in, DataFrame out.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from typing import List\n",
    "\n",
    "__version__ = \"1.0.0\"\n",
    "__author__ = \"Your Team\"\n",
    "\n",
    "def remove_nulls(df: DataFrame, columns: List[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows with null values in specified columns.\n",
    "    \n",
    "    Pure function - no side effects.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        columns: List of column names to check for nulls\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with null rows removed\n",
    "        \n",
    "    Example:\n",
    "        >>> clean_df = remove_nulls(raw_df, [\"id\", \"amount\"])\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df = df.filter(F.col(col).isNotNull())\n",
    "    return df\n",
    "\n",
    "def standardize_column_names(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize column names to lowercase with underscores.\n",
    "    \n",
    "    Pure function - idempotent transformation.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with standardized column names\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        new_name = col_name.lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "        if new_name != col_name:\n",
    "            df = df.withColumnRenamed(col_name, new_name)\n",
    "    return df\n",
    "\n",
    "def filter_date_range(df: DataFrame, date_column: str, \n",
    "                     start_date: str, end_date: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter DataFrame to date range.\n",
    "    \n",
    "    Pure function with configurable parameters.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        date_column: Name of date column\n",
    "        start_date: Start date (YYYY-MM-DD format)\n",
    "        end_date: End date (YYYY-MM-DD format)\n",
    "        \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    return df.filter(\n",
    "        (F.col(date_column) >= F.lit(start_date)) &\n",
    "        (F.col(date_column) <= F.lit(end_date))\n",
    "    )\n",
    "'''\n",
    "\n",
    "print(\"Example reusable module:\")\n",
    "print(module_example)\n",
    "\n",
    "print(\"\\n✅ Module Best Practices:\")\n",
    "print(\"  - Clear docstrings with examples\")\n",
    "print(\"  - Type hints for all parameters\")\n",
    "print(\"  - Version information (__version__)\")\n",
    "print(\"  - Pure functions only\")\n",
    "print(\"  - Comprehensive error handling\")\n",
    "print(\"  - Unit tests for each function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Python Wheel Packages\n",
    "\n",
    "Create distributable wheel packages for PySpark libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Building Python Wheel Packages ===\")\n",
    "\n",
    "# Project structure for wheel package\n",
    "package_structure = \"\"\"\n",
    "pyspark_transformations/\n",
    "│\n",
    "├── setup.py                      # Package configuration\n",
    "├── README.md                     # Package documentation\n",
    "├── LICENSE                       # License file\n",
    "├── requirements.txt              # Runtime dependencies\n",
    "├── requirements-dev.txt          # Development dependencies\n",
    "│\n",
    "├── pyspark_transformations/      # Package directory\n",
    "│   ├── __init__.py              # Package initialization\n",
    "│   ├── __version__.py           # Version information\n",
    "│   ├── cleaning.py              # Data cleaning module\n",
    "│   ├── business_logic.py        # Business transformation module\n",
    "│   ├── analytics.py             # Analytics module\n",
    "│   └── utils.py                 # Utility functions\n",
    "│\n",
    "└── tests/                        # Test directory\n",
    "    ├── __init__.py\n",
    "    ├── test_cleaning.py\n",
    "    ├── test_business_logic.py\n",
    "    └── test_analytics.py\n",
    "\"\"\"\n",
    "\n",
    "print(\"Package structure:\")\n",
    "print(package_structure)\n",
    "\n",
    "# setup.py example\n",
    "setup_py_example = '''\n",
    "# setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "# Read version from __version__.py\n",
    "version = {}\n",
    "with open(\"pyspark_transformations/__version__.py\") as f:\n",
    "    exec(f.read(), version)\n",
    "\n",
    "# Read dependencies from requirements.txt\n",
    "with open(\"requirements.txt\") as f:\n",
    "    requirements = f.read().splitlines()\n",
    "\n",
    "# Read long description from README\n",
    "with open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    long_description = f.read()\n",
    "\n",
    "setup(\n",
    "    name=\"pyspark-transformations\",\n",
    "    version=version[\"__version__\"],\n",
    "    author=\"Your Team\",\n",
    "    author_email=\"team@example.com\",\n",
    "    description=\"Reusable PySpark transformation library\",\n",
    "    long_description=long_description,\n",
    "    long_description_content_type=\"text/markdown\",\n",
    "    url=\"https://github.com/yourorg/pyspark-transformations\",\n",
    "    packages=find_packages(exclude=[\"tests\", \"tests.*\"]),\n",
    "    install_requires=requirements,\n",
    "    python_requires=\">=3.8\",\n",
    "    classifiers=[\n",
    "        \"Development Status :: 4 - Beta\",\n",
    "        \"Intended Audience :: Developers\",\n",
    "        \"Topic :: Software Development :: Libraries\",\n",
    "        \"License :: OSI Approved :: MIT License\",\n",
    "        \"Programming Language :: Python :: 3\",\n",
    "        \"Programming Language :: Python :: 3.8\",\n",
    "        \"Programming Language :: Python :: 3.9\",\n",
    "        \"Programming Language :: Python :: 3.10\",\n",
    "    ],\n",
    "    keywords=\"pyspark spark data-engineering etl\",\n",
    "    project_urls={\n",
    "        \"Bug Reports\": \"https://github.com/yourorg/pyspark-transformations/issues\",\n",
    "        \"Source\": \"https://github.com/yourorg/pyspark-transformations\",\n",
    "    },\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"\\nsetup.py example:\")\n",
    "print(setup_py_example)\n",
    "\n",
    "# Build commands\n",
    "build_commands = \"\"\"\n",
    "# Building wheel package\n",
    "\n",
    "# 1. Install build tools\n",
    "pip install build wheel setuptools\n",
    "\n",
    "# 2. Build the wheel\n",
    "python -m build\n",
    "\n",
    "# This creates:\n",
    "# dist/pyspark_transformations-1.0.0-py3-none-any.whl\n",
    "# dist/pyspark_transformations-1.0.0.tar.gz\n",
    "\n",
    "# 3. Install locally for testing\n",
    "pip install dist/pyspark_transformations-1.0.0-py3-none-any.whl\n",
    "\n",
    "# 4. Upload to Databricks (Unity Catalog volume)\n",
    "databricks fs cp dist/pyspark_transformations-1.0.0-py3-none-any.whl \\\n",
    "  dbfs:/Volumes/main/libraries/python/\n",
    "\n",
    "# 5. Install in Databricks notebook\n",
    "%pip install /Volumes/main/libraries/python/pyspark_transformations-1.0.0-py3-none-any.whl\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nBuild and deployment commands:\")\n",
    "print(build_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Versioning and Pinning\n",
    "\n",
    "Proper version management ensures reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Dependency Versioning Strategies ===\")\n",
    "\n",
    "requirements_examples = \"\"\"\n",
    "# requirements.txt - Different versioning strategies\n",
    "\n",
    "# ✅ BEST: Exact version pinning (most reproducible)\n",
    "pyspark==3.4.1\n",
    "pandas==2.0.3\n",
    "pyarrow==12.0.1\n",
    "\n",
    "# ✅ GOOD: Compatible version (allows patch updates)\n",
    "requests~=2.31.0    # Allows 2.31.x but not 2.32.0\n",
    "boto3~=1.28.0\n",
    "\n",
    "# ⚠️  ACCEPTABLE: Minimum version (less safe)\n",
    "numpy>=1.24.0\n",
    "scikit-learn>=1.3.0\n",
    "\n",
    "# ❌ AVOID: No version constraint (not reproducible)\n",
    "# pandas\n",
    "# requests\n",
    "\n",
    "# ✅ GOOD: Version range (balanced approach)\n",
    "matplotlib>=3.7.0,<4.0.0\n",
    "\n",
    "# ✅ GOOD: Exclude problematic versions\n",
    "sqlalchemy>=1.4.0,!=1.4.42,<2.0.0\n",
    "\n",
    "# Development dependencies (requirements-dev.txt)\n",
    "pytest==7.4.0\n",
    "pytest-cov==4.1.0\n",
    "black==23.7.0\n",
    "ruff==0.0.280\n",
    "\"\"\"\n",
    "\n",
    "print(requirements_examples)\n",
    "\n",
    "print(\"\\n✅ Versioning Best Practices:\")\n",
    "print(\"\\n1. Production Dependencies:\")\n",
    "print(\"   - Use exact pinning (==) for critical dependencies\")\n",
    "print(\"   - Document why each version is chosen\")\n",
    "print(\"   - Test before upgrading versions\")\n",
    "print(\"   - Keep security patches current\")\n",
    "\n",
    "print(\"\\n2. Development Dependencies:\")\n",
    "print(\"   - Separate requirements-dev.txt file\")\n",
    "print(\"   - Pin testing framework versions\")\n",
    "print(\"   - Update tooling regularly\")\n",
    "\n",
    "print(\"\\n3. Version Updates:\")\n",
    "print(\"   - Use dependabot or similar tools\")\n",
    "print(\"   - Review changelogs before upgrading\")\n",
    "print(\"   - Test in dev/staging before production\")\n",
    "print(\"   - Document breaking changes\")\n",
    "\n",
    "print(\"\\n4. Conflict Resolution:\")\n",
    "print(\"   - Use pip-compile for dependency resolution\")\n",
    "print(\"   - Document conflicts and resolutions\")\n",
    "print(\"   - Consider using virtual environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Requirements Files in Databricks\n",
    "\n",
    "Demonstrate requirements file management in notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Requirements File Management ===\")\n",
    "\n",
    "# Create example requirements file\n",
    "requirements_content = \"\"\"# Production dependencies for revenue analytics pipeline\n",
    "# Updated: 2024-01-15\n",
    "\n",
    "# Core dependencies\n",
    "pyspark==3.4.1\n",
    "pandas==2.0.3\n",
    "pyarrow==12.0.1\n",
    "\n",
    "# Data validation\n",
    "great-expectations==0.17.23\n",
    "pydantic==2.4.0\n",
    "\n",
    "# Utilities\n",
    "python-dateutil==2.8.2\n",
    "pyyaml==6.0.1\n",
    "\n",
    "# AWS integration (if needed)\n",
    "boto3==1.28.57\n",
    "\"\"\"\n",
    "\n",
    "# Write to temporary file (for demonstration)\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "    f.write(requirements_content)\n",
    "    temp_req_file = f.name\n",
    "\n",
    "print(\"Example requirements.txt:\")\n",
    "print(requirements_content)\n",
    "\n",
    "# In Databricks, you would install with:\n",
    "# %pip install -r /Workspace/path/to/requirements.txt\n",
    "# or\n",
    "# %pip install -r /Volumes/catalog/schema/volume/requirements.txt\n",
    "\n",
    "print(\"\\n✅ Installation in Databricks:\")\n",
    "print(\"  # From workspace file:\")\n",
    "print(\"  %pip install -r /Workspace/Repos/my-project/requirements.txt\")\n",
    "print(\"\")\n",
    "print(\"  # From Unity Catalog volume (recommended):\")\n",
    "print(\"  %pip install -r /Volumes/main/config/dependencies/requirements.txt\")\n",
    "print(\"\")\n",
    "print(\"  # From local file (for testing):\")\n",
    "print(\"  %pip install -r requirements.txt\")\n",
    "\n",
    "# Clean up\n",
    "os.unlink(temp_req_file)\n",
    "\n",
    "print(\"\\n✅ Benefits of requirements files:\")\n",
    "print(\"  - Version control for dependencies\")\n",
    "print(\"  - Reproducible environments\")\n",
    "print(\"  - Easy collaboration\")\n",
    "print(\"  - Simplified deployment\")\n",
    "print(\"  - Documentation of dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Using Custom Modules\n",
    "\n",
    "Demonstrate importing custom PySpark modules in notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Importing Custom Modules ===\")\n",
    "\n",
    "import_examples = '''\n",
    "# ===================================================\n",
    "# METHOD 1: Import from workspace file\n",
    "# ===================================================\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Workspace/Repos/my-project/src')\n",
    "\n",
    "from pyspark_transformations import cleaning\n",
    "from pyspark_transformations import business_logic\n",
    "\n",
    "# Use the imported functions\n",
    "clean_df = cleaning.remove_nulls(raw_df, [\"id\", \"amount\"])\n",
    "result_df = business_logic.calculate_revenue(clean_df)\n",
    "\n",
    "# ===================================================\n",
    "# METHOD 2: Import from installed wheel package\n",
    "# ===================================================\n",
    "\n",
    "# First install the package\n",
    "%pip install /Volumes/main/libraries/python/pyspark_transformations-1.0.0-py3-none-any.whl\n",
    "\n",
    "# Then import normally\n",
    "from pyspark_transformations import cleaning\n",
    "from pyspark_transformations import business_logic\n",
    "from pyspark_transformations import analytics\n",
    "\n",
    "# ===================================================\n",
    "# METHOD 3: Import from Databricks Repos\n",
    "# ===================================================\n",
    "\n",
    "# Add repo path to Python path\n",
    "import sys\n",
    "sys.path.append('/Workspace/Repos/user@example.com/my-pyspark-project/src')\n",
    "\n",
    "# Import modules\n",
    "from extraction import readers\n",
    "from transformation import cleaning, business_logic\n",
    "from loading import writers\n",
    "\n",
    "# ===================================================\n",
    "# METHOD 4: Use %run for notebook modules\n",
    "# ===================================================\n",
    "\n",
    "# Run another notebook to import its functions\n",
    "%run ./modules/cleaning_functions\n",
    "%run ./modules/business_logic_functions\n",
    "\n",
    "# Now use functions defined in those notebooks\n",
    "clean_df = clean_transactions(raw_df)\n",
    "result_df = calculate_metrics(clean_df)\n",
    "\n",
    "# ===================================================\n",
    "# BEST PRACTICE: Package initialization\n",
    "# ===================================================\n",
    "\n",
    "# __init__.py in package root\n",
    "from .cleaning import remove_nulls, standardize_column_names\n",
    "from .business_logic import calculate_tax, calculate_revenue\n",
    "from .analytics import revenue_by_category\n",
    "from .__version__ import __version__\n",
    "\n",
    "__all__ = [\n",
    "    \"remove_nulls\",\n",
    "    \"standardize_column_names\",\n",
    "    \"calculate_tax\",\n",
    "    \"calculate_revenue\",\n",
    "    \"revenue_by_category\",\n",
    "    \"__version__\",\n",
    "]\n",
    "\n",
    "# Then import is simpler:\n",
    "from pyspark_transformations import remove_nulls, calculate_revenue\n",
    "'''\n",
    "\n",
    "print(import_examples)\n",
    "\n",
    "print(\"\\n✅ Import Best Practices:\")\n",
    "print(\"  - Use wheel packages for production code\")\n",
    "print(\"  - Keep import paths consistent across team\")\n",
    "print(\"  - Use __init__.py for clean imports\")\n",
    "print(\"  - Document import methods in README\")\n",
    "print(\"  - Avoid circular imports\")\n",
    "print(\"  - Use relative imports within packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Governance and Security Considerations\n",
    "\n",
    "Best practices for secure dependency management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Governance and Security ===\")\n",
    "\n",
    "governance_guidelines = \"\"\"\n",
    "1. LIBRARY APPROVAL PROCESS\n",
    "   \n",
    "   ✅ Before using external libraries:\n",
    "   - Security scan for known vulnerabilities\n",
    "   - License compatibility check\n",
    "   - Maintenance status verification\n",
    "   - Performance impact assessment\n",
    "   - Team approval for production use\n",
    "   \n",
    "   Tools:\n",
    "   - safety check (vulnerability scanning)\n",
    "   - pip-audit (dependency auditing)\n",
    "   - GitHub Dependabot (automated alerts)\n",
    "\n",
    "2. UNITY CATALOG VOLUMES (Recommended)\n",
    "   \n",
    "   ✅ Advantages:\n",
    "   - Centralized library storage\n",
    "   - Access control and auditing\n",
    "   - Version management\n",
    "   - Compliance tracking\n",
    "   - Team collaboration\n",
    "   \n",
    "   Setup:\n",
    "   CREATE VOLUME main.libraries.python;\n",
    "   \n",
    "   Grant access:\n",
    "   GRANT READ FILES ON VOLUME main.libraries.python TO data_engineers;\n",
    "\n",
    "3. SECURITY SCANNING\n",
    "   \n",
    "   Regular vulnerability checks:\n",
    "   \n",
    "   # Scan dependencies for vulnerabilities\n",
    "   pip install safety\n",
    "   safety check -r requirements.txt\n",
    "   \n",
    "   # Audit with pip-audit\n",
    "   pip install pip-audit\n",
    "   pip-audit -r requirements.txt\n",
    "\n",
    "4. LICENSE COMPLIANCE\n",
    "   \n",
    "   Track dependency licenses:\n",
    "   \n",
    "   # List licenses\n",
    "   pip install pip-licenses\n",
    "   pip-licenses\n",
    "   \n",
    "   Approved licenses:\n",
    "   - MIT, Apache 2.0, BSD (permissive)\n",
    "   \n",
    "   Avoid:\n",
    "   - GPL (copyleft, may require source disclosure)\n",
    "   - Unknown/custom licenses\n",
    "\n",
    "5. PRIVATE PACKAGE REPOSITORIES\n",
    "   \n",
    "   For proprietary libraries:\n",
    "   \n",
    "   # Install from private PyPI\n",
    "   %pip install --extra-index-url https://your-pypi.example.com/simple/ \\\n",
    "        your-private-package==1.0.0\n",
    "   \n",
    "   # Or use Unity Catalog volumes\n",
    "   %pip install /Volumes/main/libraries/python/your_package-1.0.0-py3-none-any.whl\n",
    "\n",
    "6. ENVIRONMENT ISOLATION\n",
    "   \n",
    "   Development vs Production:\n",
    "   \n",
    "   - Development: Notebook-scoped installations for experimentation\n",
    "   - Staging: Cluster libraries matching production\n",
    "   - Production: Locked versions, approved libraries only\n",
    "\n",
    "7. DEPENDENCY MONITORING\n",
    "   \n",
    "   Continuous monitoring:\n",
    "   \n",
    "   - GitHub Dependabot alerts\n",
    "   - Snyk vulnerability scanning\n",
    "   - Regular dependency updates\n",
    "   - Security advisory subscriptions\n",
    "\"\"\"\n",
    "\n",
    "print(governance_guidelines)\n",
    "\n",
    "print(\"\\n✅ Security Checklist:\")\n",
    "print(\"  □ All dependencies scanned for vulnerabilities\")\n",
    "print(\"  □ Licenses reviewed and approved\")\n",
    "print(\"  □ Versions pinned in requirements.txt\")\n",
    "print(\"  □ Unity Catalog volumes used for production libraries\")\n",
    "print(\"  □ Access controls configured\")\n",
    "print(\"  □ Monitoring and alerting enabled\")\n",
    "print(\"  □ Update process documented\")\n",
    "print(\"  □ Rollback plan in place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example: Building and Deploying a Package\n",
    "\n",
    "End-to-end workflow for creating and deploying a PySpark package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Complete Package Deployment Workflow ===\")\n",
    "\n",
    "workflow = \"\"\"\n",
    "STEP 1: PROJECT SETUP\n",
    "=====================\n",
    "\n",
    "# Create project structure\n",
    "mkdir -p pyspark_revenue_analytics/pyspark_revenue_analytics\n",
    "mkdir -p pyspark_revenue_analytics/tests\n",
    "cd pyspark_revenue_analytics\n",
    "\n",
    "# Initialize git\n",
    "git init\n",
    "git remote add origin https://github.com/yourorg/pyspark-revenue-analytics.git\n",
    "\n",
    "# Create virtual environment\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\\\Scripts\\\\activate\n",
    "\n",
    "\n",
    "STEP 2: DEVELOP MODULES\n",
    "=======================\n",
    "\n",
    "# Create module files\n",
    "# pyspark_revenue_analytics/__init__.py\n",
    "# pyspark_revenue_analytics/cleaning.py\n",
    "# pyspark_revenue_analytics/business_logic.py\n",
    "# pyspark_revenue_analytics/analytics.py\n",
    "\n",
    "# Write unit tests\n",
    "# tests/test_cleaning.py\n",
    "# tests/test_business_logic.py\n",
    "\n",
    "\n",
    "STEP 3: CREATE PACKAGE CONFIGURATION\n",
    "====================================\n",
    "\n",
    "# Create setup.py (shown earlier)\n",
    "# Create __version__.py\n",
    "echo '__version__ = \"1.0.0\"' > pyspark_revenue_analytics/__version__.py\n",
    "\n",
    "# Create requirements.txt\n",
    "cat > requirements.txt << EOF\n",
    "pyspark>=3.4.0\n",
    "pandas>=2.0.0\n",
    "pyarrow>=12.0.0\n",
    "EOF\n",
    "\n",
    "# Create requirements-dev.txt\n",
    "cat > requirements-dev.txt << EOF\n",
    "pytest>=7.4.0\n",
    "pytest-cov>=4.1.0\n",
    "black>=23.7.0\n",
    "ruff>=0.0.280\n",
    "EOF\n",
    "\n",
    "\n",
    "STEP 4: TEST LOCALLY\n",
    "====================\n",
    "\n",
    "# Install dev dependencies\n",
    "pip install -r requirements-dev.txt\n",
    "\n",
    "# Run tests\n",
    "pytest tests/ -v --cov=pyspark_revenue_analytics\n",
    "\n",
    "# Format code\n",
    "black pyspark_revenue_analytics/\n",
    "\n",
    "# Lint code\n",
    "ruff check pyspark_revenue_analytics/\n",
    "\n",
    "\n",
    "STEP 5: BUILD PACKAGE\n",
    "=====================\n",
    "\n",
    "# Install build tools\n",
    "pip install build wheel setuptools\n",
    "\n",
    "# Build wheel package\n",
    "python -m build\n",
    "\n",
    "# Output:\n",
    "# dist/pyspark_revenue_analytics-1.0.0-py3-none-any.whl\n",
    "# dist/pyspark_revenue_analytics-1.0.0.tar.gz\n",
    "\n",
    "# Test installation\n",
    "pip install dist/pyspark_revenue_analytics-1.0.0-py3-none-any.whl\n",
    "\n",
    "\n",
    "STEP 6: DEPLOY TO DATABRICKS\n",
    "============================\n",
    "\n",
    "# Option A: Upload to Unity Catalog volume (RECOMMENDED)\n",
    "databricks fs cp dist/pyspark_revenue_analytics-1.0.0-py3-none-any.whl \\\n",
    "  dbfs:/Volumes/main/libraries/python/\n",
    "\n",
    "# Grant access\n",
    "# In SQL:\n",
    "# GRANT READ FILES ON VOLUME main.libraries.python TO data_engineers;\n",
    "\n",
    "# Option B: Upload to workspace\n",
    "databricks workspace import dist/pyspark_revenue_analytics-1.0.0-py3-none-any.whl \\\n",
    "  /Workspace/Shared/libraries/\n",
    "\n",
    "\n",
    "STEP 7: INSTALL IN NOTEBOOKS\n",
    "============================\n",
    "\n",
    "# In Databricks notebook:\n",
    "%pip install /Volumes/main/libraries/python/pyspark_revenue_analytics-1.0.0-py3-none-any.whl\n",
    "\n",
    "# Import and use\n",
    "from pyspark_revenue_analytics import cleaning, business_logic, analytics\n",
    "\n",
    "clean_df = cleaning.remove_nulls(raw_df, [\"id\"])\n",
    "revenue_df = business_logic.calculate_revenue(clean_df)\n",
    "summary_df = analytics.revenue_by_category(revenue_df)\n",
    "\n",
    "\n",
    "STEP 8: VERSION UPDATES\n",
    "=======================\n",
    "\n",
    "# Update version\n",
    "echo '__version__ = \"1.1.0\"' > pyspark_revenue_analytics/__version__.py\n",
    "\n",
    "# Rebuild and redeploy\n",
    "python -m build\n",
    "databricks fs cp dist/pyspark_revenue_analytics-1.1.0-py3-none-any.whl \\\n",
    "  dbfs:/Volumes/main/libraries/python/\n",
    "\n",
    "# Update installations\n",
    "%pip install --upgrade /Volumes/main/libraries/python/pyspark_revenue_analytics-1.1.0-py3-none-any.whl\n",
    "\n",
    "\n",
    "STEP 9: CI/CD AUTOMATION\n",
    "========================\n",
    "\n",
    "# .github/workflows/build-and-deploy.yml\n",
    "name: Build and Deploy\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    tags:\n",
    "      - 'v*'\n",
    "\n",
    "jobs:\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install build wheel\n",
    "      - name: Run tests\n",
    "        run: |\n",
    "          pip install -r requirements-dev.txt\n",
    "          pytest\n",
    "      - name: Build package\n",
    "        run: python -m build\n",
    "      - name: Deploy to Databricks\n",
    "        run: |\n",
    "          databricks fs cp dist/*.whl dbfs:/Volumes/main/libraries/python/\n",
    "        env:\n",
    "          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}\n",
    "          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n",
    "\"\"\"\n",
    "\n",
    "print(workflow)\n",
    "\n",
    "print(\"\\n✅ Deployment Best Practices:\")\n",
    "print(\"  - Use semantic versioning (MAJOR.MINOR.PATCH)\")\n",
    "print(\"  - Tag releases in git\")\n",
    "print(\"  - Maintain CHANGELOG.md\")\n",
    "print(\"  - Test in development environment first\")\n",
    "print(\"  - Use CI/CD for automated deployment\")\n",
    "print(\"  - Keep multiple versions for rollback\")\n",
    "print(\"  - Document upgrade paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Dependency Management Strategies**:\n",
    "   - Cluster-scoped for shared dependencies\n",
    "   - Notebook-scoped for experimentation\n",
    "   - Unity Catalog volumes for production\n",
    "   - Requirements files for reproducibility\n",
    "\n",
    "2. **Package Development**:\n",
    "   - Modular structure with setup.py\n",
    "   - Wheel packages for distribution\n",
    "   - Comprehensive testing\n",
    "   - Clear documentation\n",
    "\n",
    "3. **Version Control**:\n",
    "   - Exact pinning for production\n",
    "   - Semantic versioning\n",
    "   - Requirements files in version control\n",
    "   - Regular security updates\n",
    "\n",
    "4. **Security and Governance**:\n",
    "   - Vulnerability scanning\n",
    "   - License compliance\n",
    "   - Access controls\n",
    "   - Monitoring and auditing\n",
    "\n",
    "5. **Deployment Workflow**:\n",
    "   - Local development and testing\n",
    "   - Package building\n",
    "   - Upload to Unity Catalog\n",
    "   - Installation in notebooks\n",
    "   - CI/CD automation\n",
    "\n",
    "**Best Practices for Dependency Management**:\n",
    "- Use Unity Catalog volumes for production libraries\n",
    "- Pin dependency versions for reproducibility\n",
    "- Scan for vulnerabilities regularly\n",
    "- Document dependencies and their purpose\n",
    "- Automate package building and deployment\n",
    "- Maintain separate dev and production environments\n",
    "- Test upgrades thoroughly before production\n",
    "\n",
    "**Functional Programming Benefits**:\n",
    "- Pure function libraries are easily packaged and distributed\n",
    "- Versioned packages ensure deterministic transformations\n",
    "- Modular design enables reusable components\n",
    "- Proper dependencies make functional code portable across projects\n",
    "\n",
    "This completes the PySpark Best Practices curriculum. You now have the knowledge to build production-grade, functional PySpark applications with proper modular design and dependency management!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Practice dependency management and packaging:\n",
    "\n",
    "1. Create a requirements.txt with pinned versions for a sample project\n",
    "2. Build a simple Python package with setup.py\n",
    "3. Create a wheel package using python -m build\n",
    "4. Write installation instructions for Databricks\n",
    "5. Plan a CI/CD workflow for automated package deployment\n",
    "6. Document security scanning procedures\n",
    "7. Create a version update strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "\n",
    "# 1. Create requirements.txt content\n",
    "your_requirements = \"\"\"\n",
    "# Your project dependencies\n",
    "\"\"\"\n",
    "\n",
    "# 2. Design setup.py structure\n",
    "your_setup_py = \"\"\"\n",
    "# Your setup.py configuration\n",
    "\"\"\"\n",
    "\n",
    "# 3. Plan deployment workflow\n",
    "your_deployment_plan = \"\"\"\n",
    "# Your step-by-step deployment process\n",
    "\"\"\"\n",
    "\n",
    "# 4. Document security procedures\n",
    "your_security_checklist = \"\"\"\n",
    "# Your security scanning and approval process\n",
    "\"\"\"\n",
    "\n",
    "# 5. Design CI/CD workflow\n",
    "your_cicd_workflow = \"\"\"\n",
    "# Your automated build and deployment workflow\n",
    "\"\"\"\n",
    "\n",
    "# Print your plans\n",
    "# print(your_requirements)\n",
    "# print(your_setup_py)\n",
    "# print(your_deployment_plan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
