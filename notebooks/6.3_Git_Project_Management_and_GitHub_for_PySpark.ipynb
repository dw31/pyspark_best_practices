{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Git Project Management and GitHub for PySpark Development\n",
    "\n",
    "This notebook demonstrates best practices for using Git and GitHub in PySpark data engineering projects, with special focus on Databricks Repos integration.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master Git workflows for data engineering teams\n",
    "- Integrate Databricks Repos with Git repositories\n",
    "- Implement effective branching strategies\n",
    "- Conduct code reviews for PySpark code\n",
    "- Set up CI/CD pipelines for data pipelines\n",
    "- Manage notebooks and Python modules in version control\n",
    "- Collaborate effectively on PySpark projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local development: Uncomment the next line\n",
    "# %run 00_Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Git and GitHub Matter for Data Engineering\n",
    "\n",
    "**Version control is critical** for data engineering:\n",
    "\n",
    "**Benefits**:\n",
    "1. **Collaboration**: Multiple data engineers working on same codebase\n",
    "2. **History**: Track changes to pipelines over time\n",
    "3. **Rollback**: Revert to working versions when issues arise\n",
    "4. **Code Review**: Peer review of data transformation logic\n",
    "5. **CI/CD**: Automated testing and deployment\n",
    "6. **Documentation**: Commit messages document evolution\n",
    "\n",
    "**Challenges in Data Engineering**:\n",
    "- Large notebooks can be difficult to diff\n",
    "- Binary outputs in notebooks create merge conflicts\n",
    "- Data pipelines have complex dependencies\n",
    "- Testing requires data and compute resources\n",
    "\n",
    "**Solution**: Databricks Repos + GitHub + proper Git workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Repos Integration\n",
    "\n",
    "Databricks Repos provides native Git integration within the Databricks workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Databricks Repos Integration ===\")\n",
    "\n",
    "repos_overview = \"\"\"\n",
    "DATABRICKS REPOS FEATURES\n",
    "=========================\n",
    "\n",
    "What is Databricks Repos?\n",
    "- Git repository integration directly in Databricks workspace\n",
    "- Supports GitHub, GitLab, Bitbucket, Azure DevOps\n",
    "- Visual Git operations (commit, push, pull, branch, merge)\n",
    "- Notebook and Python file support\n",
    "- Collaborative development environment\n",
    "\n",
    "Setting Up a Repo:\n",
    "1. Navigate to Repos in Databricks workspace\n",
    "2. Click \"Add Repo\"\n",
    "3. Connect to Git provider (GitHub, GitLab, etc.)\n",
    "4. Clone repository or create new\n",
    "5. Start developing with full Git integration\n",
    "\n",
    "Supported Operations:\n",
    "- Clone: Import existing repository\n",
    "- Commit: Save changes with commit message\n",
    "- Push: Upload changes to remote\n",
    "- Pull: Download changes from remote\n",
    "- Branch: Create feature branches\n",
    "- Merge: Integrate branches\n",
    "- Conflict Resolution: Visual diff and merge tools\n",
    "\n",
    "File Types Supported:\n",
    "- Notebooks (.py, .sql, .scala, .r)\n",
    "- Python modules (.py)\n",
    "- Configuration files (.yaml, .json, .txt)\n",
    "- Documentation (.md)\n",
    "- Any text file\n",
    "\n",
    "Best Practices:\n",
    "- One repo per project or pipeline group\n",
    "- Separate notebooks from Python modules\n",
    "- Use .gitignore for generated files\n",
    "- Clear README.md for onboarding\n",
    "- Structured directory layout\n",
    "\"\"\"\n",
    "\n",
    "print(repos_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git Branching Strategies\n",
    "\n",
    "Different branching strategies for different team sizes and workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Git Branching Strategies ===\")\n",
    "\n",
    "branching_strategies = \"\"\"\n",
    "1. GITHUB FLOW (Recommended for Most Teams)\n",
    "============================================\n",
    "\n",
    "Structure:\n",
    "  main (production)\n",
    "    |\n",
    "    +-- feature/add-customer-segmentation\n",
    "    +-- feature/fix-revenue-calculation\n",
    "    +-- hotfix/data-quality-issue\n",
    "\n",
    "Workflow:\n",
    "1. Branch from main for every change\n",
    "2. Name branches descriptively (feature/*, bugfix/*, hotfix/*)\n",
    "3. Commit frequently with clear messages\n",
    "4. Open PR when ready for review\n",
    "5. Review, test, and merge to main\n",
    "6. Deploy main to production\n",
    "7. Delete feature branch after merge\n",
    "\n",
    "Pros:\n",
    "- Simple and easy to understand\n",
    "- Fast feedback loops\n",
    "- Continuous deployment friendly\n",
    "- Clear separation of work\n",
    "\n",
    "Cons:\n",
    "- Requires good CI/CD and testing\n",
    "- Main must always be deployable\n",
    "- Not ideal for multiple production versions\n",
    "\n",
    "Best For:\n",
    "- Small to medium teams\n",
    "- Continuous deployment\n",
    "- Databricks environments (dev/staging/prod clusters)\n",
    "\n",
    "\n",
    "2. GIT FLOW (For Complex Release Cycles)\n",
    "==========================================\n",
    "\n",
    "Structure:\n",
    "  main (production)\n",
    "    |\n",
    "  develop (integration)\n",
    "    |\n",
    "    +-- feature/feature-a\n",
    "    +-- feature/feature-b\n",
    "    |\n",
    "  release/v1.2.0 (release prep)\n",
    "    |\n",
    "  hotfix/critical-bug (emergency fixes)\n",
    "\n",
    "Workflow:\n",
    "1. Develop branch for ongoing work\n",
    "2. Feature branches from develop\n",
    "3. Merge features back to develop\n",
    "4. Create release branch from develop\n",
    "5. Test and fix bugs in release branch\n",
    "6. Merge release to main and tag version\n",
    "7. Merge release back to develop\n",
    "8. Hotfixes branch from main, merge to both\n",
    "\n",
    "Pros:\n",
    "- Clear separation of concerns\n",
    "- Supports multiple release versions\n",
    "- Organized release management\n",
    "- Good for scheduled releases\n",
    "\n",
    "Cons:\n",
    "- More complex workflow\n",
    "- Slower feedback loops\n",
    "- Overhead for small teams\n",
    "- Multiple merge points\n",
    "\n",
    "Best For:\n",
    "- Large teams\n",
    "- Scheduled release cycles\n",
    "- Multiple production versions\n",
    "- Strict change control\n",
    "\n",
    "\n",
    "3. TRUNK-BASED DEVELOPMENT (For Fast-Moving Teams)\n",
    "====================================================\n",
    "\n",
    "Structure:\n",
    "  main (always deployable)\n",
    "    |\n",
    "    +-- short-lived branches (< 1 day)\n",
    "\n",
    "Workflow:\n",
    "1. All work directly on main or very short-lived branches\n",
    "2. Commit multiple times per day\n",
    "3. Use feature flags for incomplete features\n",
    "4. Automated testing on every commit\n",
    "5. Continuous deployment to production\n",
    "\n",
    "Pros:\n",
    "- Simplest possible workflow\n",
    "- Fastest integration\n",
    "- Minimal merge conflicts\n",
    "- Forces small, incremental changes\n",
    "\n",
    "Cons:\n",
    "- Requires excellent CI/CD\n",
    "- Needs feature flags for large features\n",
    "- High discipline required\n",
    "- Not suitable for all team sizes\n",
    "\n",
    "Best For:\n",
    "- Elite performing teams\n",
    "- Mature CI/CD practices\n",
    "- Microservices architectures\n",
    "- High-trust environments\n",
    "\n",
    "\n",
    "RECOMMENDATION FOR DATA ENGINEERING\n",
    "===================================\n",
    "\n",
    "Use GitHub Flow with environment branches:\n",
    "\n",
    "  main (production)\n",
    "    |\n",
    "  staging (pre-production testing)\n",
    "    |\n",
    "  develop (integration testing)\n",
    "    |\n",
    "    +-- feature/* (individual features)\n",
    "    +-- bugfix/* (bug fixes)\n",
    "    +-- hotfix/* (emergency fixes)\n",
    "\n",
    "Workflow:\n",
    "1. Feature branch from develop\n",
    "2. Merge to develop, test with dev data\n",
    "3. Merge to staging, test with prod-like data\n",
    "4. Merge to main, deploy to production\n",
    "5. Tag releases for rollback capability\n",
    "\"\"\"\n",
    "\n",
    "print(branching_strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git Workflow Best Practices\n",
    "\n",
    "Step-by-step workflow for common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Git Workflow Commands ===\")\n",
    "\n",
    "git_workflow = \"\"\"\n",
    "STARTING A NEW FEATURE\n",
    "======================\n",
    "\n",
    "# 1. Ensure main is up to date\n",
    "git checkout main\n",
    "git pull origin main\n",
    "\n",
    "# 2. Create feature branch with descriptive name\n",
    "git checkout -b feature/add-revenue-analytics\n",
    "\n",
    "# Branch naming conventions:\n",
    "# - feature/short-description\n",
    "# - bugfix/issue-description\n",
    "# - hotfix/critical-fix-description\n",
    "# - experiment/hypothesis-name\n",
    "\n",
    "# 3. Make changes to your code\n",
    "# (Edit files in Databricks or locally)\n",
    "\n",
    "# 4. Check status of changes\n",
    "git status\n",
    "\n",
    "# 5. Review changes before committing\n",
    "git diff\n",
    "\n",
    "# 6. Stage files for commit\n",
    "git add notebooks/revenue_analytics.py\n",
    "git add src/transformations/revenue.py\n",
    "# Or stage all changes:\n",
    "git add .\n",
    "\n",
    "# 7. Commit with descriptive message\n",
    "git commit -m \"Add revenue analytics transformation\n",
    "\n",
    "- Implement revenue calculation by customer segment\n",
    "- Add data quality checks for revenue data\n",
    "- Include unit tests for transformation functions\"\n",
    "\n",
    "# 8. Push to remote repository\n",
    "git push origin feature/add-revenue-analytics\n",
    "\n",
    "# If first push on this branch:\n",
    "git push -u origin feature/add-revenue-analytics\n",
    "\n",
    "\n",
    "COMMITTING CHANGES\n",
    "==================\n",
    "\n",
    "# ✅ GOOD: Small, focused commits\n",
    "git commit -m \"Add customer segmentation logic\"\n",
    "git commit -m \"Add unit tests for segmentation\"\n",
    "git commit -m \"Update documentation for segmentation\"\n",
    "\n",
    "# ❌ BAD: Large, unfocused commits\n",
    "git commit -m \"Various changes\"\n",
    "git commit -m \"Fix stuff\"\n",
    "git commit -m \"WIP\"\n",
    "\n",
    "# Good commit message structure:\n",
    "git commit -m \"<type>: <short summary (50 chars max)>\n",
    "\n",
    "<optional detailed description>\n",
    "\n",
    "<optional references to issues/tickets>\"\n",
    "\n",
    "# Commit types:\n",
    "# - feat: New feature\n",
    "# - fix: Bug fix\n",
    "# - refactor: Code restructuring\n",
    "# - test: Adding tests\n",
    "# - docs: Documentation changes\n",
    "# - perf: Performance improvements\n",
    "# - style: Formatting changes\n",
    "\n",
    "\n",
    "UPDATING YOUR BRANCH\n",
    "====================\n",
    "\n",
    "# Keep feature branch up to date with main\n",
    "git checkout feature/add-revenue-analytics\n",
    "git fetch origin\n",
    "git rebase origin/main\n",
    "# Or use merge (creates merge commit):\n",
    "git merge origin/main\n",
    "\n",
    "# Resolve conflicts if any\n",
    "# 1. Edit conflicted files\n",
    "# 2. Mark as resolved:\n",
    "git add <conflicted-file>\n",
    "# 3. Continue rebase:\n",
    "git rebase --continue\n",
    "# Or for merge:\n",
    "git commit\n",
    "\n",
    "# Push updated branch (may need force push after rebase)\n",
    "git push --force-with-lease origin feature/add-revenue-analytics\n",
    "\n",
    "\n",
    "CREATING A PULL REQUEST\n",
    "=======================\n",
    "\n",
    "# 1. Push your branch to remote (if not already)\n",
    "git push origin feature/add-revenue-analytics\n",
    "\n",
    "# 2. Go to GitHub repository\n",
    "# 3. Click \"Pull requests\" → \"New pull request\"\n",
    "# 4. Select base: main, compare: feature/add-revenue-analytics\n",
    "# 5. Fill in PR template:\n",
    "\n",
    "\"\"\"\n",
    "PR Title: Add revenue analytics transformation\n",
    "\n",
    "## Summary\n",
    "This PR adds a new revenue analytics transformation pipeline that calculates \n",
    "revenue by customer segment.\n",
    "\n",
    "## Changes\n",
    "- New transformation: `calculate_revenue_by_segment()`\n",
    "- Data quality checks for revenue data\n",
    "- Unit tests with 90% coverage\n",
    "- Documentation updates\n",
    "\n",
    "## Testing\n",
    "- [x] Unit tests pass\n",
    "- [x] Integration tests pass\n",
    "- [x] Manual testing on dev cluster\n",
    "- [x] Data quality checks verified\n",
    "\n",
    "## Checklist\n",
    "- [x] Code follows project style guidelines\n",
    "- [x] Tests added for new functionality\n",
    "- [x] Documentation updated\n",
    "- [x] No breaking changes\n",
    "\n",
    "## Related Issues\n",
    "Closes #123\n",
    "\"\"\"\n",
    "\n",
    "# 6. Request reviewers\n",
    "# 7. Address review comments\n",
    "# 8. Merge when approved\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(git_workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .gitignore for PySpark Projects\n",
    "\n",
    "Essential files to exclude from version control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== .gitignore for PySpark Projects ===\")\n",
    "\n",
    "gitignore_content = \"\"\"\n",
    "# .gitignore for PySpark/Databricks projects\n",
    "\n",
    "# ========================================\n",
    "# Python\n",
    "# ========================================\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "build/\n",
    "develop-eggs/\n",
    "dist/\n",
    "downloads/\n",
    "eggs/\n",
    ".eggs/\n",
    "lib/\n",
    "lib64/\n",
    "parts/\n",
    "sdist/\n",
    "var/\n",
    "wheels/\n",
    "*.egg-info/\n",
    ".installed.cfg\n",
    "*.egg\n",
    "MANIFEST\n",
    "\n",
    "# ========================================\n",
    "# Virtual Environments\n",
    "# ========================================\n",
    "venv/\n",
    "ENV/\n",
    "env/\n",
    ".venv/\n",
    "pipenv/\n",
    "\n",
    "# ========================================\n",
    "# Jupyter Notebooks\n",
    "# ========================================\n",
    ".ipynb_checkpoints/\n",
    "*-checkpoint.ipynb\n",
    "*.ipynb_checkpoints\n",
    "\n",
    "# Notebook outputs (optional - team decision)\n",
    "# Uncomment to exclude all outputs:\n",
    "# *.ipynb\n",
    "\n",
    "# ========================================\n",
    "# Spark and PySpark\n",
    "# ========================================\n",
    "spark-warehouse/\n",
    "metastore_db/\n",
    "derby.log\n",
    "*.log\n",
    "*.parquet\n",
    "*.orc\n",
    "*.avro\n",
    "\n",
    "# Local Spark data\n",
    "data/raw/\n",
    "data/processed/\n",
    "data/temp/\n",
    "tmp/\n",
    "output/\n",
    "\n",
    "# ========================================\n",
    "# Delta Lake\n",
    "# ========================================\n",
    "_delta_log/\n",
    "*.checkpoint.parquet\n",
    "\n",
    "# ========================================\n",
    "# Databricks\n",
    "# ========================================\n",
    ".databricks/\n",
    ".databrickscfg\n",
    "databricks_cli.log\n",
    "\n",
    "# ========================================\n",
    "# IDE and Editor\n",
    "# ========================================\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "*~\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# ========================================\n",
    "# Testing\n",
    "# ========================================\n",
    ".pytest_cache/\n",
    ".coverage\n",
    "htmlcov/\n",
    ".tox/\n",
    "coverage.xml\n",
    "*.cover\n",
    ".hypothesis/\n",
    "\n",
    "# ========================================\n",
    "# Secrets and Configuration\n",
    "# ========================================\n",
    "*.env\n",
    ".env.*\n",
    "secrets.yaml\n",
    "secrets.json\n",
    "credentials.json\n",
    "*.key\n",
    "*.pem\n",
    "*.p12\n",
    "\n",
    "# Keep example configs\n",
    "!.env.example\n",
    "!secrets.yaml.example\n",
    "\n",
    "# ========================================\n",
    "# Documentation\n",
    "# ========================================\n",
    "docs/_build/\n",
    "site/\n",
    "\n",
    "# ========================================\n",
    "# Project-Specific\n",
    "# ========================================\n",
    "# Add any project-specific files to ignore\n",
    "# local_data/\n",
    "# experiments/\n",
    "\"\"\"\n",
    "\n",
    "print(gitignore_content)\n",
    "\n",
    "print(\"\\n✅ Best practices for .gitignore:\")\n",
    "print(\"  - Exclude generated files (outputs, logs)\")\n",
    "print(\"  - Never commit secrets or credentials\")\n",
    "print(\"  - Exclude large data files (use .gitattributes for LFS)\")\n",
    "print(\"  - Keep example config files with .example extension\")\n",
    "print(\"  - Team decision on notebook outputs\")\n",
    "print(\"  - Document why files are ignored (comments)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Review Best Practices\n",
    "\n",
    "Effective code review for PySpark projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Code Review Best Practices ===\")\n",
    "\n",
    "code_review_guide = \"\"\"\n",
    "CODE REVIEW CHECKLIST FOR PYSPARK\n",
    "==================================\n",
    "\n",
    "Functional Correctness\n",
    "----------------------\n",
    "□ Does the code solve the stated problem?\n",
    "□ Are edge cases handled?\n",
    "□ Are transformations idempotent (safe to retry)?\n",
    "□ Are pure functions truly pure (no side effects)?\n",
    "□ Is business logic correct and validated?\n",
    "\n",
    "Data Quality\n",
    "------------\n",
    "□ Are null values handled appropriately?\n",
    "□ Is data validation implemented?\n",
    "□ Are schema contracts explicit?\n",
    "□ Are data quality checks in place?\n",
    "□ Is error handling comprehensive?\n",
    "\n",
    "Performance\n",
    "-----------\n",
    "□ Are built-in functions used (not UDFs)?\n",
    "□ Is unnecessary data shuffling avoided?\n",
    "□ Are partitions properly managed?\n",
    "□ Is caching used appropriately?\n",
    "□ Are broadcast joins used for small tables?\n",
    "□ Is column pruning applied?\n",
    "\n",
    "Code Quality\n",
    "------------\n",
    "□ Are functions small and focused?\n",
    "□ Is code readable and well-structured?\n",
    "□ Are variable names descriptive?\n",
    "□ Is complex logic extracted to named functions?\n",
    "□ Are magic numbers avoided (use constants)?\n",
    "□ Is code DRY (Don't Repeat Yourself)?\n",
    "\n",
    "Testing\n",
    "-------\n",
    "□ Are unit tests included?\n",
    "□ Is test coverage adequate (>80%)?\n",
    "□ Are integration tests included?\n",
    "□ Are tests independent and repeatable?\n",
    "□ Do tests use realistic data?\n",
    "\n",
    "Documentation\n",
    "-------------\n",
    "□ Are docstrings present for all functions?\n",
    "□ Are complex algorithms explained?\n",
    "□ Is README updated if needed?\n",
    "□ Are breaking changes documented?\n",
    "□ Is the PR description clear?\n",
    "\n",
    "Security & Compliance\n",
    "---------------------\n",
    "□ Are credentials properly secured?\n",
    "□ Are secrets not hardcoded?\n",
    "□ Is PII data handled correctly?\n",
    "□ Are access patterns appropriate?\n",
    "□ Does code comply with data governance policies?\n",
    "\n",
    "\n",
    "REVIEW COMMENT EXAMPLES\n",
    "=======================\n",
    "\n",
    "✅ Good Comments (Constructive)\n",
    "--------------------------------\n",
    "\n",
    "\"Consider using F.when() here instead of a UDF for better performance.\n",
    "UDFs have serialization overhead. Would this work?\n",
    "\n",
    "    return df.withColumn('category',\n",
    "        F.when(F.col('amount') > 1000, 'high')\n",
    "         .otherwise('low')\n",
    "    )\n",
    "\"\n",
    "\n",
    "\"This function looks great! One suggestion: could you add a docstring \n",
    "with an example? It would help other team members understand the \n",
    "expected input/output schema.\"\n",
    "\n",
    "\"Nice refactoring! I like how you extracted the complex logic into \n",
    "separate functions. This will be much easier to test.\"\n",
    "\n",
    "\"Question: What happens if the input DataFrame is empty? Should we \n",
    "add a check or is that handled elsewhere?\"\n",
    "\n",
    "\n",
    "❌ Bad Comments (Unhelpful)\n",
    "---------------------------\n",
    "\n",
    "\"This is wrong.\" \n",
    "# Better: Explain what's wrong and suggest a fix\n",
    "\n",
    "\"I wouldn't do it this way.\"\n",
    "# Better: Explain why and show alternative\n",
    "\n",
    "\"This won't scale.\"\n",
    "# Better: Explain performance concerns with evidence\n",
    "\n",
    "\"Change this.\"\n",
    "# Better: Explain reasoning and suggest improvements\n",
    "\n",
    "\n",
    "RESPONDING TO REVIEWS\n",
    "=====================\n",
    "\n",
    "As Author:\n",
    "----------\n",
    "✅ Thank reviewers for their time\n",
    "✅ Ask clarifying questions\n",
    "✅ Explain your reasoning when disagreeing\n",
    "✅ Mark resolved comments after addressing\n",
    "✅ Re-request review after major changes\n",
    "\n",
    "❌ Don't be defensive\n",
    "❌ Don't ignore feedback\n",
    "❌ Don't argue without data\n",
    "\n",
    "As Reviewer:\n",
    "------------\n",
    "✅ Be respectful and constructive\n",
    "✅ Ask questions, don't demand changes\n",
    "✅ Provide examples and alternatives\n",
    "✅ Distinguish between \"must fix\" and \"nice to have\"\n",
    "✅ Approve when requirements are met\n",
    "\n",
    "❌ Don't nitpick style (use linters)\n",
    "❌ Don't block on personal preferences\n",
    "❌ Don't delay reviews unnecessarily\n",
    "\n",
    "\n",
    "REVIEW TURNAROUND TIMES\n",
    "=======================\n",
    "\n",
    "Target SLAs:\n",
    "- Initial review: Within 24 hours\n",
    "- Follow-up reviews: Within 4 hours\n",
    "- Hotfix reviews: Within 1 hour\n",
    "\n",
    "Tips for Fast Reviews:\n",
    "- Keep PRs small (<400 lines)\n",
    "- Provide context in description\n",
    "- Self-review before requesting\n",
    "- Tag specific reviewers\n",
    "- Use draft PRs for early feedback\n",
    "\"\"\"\n",
    "\n",
    "print(code_review_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI/CD Pipeline with GitHub Actions\n",
    "\n",
    "Automated testing and deployment for PySpark projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CI/CD Pipeline Example ===\")\n",
    "\n",
    "github_actions_workflow = \"\"\"\n",
    "# .github/workflows/ci.yml\n",
    "# CI/CD Pipeline for PySpark Project\n",
    "\n",
    "name: CI/CD Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main, develop, staging ]\n",
    "  pull_request:\n",
    "    branches: [ main, develop ]\n",
    "\n",
    "env:\n",
    "  PYTHON_VERSION: '3.10'\n",
    "  SPARK_VERSION: '3.4.1'\n",
    "\n",
    "jobs:\n",
    "  # ================================\n",
    "  # Linting and Code Quality\n",
    "  # ================================\n",
    "  lint:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "      \n",
    "      - name: Install linting tools\n",
    "        run: |\n",
    "          pip install black ruff mypy\n",
    "      \n",
    "      - name: Run Black (code formatting)\n",
    "        run: black --check src/ tests/\n",
    "      \n",
    "      - name: Run Ruff (linting)\n",
    "        run: ruff check src/ tests/\n",
    "      \n",
    "      - name: Run MyPy (type checking)\n",
    "        run: mypy src/\n",
    "\n",
    "  # ================================\n",
    "  # Unit Tests\n",
    "  # ================================\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: lint\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "      \n",
    "      - name: Install Java (for PySpark)\n",
    "        uses: actions/setup-java@v3\n",
    "        with:\n",
    "          distribution: 'temurin'\n",
    "          java-version: '11'\n",
    "      \n",
    "      - name: Cache pip dependencies\n",
    "        uses: actions/cache@v3\n",
    "        with:\n",
    "          path: ~/.cache/pip\n",
    "          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "          pip install -r requirements-dev.txt\n",
    "      \n",
    "      - name: Run unit tests\n",
    "        run: |\n",
    "          pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html\n",
    "      \n",
    "      - name: Upload coverage reports\n",
    "        uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          file: ./coverage.xml\n",
    "          flags: unittests\n",
    "      \n",
    "      - name: Check coverage threshold\n",
    "        run: |\n",
    "          coverage report --fail-under=80\n",
    "\n",
    "  # ================================\n",
    "  # Integration Tests\n",
    "  # ================================\n",
    "  integration-test:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: test\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "      \n",
    "      - name: Install Java\n",
    "        uses: actions/setup-java@v3\n",
    "        with:\n",
    "          distribution: 'temurin'\n",
    "          java-version: '11'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "          pip install -r requirements-dev.txt\n",
    "      \n",
    "      - name: Run integration tests\n",
    "        run: |\n",
    "          pytest tests/integration/ -v --durations=10\n",
    "\n",
    "  # ================================\n",
    "  # Build Package\n",
    "  # ================================\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [test, integration-test]\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "      \n",
    "      - name: Install build tools\n",
    "        run: |\n",
    "          pip install build wheel\n",
    "      \n",
    "      - name: Build wheel package\n",
    "        run: |\n",
    "          python -m build\n",
    "      \n",
    "      - name: Upload artifacts\n",
    "        uses: actions/upload-artifact@v3\n",
    "        with:\n",
    "          name: dist-package\n",
    "          path: dist/\n",
    "\n",
    "  # ================================\n",
    "  # Deploy to Development\n",
    "  # ================================\n",
    "  deploy-dev:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: build\n",
    "    if: github.ref == 'refs/heads/develop'\n",
    "    environment: development\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Download artifacts\n",
    "        uses: actions/download-artifact@v3\n",
    "        with:\n",
    "          name: dist-package\n",
    "          path: dist/\n",
    "      \n",
    "      - name: Install Databricks CLI\n",
    "        run: |\n",
    "          pip install databricks-cli\n",
    "      \n",
    "      - name: Deploy to Databricks Dev\n",
    "        env:\n",
    "          DATABRICKS_HOST: ${{ secrets.DATABRICKS_DEV_HOST }}\n",
    "          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_DEV_TOKEN }}\n",
    "        run: |\n",
    "          # Upload wheel to Databricks\n",
    "          databricks fs cp dist/*.whl dbfs:/Volumes/dev/libraries/python/ --overwrite\n",
    "          \n",
    "          # Update job configuration\n",
    "          # databricks jobs reset --job-id <job-id> --json-file job-config.json\n",
    "\n",
    "  # ================================\n",
    "  # Deploy to Production\n",
    "  # ================================\n",
    "  deploy-prod:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: build\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    environment: production\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Download artifacts\n",
    "        uses: actions/download-artifact@v3\n",
    "        with:\n",
    "          name: dist-package\n",
    "          path: dist/\n",
    "      \n",
    "      - name: Install Databricks CLI\n",
    "        run: |\n",
    "          pip install databricks-cli\n",
    "      \n",
    "      - name: Deploy to Databricks Prod\n",
    "        env:\n",
    "          DATABRICKS_HOST: ${{ secrets.DATABRICKS_PROD_HOST }}\n",
    "          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_PROD_TOKEN }}\n",
    "        run: |\n",
    "          databricks fs cp dist/*.whl dbfs:/Volumes/prod/libraries/python/ --overwrite\n",
    "      \n",
    "      - name: Create GitHub Release\n",
    "        uses: actions/create-release@v1\n",
    "        env:\n",
    "          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n",
    "        with:\n",
    "          tag_name: v${{ github.run_number }}\n",
    "          release_name: Release v${{ github.run_number }}\n",
    "          body: |\n",
    "            Automated release from main branch\n",
    "          draft: false\n",
    "          prerelease: false\n",
    "\"\"\"\n",
    "\n",
    "print(github_actions_workflow)\n",
    "\n",
    "print(\"\\n✅ CI/CD Best Practices:\")\n",
    "print(\"  - Run tests on every PR\")\n",
    "print(\"  - Require passing tests before merge\")\n",
    "print(\"  - Use caching to speed up builds\")\n",
    "print(\"  - Separate dev and prod deployments\")\n",
    "print(\"  - Use GitHub environments for approvals\")\n",
    "print(\"  - Tag releases for rollback capability\")\n",
    "print(\"  - Monitor deployment success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Notebooks in Git\n",
    "\n",
    "Special considerations for Jupyter notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Managing Notebooks in Git ===\")\n",
    "\n",
    "notebook_management = \"\"\"\n",
    "CHALLENGES WITH NOTEBOOKS IN GIT\n",
    "=================================\n",
    "\n",
    "Problems:\n",
    "1. JSON format makes diffs hard to read\n",
    "2. Cell outputs create large diffs\n",
    "3. Cell execution order can vary\n",
    "4. Merge conflicts are difficult to resolve\n",
    "5. Binary outputs (images) aren't diffable\n",
    "\n",
    "\n",
    "SOLUTION 1: Clear Outputs Before Commit\n",
    "========================================\n",
    "\n",
    "Manual approach:\n",
    "- Jupyter: Kernel → Restart & Clear Output\n",
    "- Databricks: Clear → Clear All Outputs\n",
    "\n",
    "Automated approach with nbstripout:\n",
    "\n",
    "# Install nbstripout\n",
    "pip install nbstripout\n",
    "\n",
    "# Configure for repository (one-time setup)\n",
    "nbstripout --install\n",
    "\n",
    "# This creates .gitattributes:\n",
    "*.ipynb filter=nbstripout\n",
    "*.zpln filter=nbstripout\n",
    "*.ipynb diff=ipynb\n",
    "\n",
    "# Now outputs are automatically stripped on commit\n",
    "\n",
    "Pros:\n",
    "- Cleaner diffs\n",
    "- Smaller repo size\n",
    "- Easier merges\n",
    "\n",
    "Cons:\n",
    "- Lose output history\n",
    "- Need to re-run to see results\n",
    "\n",
    "\n",
    "SOLUTION 2: Reviewable Notebook Outputs\n",
    "========================================\n",
    "\n",
    "Use ReviewNB for GitHub:\n",
    "- Visual diff tool for notebooks\n",
    "- Shows side-by-side comparisons\n",
    "- Displays cell outputs\n",
    "- Integrates with GitHub PRs\n",
    "\n",
    "Setup:\n",
    "1. Install ReviewNB GitHub app\n",
    "2. Authorize for your repository\n",
    "3. Automatic notebook diffs in PRs\n",
    "\n",
    "Alternative: nbdime\n",
    "- Command-line notebook diff tool\n",
    "- Git integration available\n",
    "\n",
    "pip install nbdime\n",
    "nbdime config-git --enable\n",
    "\n",
    "\n",
    "SOLUTION 3: Convert to Python Scripts\n",
    "======================================\n",
    "\n",
    "Use Jupytext for dual format:\n",
    "\n",
    "# Install jupytext\n",
    "pip install jupytext\n",
    "\n",
    "# Pair notebook with .py file\n",
    "jupytext --set-formats ipynb,py:percent notebook.ipynb\n",
    "\n",
    "# Creates notebook.py alongside notebook.ipynb\n",
    "# Changes sync automatically\n",
    "\n",
    "Benefits:\n",
    "- .py files are easy to diff\n",
    "- Can edit in IDE or notebook\n",
    "- Better for code review\n",
    "- Merge conflicts easier to resolve\n",
    "\n",
    "Workflow:\n",
    "1. Develop in .ipynb (interactive)\n",
    "2. Save syncs to .py (version control)\n",
    "3. Review .py in PRs (readable)\n",
    "4. Collaborators can use either format\n",
    "\n",
    "\n",
    "SOLUTION 4: Modular Code Approach (Recommended)\n",
    "================================================\n",
    "\n",
    "Separate notebooks from logic:\n",
    "\n",
    "project/\n",
    "  src/\n",
    "    transformations/  # Pure Python modules\n",
    "      cleaning.py\n",
    "      business_logic.py\n",
    "    \n",
    "  notebooks/\n",
    "    exploration/      # Keep outputs, don't commit\n",
    "    production/       # Strip outputs, commit\n",
    "\n",
    "Workflow:\n",
    "1. Core logic in .py modules (easy to version)\n",
    "2. Notebooks import and use modules\n",
    "3. Exploratory notebooks in separate folder\n",
    "4. Production notebooks are minimal (mostly imports)\n",
    "\n",
    "Benefits:\n",
    "- Clean separation of concerns\n",
    "- .py files are Git-friendly\n",
    "- Easy to test modules\n",
    "- Notebooks focus on orchestration\n",
    "\n",
    "\n",
    "NOTEBOOK BEST PRACTICES\n",
    "=======================\n",
    "\n",
    "✅ DO:\n",
    "- Keep notebooks focused and concise\n",
    "- Extract reusable logic to .py modules\n",
    "- Use clear cell descriptions\n",
    "- Run \"Restart & Run All\" before committing\n",
    "- Clear outputs for production notebooks\n",
    "- Keep exploratory notebooks separate\n",
    "\n",
    "❌ DON'T:\n",
    "- Commit notebooks with errors\n",
    "- Include large outputs (images, DataFrames)\n",
    "- Use notebooks for complex logic\n",
    "- Commit notebooks with cell execution out of order\n",
    "- Include sensitive data in outputs\n",
    "\"\"\"\n",
    "\n",
    "print(notebook_management)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration Patterns\n",
    "\n",
    "Team workflows for effective collaboration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Collaboration Patterns ===\")\n",
    "\n",
    "collaboration_guide = \"\"\"\n",
    "PAIR PROGRAMMING FOR DATA ENGINEERING\n",
    "=====================================\n",
    "\n",
    "Effective pair programming:\n",
    "\n",
    "Driver-Navigator Pattern:\n",
    "- Driver: Types code in Databricks notebook\n",
    "- Navigator: Reviews logic, suggests improvements\n",
    "- Switch roles every 30 minutes\n",
    "\n",
    "Benefits:\n",
    "- Knowledge sharing\n",
    "- Fewer bugs\n",
    "- Better design decisions\n",
    "- Real-time code review\n",
    "\n",
    "Best for:\n",
    "- Complex transformations\n",
    "- Learning new patterns\n",
    "- Onboarding new team members\n",
    "- Critical production code\n",
    "\n",
    "\n",
    "ASYNC COLLABORATION\n",
    "===================\n",
    "\n",
    "Using GitHub for async work:\n",
    "\n",
    "1. Clear Communication:\n",
    "   - Detailed PR descriptions\n",
    "   - Inline code comments\n",
    "   - Design documents in repo\n",
    "   - Architecture Decision Records (ADRs)\n",
    "\n",
    "2. Documentation:\n",
    "   - README.md for project overview\n",
    "   - CONTRIBUTING.md for dev setup\n",
    "   - docs/ for detailed guides\n",
    "   - Docstrings in all functions\n",
    "\n",
    "3. Issue Tracking:\n",
    "   - Use GitHub Issues for tasks\n",
    "   - Link PRs to issues\n",
    "   - Use labels for categorization\n",
    "   - Use milestones for sprints\n",
    "\n",
    "4. Project Boards:\n",
    "   - Kanban board for workflow\n",
    "   - Columns: Backlog, In Progress, Review, Done\n",
    "   - Automate card movement\n",
    "\n",
    "\n",
    "HANDLING CONFLICTS\n",
    "==================\n",
    "\n",
    "Prevention:\n",
    "- Small, frequent commits\n",
    "- Short-lived feature branches\n",
    "- Regular rebasing from main\n",
    "- Clear code ownership\n",
    "\n",
    "Resolution:\n",
    "- Communicate with teammate\n",
    "- Understand both changes\n",
    "- Prefer their changes if unsure\n",
    "- Test after resolving\n",
    "- Ask for help if complex\n",
    "\n",
    "\n",
    "KNOWLEDGE SHARING\n",
    "=================\n",
    "\n",
    "Team Practices:\n",
    "1. Code reviews as learning opportunities\n",
    "2. Regular knowledge sharing sessions\n",
    "3. Internal documentation wiki\n",
    "4. Lunch & learn presentations\n",
    "5. Recorded demos of complex features\n",
    "6. Onboarding checklist and buddy system\n",
    "\n",
    "Documentation:\n",
    "- Architecture diagrams\n",
    "- Data lineage documentation\n",
    "- Runbooks for common tasks\n",
    "- Troubleshooting guides\n",
    "- Best practices guide\n",
    "\n",
    "\n",
    "RELEASE MANAGEMENT\n",
    "==================\n",
    "\n",
    "Semantic Versioning:\n",
    "- MAJOR.MINOR.PATCH (e.g., 2.3.1)\n",
    "- MAJOR: Breaking changes\n",
    "- MINOR: New features (backward compatible)\n",
    "- PATCH: Bug fixes\n",
    "\n",
    "Release Process:\n",
    "1. Create release branch from main\n",
    "2. Test thoroughly on staging\n",
    "3. Create GitHub release with notes\n",
    "4. Tag commit with version number\n",
    "5. Deploy to production\n",
    "6. Monitor for issues\n",
    "7. Keep previous version for rollback\n",
    "\n",
    "Release Notes Template:\n",
    "---\n",
    "## v2.3.0 - 2024-01-15\n",
    "\n",
    "### Added\n",
    "- New customer segmentation pipeline\n",
    "- Revenue analytics dashboard integration\n",
    "\n",
    "### Changed\n",
    "- Improved performance of aggregation queries (2x faster)\n",
    "- Updated dependencies (see requirements.txt)\n",
    "\n",
    "### Fixed\n",
    "- Fixed null handling in revenue calculation\n",
    "- Resolved data quality check false positives\n",
    "\n",
    "### Deprecated\n",
    "- Old segmentation logic (will be removed in v3.0.0)\n",
    "\n",
    "### Migration Guide\n",
    "- Update import: from `old_module` to `new_module`\n",
    "- Run migration script: `python scripts/migrate_v2_to_v3.py`\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "print(collaboration_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Databricks Repos Integration**:\n",
    "   - Native Git support in Databricks workspace\n",
    "   - Visual Git operations (commit, push, pull, branch)\n",
    "   - Collaborative development environment\n",
    "   - Supports multiple Git providers\n",
    "\n",
    "2. **Branching Strategies**:\n",
    "   - GitHub Flow: Simple, continuous deployment\n",
    "   - Git Flow: Complex releases, multiple versions\n",
    "   - Trunk-Based: Fast-moving teams with mature CI/CD\n",
    "   - Recommendation: GitHub Flow with environment branches\n",
    "\n",
    "3. **Git Workflows**:\n",
    "   - Small, focused commits with clear messages\n",
    "   - Feature branches for every change\n",
    "   - Pull requests for code review\n",
    "   - Regular rebasing to stay current\n",
    "\n",
    "4. **Code Review**:\n",
    "   - Comprehensive checklists for PySpark code\n",
    "   - Constructive, helpful comments\n",
    "   - Focus on correctness, performance, quality\n",
    "   - Fast turnaround times (< 24 hours)\n",
    "\n",
    "5. **CI/CD Automation**:\n",
    "   - Automated testing on every PR\n",
    "   - Linting and code quality checks\n",
    "   - Build and deploy pipelines\n",
    "   - Environment-specific deployments\n",
    "\n",
    "6. **Notebook Management**:\n",
    "   - Clear outputs before commit (nbstripout)\n",
    "   - Use ReviewNB for visual diffs\n",
    "   - Convert to Python scripts (Jupytext)\n",
    "   - Modular code approach (recommended)\n",
    "\n",
    "7. **Collaboration**:\n",
    "   - Pair programming for complex features\n",
    "   - Async collaboration with clear communication\n",
    "   - Knowledge sharing practices\n",
    "   - Release management with semantic versioning\n",
    "\n",
    "**Best Practices for Git/GitHub in PySpark**:\n",
    "- Integrate Databricks Repos early in project\n",
    "- Use consistent branching strategy across team\n",
    "- Write clear commit messages and PR descriptions\n",
    "- Automate testing and deployment\n",
    "- Extract logic to .py modules for better version control\n",
    "- Clear notebook outputs before committing\n",
    "- Conduct thorough code reviews\n",
    "- Document everything (README, docstrings, ADRs)\n",
    "- Tag releases for easy rollback\n",
    "- Foster collaborative culture\n",
    "\n",
    "This completes Section 6 on project structure and collaboration! You now have a complete understanding of how to build, organize, and collaborate on production-grade PySpark projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Practice Git and GitHub workflows:\n",
    "\n",
    "1. Set up a Databricks Repos connection to a GitHub repository\n",
    "2. Create a feature branch for a new transformation\n",
    "3. Make commits following best practices\n",
    "4. Write a comprehensive pull request description\n",
    "5. Set up a .gitignore file for your project\n",
    "6. Create a GitHub Actions workflow for testing\n",
    "7. Practice code review on a teammate's PR\n",
    "8. Set up nbstripout for notebook output management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise notes here\n",
    "\n",
    "# 1. Databricks Repos setup checklist\n",
    "repos_setup = \"\"\"\n",
    "□ Navigate to Repos in Databricks\n",
    "□ Click \"Add Repo\"\n",
    "□ Connect to GitHub\n",
    "□ Clone repository\n",
    "□ Verify files are accessible\n",
    "\"\"\"\n",
    "\n",
    "# 2. Your feature branch name\n",
    "feature_branch = \"feature/your-feature-name\"\n",
    "\n",
    "# 3. Your commit messages\n",
    "commit_messages = \"\"\"\n",
    "feat: Add customer segmentation logic\n",
    "\n",
    "- Implement RFM segmentation algorithm\n",
    "- Add data quality checks\n",
    "- Include unit tests with 85% coverage\n",
    "\"\"\"\n",
    "\n",
    "# 4. Your PR description template\n",
    "pr_description = \"\"\"\n",
    "## Summary\n",
    "[Your summary]\n",
    "\n",
    "## Changes\n",
    "- [Change 1]\n",
    "- [Change 2]\n",
    "\n",
    "## Testing\n",
    "- [x] Unit tests pass\n",
    "- [ ] Integration tests pass\n",
    "\n",
    "## Checklist\n",
    "- [ ] Code follows style guide\n",
    "- [ ] Tests added\n",
    "- [ ] Documentation updated\n",
    "\"\"\"\n",
    "\n",
    "# Print your exercise plan\n",
    "# print(repos_setup)\n",
    "# print(commit_messages)\n",
    "# print(pr_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
