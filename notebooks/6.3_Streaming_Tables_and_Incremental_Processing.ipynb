{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Streaming Tables and Incremental Processing\n",
    "\n",
    "This notebook explores real-time data processing with Lakeflow's `@dp.streaming_table` decorator. We'll learn how to build robust streaming pipelines with checkpointing, exactly-once semantics, watermarking, and incremental processing patterns using functional programming principles.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Define streaming tables with `@dp.streaming_table` for real-time processing\n",
    "- Implement checkpointing for fault tolerance and exactly-once guarantees\n",
    "- Handle late-arriving data with watermarking strategies\n",
    "- Design incremental processing patterns for efficient data pipelines\n",
    "- Perform stream-to-stream and stream-to-batch joins\n",
    "- Optimize streaming workload performance\n",
    "- Apply functional programming to streaming transformations\n",
    "- Test streaming pipelines with synthetic data\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Notebooks 6.1 and 6.2\n",
    "- Understanding of Spark Structured Streaming concepts\n",
    "- Knowledge of Delta Lake and change data capture\n",
    "- Familiarity with event-driven architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform setup detection\n",
    "# In Databricks: Keep commented\n",
    "# In Local: Uncomment this line\n",
    "# %run 00_Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# In a real Lakeflow pipeline:\n",
    "# from pyspark import pipelines as dp\n",
    "\n",
    "print(\"✅ Imports complete - Ready for streaming table demonstration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Streaming Tables vs Batch Tables\n",
    "\n",
    "### Fundamental Differences\n",
    "\n",
    "**Batch Table (`@dp.table`)**:\n",
    "```python\n",
    "@dp.table\n",
    "def customers():\n",
    "    \"\"\"Batch processing: Complete snapshot each run\"\"\"\n",
    "    return spark.read.table(\"raw.customers\")\n",
    "    # Processes entire dataset on each pipeline execution\n",
    "```\n",
    "\n",
    "**Streaming Table (`@dp.streaming_table`)**:\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def events():\n",
    "    \"\"\"Streaming: Continuous incremental processing\"\"\"\n",
    "    return spark.readStream.table(\"raw.events\")\n",
    "    # Only processes new data since last checkpoint\n",
    "```\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "| Criteria | Batch Table | Streaming Table |\n",
    "|----------|-------------|------------------|\n",
    "| **Data Pattern** | Complete snapshots | Continuous stream |\n",
    "| **Processing** | Full table each run | Only new records |\n",
    "| **Latency** | Minutes to hours | Seconds to minutes |\n",
    "| **State Management** | None | Checkpoints maintained |\n",
    "| **Fault Tolerance** | Re-run from start | Resume from checkpoint |\n",
    "| **Cost** | Higher for large tables | Lower (incremental) |\n",
    "| **Use Case** | Dimensions, aggregates | Events, logs, CDC |\n",
    "| **Update Frequency** | Scheduled (hourly/daily) | Continuous/micro-batch |\n",
    "\n",
    "### When to Use Streaming Tables\n",
    "\n",
    "**✅ Use streaming tables for:**\n",
    "- Event logs (clickstreams, application logs, IoT sensors)\n",
    "- Change data capture (CDC) from databases\n",
    "- Real-time analytics and dashboards\n",
    "- Message queue data (Kafka, Event Hubs, Kinesis)\n",
    "- Monitoring and alerting systems\n",
    "- Time-series data with continuous ingestion\n",
    "\n",
    "**❌ Don't use streaming tables for:**\n",
    "- Dimension tables with full snapshots\n",
    "- Data that arrives in large batches\n",
    "- One-time historical loads\n",
    "- Tables requiring complex aggregations without time windows\n",
    "- Data sources without natural append ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining Streaming Tables\n",
    "\n",
    "### Basic Streaming Table Definition\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "@dp.streaming_table(\n",
    "    name=\"bronze_events\",\n",
    "    comment=\"Real-time event stream from application logs\"\n",
    ")\n",
    "def bronze_events():\n",
    "    \"\"\"\n",
    "    Pure function returning a streaming DataFrame.\n",
    "    Lakeflow manages checkpointing and state automatically.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .table(\"raw.events\")\n",
    "    )\n",
    "```\n",
    "\n",
    "### Reading from Various Streaming Sources\n",
    "\n",
    "```python\n",
    "# Source 1: Delta table (Auto Loader pattern)\n",
    "@dp.streaming_table\n",
    "def events_from_delta():\n",
    "    return spark.readStream.format(\"delta\").table(\"raw.events\")\n",
    "\n",
    "# Source 2: Cloud Files (Auto Loader for S3, ADLS, GCS)\n",
    "@dp.streaming_table\n",
    "def events_from_cloud_files():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/mnt/schemas/events\")\n",
    "        .load(\"/mnt/landing/events/\")\n",
    "    )\n",
    "\n",
    "# Source 3: Kafka topic\n",
    "@dp.streaming_table\n",
    "def events_from_kafka():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"broker:9092\")\n",
    "        .option(\"subscribe\", \"events-topic\")\n",
    "        .option(\"startingOffsets\", \"latest\")\n",
    "        .load()\n",
    "        .select(\n",
    "            F.col(\"key\").cast(\"string\"),\n",
    "            F.from_json(F.col(\"value\").cast(\"string\"), event_schema).alias(\"data\")\n",
    "        )\n",
    "        .select(\"key\", \"data.*\")\n",
    "    )\n",
    "\n",
    "# Source 4: Event Hub (Azure)\n",
    "@dp.streaming_table\n",
    "def events_from_eventhub():\n",
    "    connection_string = dbutils.secrets.get(\"eventhub\", \"connection-string\")\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"eventhubs\")\n",
    "        .options(**{\n",
    "            \"eventhubs.connectionString\": connection_string,\n",
    "            \"eventhubs.consumerGroup\": \"lakeflow\"\n",
    "        })\n",
    "        .load()\n",
    "    )\n",
    "```\n",
    "\n",
    "### Streaming Table with Transformations\n",
    "\n",
    "```python\n",
    "# Pure transformation functions (testable)\n",
    "def parse_event_timestamp(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function: Parse timestamp from string\"\"\"\n",
    "    return df.withColumn(\n",
    "        \"event_timestamp\",\n",
    "        F.to_timestamp(\"timestamp_str\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    "    )\n",
    "\n",
    "def enrich_with_metadata(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function: Add processing metadata\"\"\"\n",
    "    return df.withColumn(\"processed_at\", F.current_timestamp())\n",
    "\n",
    "def extract_user_agent_info(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pure function: Parse user agent string\"\"\"\n",
    "    return df.withColumn(\n",
    "        \"device_type\",\n",
    "        F.when(F.col(\"user_agent\").like(\"%Mobile%\"), \"mobile\")\n",
    "         .when(F.col(\"user_agent\").like(\"%Tablet%\"), \"tablet\")\n",
    "         .otherwise(\"desktop\")\n",
    "    )\n",
    "\n",
    "# Compose pure functions in streaming table\n",
    "@dp.streaming_table(\n",
    "    name=\"bronze_events_enriched\",\n",
    "    comment=\"Streaming events with parsed timestamps and metadata\"\n",
    ")\n",
    "def bronze_events_enriched():\n",
    "    \"\"\"Streaming table with functional transformation composition\"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.events\")\n",
    "        .transform(parse_event_timestamp)\n",
    "        .transform(enrich_with_metadata)\n",
    "        .transform(extract_user_agent_info)\n",
    "        .select(\n",
    "            \"event_id\",\n",
    "            \"event_type\",\n",
    "            \"event_timestamp\",\n",
    "            \"user_id\",\n",
    "            \"device_type\",\n",
    "            \"processed_at\"\n",
    "        )\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checkpointing and Exactly-Once Semantics\n",
    "\n",
    "### What is Checkpointing?\n",
    "\n",
    "Checkpointing is Spark's mechanism for maintaining **streaming state** and enabling **exactly-once processing**.\n",
    "\n",
    "**Checkpoint Information Stored:**\n",
    "- Offsets read from source (which records have been processed)\n",
    "- Processing metadata (batch IDs, timestamps)\n",
    "- Aggregation state (for stateful operations)\n",
    "- Configuration and schema information\n",
    "\n",
    "### How Lakeflow Manages Checkpoints\n",
    "\n",
    "```python\n",
    "# In Lakeflow, checkpointing is AUTOMATIC\n",
    "@dp.streaming_table\n",
    "def my_stream():\n",
    "    return spark.readStream.table(\"source\")\n",
    "    # Lakeflow automatically:\n",
    "    # 1. Creates checkpoint directory\n",
    "    # 2. Manages checkpoint lifecycle\n",
    "    # 3. Ensures exactly-once processing\n",
    "    # 4. Handles recovery from failures\n",
    "```\n",
    "\n",
    "**In traditional Spark Structured Streaming** (for comparison):\n",
    "```python\n",
    "# Manual checkpoint management (NOT needed in Lakeflow)\n",
    "query = (\n",
    "    spark.readStream.table(\"source\")\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/my_stream\")  # Manual\n",
    "    .table(\"target\")\n",
    ")\n",
    "```\n",
    "\n",
    "### Exactly-Once Processing Guarantees\n",
    "\n",
    "Lakeflow streaming tables provide **exactly-once semantics**:\n",
    "\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def transaction_stream():\n",
    "    \"\"\"\n",
    "    Each transaction is processed exactly once:\n",
    "    - No duplicates (not at-least-once)\n",
    "    - No missing records (not at-most-once)\n",
    "    - Exactly once, even with failures and retries\n",
    "    \"\"\"\n",
    "    return spark.readStream.table(\"raw.transactions\")\n",
    "```\n",
    "\n",
    "**How Exactly-Once Works:**\n",
    "1. Source offsets tracked in checkpoint\n",
    "2. Transformation applied idempotently\n",
    "3. Output written with transactional guarantees (Delta Lake)\n",
    "4. Checkpoint updated only after successful write\n",
    "5. On failure, replay from last successful checkpoint\n",
    "\n",
    "### Checkpoint Lifecycle and Management\n",
    "\n",
    "```python\n",
    "# Lakeflow manages checkpoint lifecycle automatically\n",
    "@dp.streaming_table(\n",
    "    name=\"events\",\n",
    "    comment=\"Checkpoint managed by Lakeflow\"\n",
    ")\n",
    "def events():\n",
    "    return spark.readStream.table(\"raw.events\")\n",
    "\n",
    "# Checkpoint location: Automatically determined by Lakeflow\n",
    "# Format: /pipelines/<pipeline_id>/checkpoints/<table_name>/\n",
    "# Lifecycle: \n",
    "#   - Created on first run\n",
    "#   - Maintained across pipeline runs\n",
    "#   - Cleaned up when table is dropped\n",
    "```\n",
    "\n",
    "### Schema Evolution with Checkpoints\n",
    "\n",
    "```python\n",
    "# Schema changes require checkpoint reset\n",
    "@dp.streaming_table\n",
    "def events_with_schema():\n",
    "    \"\"\"\n",
    "    If source schema changes:\n",
    "    1. Lakeflow detects schema mismatch\n",
    "    2. Pipeline fails with clear error message\n",
    "    3. Options:\n",
    "       a) Reset checkpoint (reprocess all data)\n",
    "       b) Add schema evolution logic\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .option(\"mergeSchema\", \"true\")  # Enable schema evolution\n",
    "        .table(\"raw.events\")\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Watermarking and Late Data Handling\n",
    "\n",
    "### What is Watermarking?\n",
    "\n",
    "**Watermarking** defines how long Spark should wait for late-arriving data in event-time processing.\n",
    "\n",
    "**Problem**: Events may arrive out of order or delayed\n",
    "```\n",
    "Event time:  10:00  10:01  10:02  10:03  10:04\n",
    "Arrival:     10:00  10:02  10:01  10:05  10:03  <- Out of order!\n",
    "```\n",
    "\n",
    "**Solution**: Watermark defines acceptable delay\n",
    "\n",
    "### Watermarking in Streaming Tables\n",
    "\n",
    "```python\n",
    "@dp.streaming_table(\n",
    "    name=\"events_with_watermark\",\n",
    "    comment=\"Events with 10-minute watermark for late data\"\n",
    ")\n",
    "def events_with_watermark():\n",
    "    \"\"\"\n",
    "    Watermark: \"Allow events up to 10 minutes late\"\n",
    "    \n",
    "    If current max event_time is 12:00:\n",
    "    - Accept events with event_time >= 11:50\n",
    "    - Drop events with event_time < 11:50 (too late)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.events\")\n",
    "        .withWatermark(\"event_timestamp\", \"10 minutes\")  # Watermark definition\n",
    "    )\n",
    "```\n",
    "\n",
    "### Watermark Impact on Aggregations\n",
    "\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def hourly_event_counts():\n",
    "    \"\"\"\n",
    "    Time-windowed aggregation with watermark.\n",
    "    \n",
    "    Without watermark:\n",
    "    - State grows infinitely (all windows kept in memory)\n",
    "    - Out-of-memory errors\n",
    "    \n",
    "    With watermark:\n",
    "    - Old windows automatically evicted\n",
    "    - Bounded state size\n",
    "    - Late data handled gracefully\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.events\")\n",
    "        .withWatermark(\"event_timestamp\", \"1 hour\")  # Wait 1 hour for late data\n",
    "        .groupBy(\n",
    "            F.window(\"event_timestamp\", \"1 hour\"),  # Hourly windows\n",
    "            \"event_type\"\n",
    "        )\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"event_count\"),\n",
    "            F.countDistinct(\"user_id\").alias(\"unique_users\")\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"window.start\").alias(\"window_start\"),\n",
    "            F.col(\"window.end\").alias(\"window_end\"),\n",
    "            \"event_type\",\n",
    "            \"event_count\",\n",
    "            \"unique_users\"\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "### Choosing Watermark Duration\n",
    "\n",
    "**Factors to Consider:**\n",
    "- Expected latency of data source\n",
    "- Business tolerance for data completeness\n",
    "- State size and memory constraints\n",
    "- Processing time vs event time skew\n",
    "\n",
    "**Guidelines:**\n",
    "```python\n",
    "# Short watermark (1-5 minutes)\n",
    "# Use when: Near real-time data, low latency requirements\n",
    ".withWatermark(\"timestamp\", \"2 minutes\")\n",
    "\n",
    "# Medium watermark (10-60 minutes)\n",
    "# Use when: Typical streaming data with occasional delays\n",
    ".withWatermark(\"timestamp\", \"30 minutes\")\n",
    "\n",
    "# Long watermark (hours)\n",
    "# Use when: High latency sources, prioritize completeness\n",
    ".withWatermark(\"timestamp\", \"4 hours\")\n",
    "```\n",
    "\n",
    "### Monitoring Late Data\n",
    "\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def events_with_lateness_tracking():\n",
    "    \"\"\"\n",
    "    Track how late events arrive for watermark tuning.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.events\")\n",
    "        .withColumn(\n",
    "            \"arrival_timestamp\",\n",
    "            F.current_timestamp()\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"lateness_seconds\",\n",
    "            F.unix_timestamp(\"arrival_timestamp\") - F.unix_timestamp(\"event_timestamp\")\n",
    "        )\n",
    "        .withWatermark(\"event_timestamp\", \"15 minutes\")\n",
    "    )\n",
    "# Analyze lateness_seconds to optimize watermark duration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stateful Operations in Streaming\n",
    "\n",
    "### Stateless vs Stateful Transformations\n",
    "\n",
    "**Stateless** (no memory of previous records):\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def stateless_transform():\n",
    "    \"\"\"Each record processed independently\"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"events\")\n",
    "        .filter(F.col(\"amount\") > 100)      # Stateless\n",
    "        .withColumn(\"double\", F.col(\"amount\") * 2)  # Stateless\n",
    "    )\n",
    "# No state maintained between batches\n",
    "```\n",
    "\n",
    "**Stateful** (maintains state across records):\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def stateful_aggregation():\n",
    "    \"\"\"Maintains running aggregates\"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"events\")\n",
    "        .withWatermark(\"timestamp\", \"1 hour\")\n",
    "        .groupBy(\"user_id\")              # Stateful: maintains groups\n",
    "        .agg(F.sum(\"amount\"))            # Stateful: running sum\n",
    "    )\n",
    "# State grows with number of unique keys (user_ids)\n",
    "```\n",
    "\n",
    "### Common Stateful Operations\n",
    "\n",
    "**1. Windowed Aggregations**\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def window_aggregations():\n",
    "    return (\n",
    "        spark.readStream.table(\"events\")\n",
    "        .withWatermark(\"event_time\", \"10 minutes\")\n",
    "        .groupBy(\n",
    "            F.window(\"event_time\", \"5 minutes\", \"1 minute\")  # Sliding window\n",
    "        )\n",
    "        .agg(F.sum(\"revenue\").alias(\"total_revenue\"))\n",
    "    )\n",
    "```\n",
    "\n",
    "**2. Deduplication**\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def deduplicated_events():\n",
    "    \"\"\"\n",
    "    Remove duplicate events based on event_id.\n",
    "    Maintains state of all seen event_ids within watermark.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.events\")\n",
    "        .withWatermark(\"event_time\", \"24 hours\")\n",
    "        .dropDuplicates([\"event_id\"])  # Stateful deduplication\n",
    "    )\n",
    "```\n",
    "\n",
    "**3. Stream-Stream Joins**\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def joined_streams():\n",
    "    \"\"\"\n",
    "    Join two streams with state management.\n",
    "    Both streams buffered within watermark window.\n",
    "    \"\"\"\n",
    "    clicks = (\n",
    "        spark.readStream.table(\"clicks\")\n",
    "        .withWatermark(\"click_time\", \"10 minutes\")\n",
    "    )\n",
    "    \n",
    "    impressions = (\n",
    "        spark.readStream.table(\"impressions\")\n",
    "        .withWatermark(\"impression_time\", \"10 minutes\")\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        clicks.join(\n",
    "            impressions,\n",
    "            F.expr(\"\"\"\n",
    "                click_id = impression_id AND\n",
    "                click_time >= impression_time AND\n",
    "                click_time <= impression_time + interval 5 minutes\n",
    "            \"\"\"),\n",
    "            \"leftOuter\"\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "### Managing State Size\n",
    "\n",
    "**State Size Growth Patterns:**\n",
    "```python\n",
    "# ⚠️ UNBOUNDED: State grows forever\n",
    "@dp.streaming_table\n",
    "def unbounded_state():\n",
    "    return (\n",
    "        spark.readStream.table(\"events\")\n",
    "        .groupBy(\"user_id\")  # No watermark!\n",
    "        .agg(F.sum(\"amount\"))  # Accumulates for all time\n",
    "    )\n",
    "# Problem: Will eventually run out of memory\n",
    "\n",
    "# ✅ BOUNDED: State cleaned up by watermark\n",
    "@dp.streaming_table\n",
    "def bounded_state():\n",
    "    return (\n",
    "        spark.readStream.table(\"events\")\n",
    "        .withWatermark(\"event_time\", \"1 day\")  # Watermark set\n",
    "        .groupBy(\n",
    "            F.window(\"event_time\", \"1 hour\"),  # Windowed\n",
    "            \"user_id\"\n",
    "        )\n",
    "        .agg(F.sum(\"amount\"))\n",
    "    )\n",
    "# State for old windows automatically evicted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stream-to-Batch Joins\n",
    "\n",
    "### Enriching Streams with Dimension Tables\n",
    "\n",
    "```python\n",
    "# Dimension table (batch)\n",
    "@dp.table\n",
    "def customers():\n",
    "    \"\"\"Slowly changing dimension - batch table\"\"\"\n",
    "    return spark.table(\"raw.customers\")\n",
    "\n",
    "# Stream enriched with dimension\n",
    "@dp.streaming_table\n",
    "def enriched_transactions():\n",
    "    \"\"\"\n",
    "    Join streaming transactions with batch customer table.\n",
    "    \n",
    "    Pattern: Stream-to-batch join\n",
    "    - Stream: transactions (real-time)\n",
    "    - Batch: customers (updated hourly/daily)\n",
    "    - Result: Enriched stream with customer details\n",
    "    \"\"\"\n",
    "    transactions_stream = spark.readStream.table(\"raw.transactions\")\n",
    "    customers_batch = dp.read(\"customers\")  # Batch table\n",
    "    \n",
    "    return (\n",
    "        transactions_stream\n",
    "        .join(\n",
    "            customers_batch,\n",
    "            \"customer_id\",\n",
    "            \"left\"  # Left join to keep all transactions\n",
    "        )\n",
    "        .select(\n",
    "            \"transaction_id\",\n",
    "            \"customer_id\",\n",
    "            customers_batch[\"customer_name\"],\n",
    "            customers_batch[\"customer_tier\"],\n",
    "            \"amount\",\n",
    "            \"transaction_time\"\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "### Performance Considerations\n",
    "\n",
    "```python\n",
    "# ❌ ANTI-PATTERN: Large dimension not broadcast\n",
    "@dp.streaming_table\n",
    "def slow_enrichment():\n",
    "    stream = spark.readStream.table(\"events\")\n",
    "    large_dimension = spark.table(\"huge_dimension\")  # 10GB table\n",
    "    \n",
    "    return stream.join(large_dimension, \"key\")  # Shuffle join (slow)\n",
    "\n",
    "# ✅ BEST PRACTICE: Broadcast small dimensions\n",
    "@dp.streaming_table\n",
    "def fast_enrichment():\n",
    "    stream = spark.readStream.table(\"events\")\n",
    "    small_dimension = F.broadcast(spark.table(\"dim_table\"))  # <10MB\n",
    "    \n",
    "    return stream.join(small_dimension, \"key\")  # Broadcast join (fast)\n",
    "```\n",
    "\n",
    "### Dimension Table Updates\n",
    "\n",
    "```python\n",
    "# Important: Batch tables read at query start\n",
    "@dp.streaming_table\n",
    "def stream_with_dimension():\n",
    "    \"\"\"\n",
    "    Dimension table is read once when streaming query starts.\n",
    "    \n",
    "    To pick up dimension updates:\n",
    "    1. Stop pipeline\n",
    "    2. Refresh dimension (run batch update)\n",
    "    3. Restart pipeline (reads new dimension version)\n",
    "    \n",
    "    Or: Use Delta table time travel for specific versions\n",
    "    \"\"\"\n",
    "    stream = spark.readStream.table(\"events\")\n",
    "    dimension = spark.table(\"customers\")  # Snapshot at query start\n",
    "    \n",
    "    return stream.join(dimension, \"customer_id\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Incremental Processing Patterns\n",
    "\n",
    "### Pattern 1: Simple Append\n",
    "\n",
    "```python\n",
    "@dp.streaming_table(\n",
    "    name=\"bronze_logs\",\n",
    "    comment=\"Incremental append of log files\"\n",
    ")\n",
    "def bronze_logs():\n",
    "    \"\"\"\n",
    "    Simplest pattern: Just append new records.\n",
    "    Use when:\n",
    "    - Data is naturally append-only\n",
    "    - No updates or deletes\n",
    "    - Order doesn't matter\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .load(\"/mnt/logs/\")\n",
    "    )\n",
    "```\n",
    "\n",
    "### Pattern 2: Deduplication on Append\n",
    "\n",
    "```python\n",
    "@dp.streaming_table(\n",
    "    name=\"deduplicated_events\",\n",
    "    comment=\"Events deduplicated by event_id\"\n",
    ")\n",
    "def deduplicated_events():\n",
    "    \"\"\"\n",
    "    Remove duplicates before materializing.\n",
    "    Use when:\n",
    "    - Source may send duplicates\n",
    "    - Have unique identifier\n",
    "    - Can afford watermark-bounded deduplication\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.events\")\n",
    "        .withWatermark(\"event_timestamp\", \"24 hours\")\n",
    "        .dropDuplicates([\"event_id\"])\n",
    "    )\n",
    "```\n",
    "\n",
    "### Pattern 3: Late-Arriving Data with Merge\n",
    "\n",
    "```python\n",
    "@dp.streaming_table(\n",
    "    name=\"merged_events\",\n",
    "    comment=\"Events merged to handle late arrivals\"\n",
    ")\n",
    "def merged_events():\n",
    "    \"\"\"\n",
    "    Handle updates and late-arriving data.\n",
    "    Use when:\n",
    "    - Records can be updated\n",
    "    - Late data must update existing records\n",
    "    - Have unique key for merging\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.events\")\n",
    "        .withWatermark(\"event_timestamp\", \"6 hours\")\n",
    "    )\n",
    "# Note: Lakeflow handles merge logic automatically for streaming tables\n",
    "```\n",
    "\n",
    "### Pattern 4: Windowed Aggregation\n",
    "\n",
    "```python\n",
    "@dp.streaming_table(\n",
    "    name=\"hourly_metrics\",\n",
    "    comment=\"Hourly aggregated metrics\"\n",
    ")\n",
    "def hourly_metrics():\n",
    "    \"\"\"\n",
    "    Time-windowed aggregations.\n",
    "    Use when:\n",
    "    - Need time-based summaries\n",
    "    - Can tolerate watermark delay\n",
    "    - Want incremental computation\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"events\")\n",
    "        .withWatermark(\"event_timestamp\", \"1 hour\")\n",
    "        .groupBy(\n",
    "            F.window(\"event_timestamp\", \"1 hour\"),\n",
    "            \"category\"\n",
    "        )\n",
    "        .agg(\n",
    "            F.sum(\"revenue\").alias(\"total_revenue\"),\n",
    "            F.count(\"*\").alias(\"event_count\")\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"window.start\").alias(\"hour\"),\n",
    "            \"category\",\n",
    "            \"total_revenue\",\n",
    "            \"event_count\"\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "### Pattern 5: Sessionization\n",
    "\n",
    "```python\n",
    "@dp.streaming_table(\n",
    "    name=\"user_sessions\",\n",
    "    comment=\"User sessions with 30-minute timeout\"\n",
    ")\n",
    "def user_sessions():\n",
    "    \"\"\"\n",
    "    Group events into sessions based on inactivity timeout.\n",
    "    Use when:\n",
    "    - Need to track user sessions\n",
    "    - Have inactivity timeout definition\n",
    "    - Can maintain session state\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"events\")\n",
    "        .withWatermark(\"event_timestamp\", \"1 hour\")\n",
    "        .groupBy(\n",
    "            \"user_id\",\n",
    "            F.session_window(\"event_timestamp\", \"30 minutes\")  # Session window\n",
    "        )\n",
    "        .agg(\n",
    "            F.min(\"event_timestamp\").alias(\"session_start\"),\n",
    "            F.max(\"event_timestamp\").alias(\"session_end\"),\n",
    "            F.count(\"*\").alias(\"events_in_session\"),\n",
    "            F.collect_list(\"page\").alias(\"pages_visited\")\n",
    "        )\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Optimization for Streaming\n",
    "\n",
    "### Trigger Intervals\n",
    "\n",
    "```python\n",
    "# Lakeflow manages triggers automatically, but understanding helps:\n",
    "\n",
    "# Micro-batch (default): Process new data every few seconds\n",
    "# - Good for: Most streaming workloads\n",
    "# - Latency: Seconds\n",
    "# - Throughput: Balanced\n",
    "\n",
    "# Continuous: Process with minimal latency\n",
    "# - Good for: Ultra-low latency requirements\n",
    "# - Latency: Milliseconds\n",
    "# - Throughput: Lower (experimental)\n",
    "```\n",
    "\n",
    "### Optimize Shuffle Operations\n",
    "\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def optimized_aggregation():\n",
    "    \"\"\"\n",
    "    Minimize shuffle by:\n",
    "    1. Using appropriate partition keys\n",
    "    2. Setting shuffle partitions\n",
    "    3. Enabling AQE (Adaptive Query Execution)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"events\")\n",
    "        .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "        .groupBy(\n",
    "            F.window(\"timestamp\", \"5 minutes\"),\n",
    "            \"user_id\"  # Good partition key (high cardinality)\n",
    "        )\n",
    "        .agg(F.sum(\"amount\"))\n",
    "    )\n",
    "# Spark configs:\n",
    "# spark.sql.shuffle.partitions=200  # Adjust based on data volume\n",
    "# spark.sql.adaptive.enabled=true   # Enable AQE\n",
    "```\n",
    "\n",
    "### File Compaction\n",
    "\n",
    "```python\n",
    "@dp.streaming_table(\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",  # Right-size files on write\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"      # Auto-compact small files\n",
    "    }\n",
    ")\n",
    "def optimized_stream():\n",
    "    \"\"\"\n",
    "    Streaming tables can generate many small files.\n",
    "    Auto-optimization prevents small file problems.\n",
    "    \"\"\"\n",
    "    return spark.readStream.table(\"raw.events\")\n",
    "```\n",
    "\n",
    "### Monitoring Streaming Performance\n",
    "\n",
    "```python\n",
    "# Key metrics to monitor:\n",
    "# 1. Input rate: Records/second received\n",
    "# 2. Processing rate: Records/second processed\n",
    "# 3. Batch duration: Time to process each micro-batch\n",
    "# 4. State size: Memory used for stateful operations\n",
    "# 5. Watermark lag: How far behind watermark is\n",
    "\n",
    "# Access via Spark UI → Streaming tab\n",
    "# Or programmatically:\n",
    "# query.lastProgress  # Last batch statistics\n",
    "# query.status        # Current query status\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored streaming tables and incremental processing in Lakeflow:\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Streaming Tables (`@dp.streaming_table`)**\n",
    "   - Real-time processing with continuous incremental execution\n",
    "   - Automatic checkpoint management by Lakeflow\n",
    "   - Exactly-once processing guarantees\n",
    "\n",
    "2. **Checkpointing**\n",
    "   - Automatic state management for fault tolerance\n",
    "   - Recovery from failures without data loss\n",
    "   - Exactly-once semantics with transactional writes\n",
    "\n",
    "3. **Watermarking**\n",
    "   - Handling late-arriving and out-of-order data\n",
    "   - Bounded state management for aggregations\n",
    "   - Configurable lateness tolerance\n",
    "\n",
    "4. **Stateful Operations**\n",
    "   - Windowed aggregations with time windows\n",
    "   - Deduplication patterns\n",
    "   - Stream-to-stream joins\n",
    "   - Session windows and user activity tracking\n",
    "\n",
    "5. **Stream-to-Batch Joins**\n",
    "   - Enriching streams with dimension tables\n",
    "   - Performance optimization with broadcast joins\n",
    "   - Handling dimension table updates\n",
    "\n",
    "6. **Incremental Processing Patterns**\n",
    "   - Simple append for log data\n",
    "   - Deduplication for idempotency\n",
    "   - Merge patterns for updates\n",
    "   - Windowed aggregations for metrics\n",
    "   - Sessionization for user tracking\n",
    "\n",
    "7. **Performance Optimization**\n",
    "   - Trigger interval configuration\n",
    "   - Shuffle optimization strategies\n",
    "   - File compaction with Delta Lake\n",
    "   - Monitoring streaming metrics\n",
    "\n",
    "### Functional Programming Benefits\n",
    "\n",
    "- **Pure Functions**: Transformation logic remains pure, side effects in actions\n",
    "- **Composition**: Stream transformations compose functionally\n",
    "- **Immutability**: Streaming DataFrames are immutable\n",
    "- **Declarative**: Define what to compute, Spark handles how\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "✅ Always use watermarks for stateful operations\n",
    "✅ Choose appropriate watermark duration for data characteristics\n",
    "✅ Use broadcast joins for small dimension tables\n",
    "✅ Monitor state size and batch processing time\n",
    "✅ Enable auto-optimization for streaming tables\n",
    "✅ Extract transformation logic into testable pure functions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **6.4**: Data quality with expectations in Lakeflow\n",
    "- **6.5**: Advanced flows and CDC patterns\n",
    "- **6.6**: Best practices and anti-patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Practice streaming table patterns:\n",
    "\n",
    "**Exercise 1: Basic Streaming Table**\n",
    "- Define a streaming table reading from a Delta source\n",
    "- Add simple transformations (filter, select)\n",
    "- Observe checkpoint creation and management\n",
    "\n",
    "**Exercise 2: Watermark Configuration**\n",
    "- Create windowed aggregation with watermark\n",
    "- Experiment with different watermark durations\n",
    "- Monitor state size and late data handling\n",
    "\n",
    "**Exercise 3: Stream Enrichment**\n",
    "- Join streaming events with batch dimension table\n",
    "- Implement broadcast join optimization\n",
    "- Compare performance with and without broadcast\n",
    "\n",
    "**Exercise 4: Deduplication Pattern**\n",
    "- Implement deduplication on streaming data\n",
    "- Test with synthetic duplicate records\n",
    "- Measure deduplication effectiveness\n",
    "\n",
    "**Exercise 5: Session Windows**\n",
    "- Create user session tracking with session windows\n",
    "- Define appropriate session timeout\n",
    "- Calculate session metrics (duration, page views)\n",
    "\n",
    "**Exercise 6: Performance Tuning**\n",
    "- Profile a streaming workload\n",
    "- Identify bottlenecks (shuffle, state, I/O)\n",
    "- Apply optimization strategies\n",
    "- Measure performance improvements\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
