{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Data Quality with Expectations in Lakeflow\n",
    "\n",
    "This notebook demonstrates how to implement declarative data quality using expectations in Lakeflow Declarative Pipelines. We'll explore the migration from legacy Delta Live Tables (`dlt`) expectations to modern `pyspark.pipelines` expectations, implement layered quality strategies, and build composable quality rules following functional programming principles.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Migrate from `@dlt.expect*` to `@dp.expect*` decorators\n",
    "- Implement three expectation strategies: WARN, DROP, and FAIL\n",
    "- Design layered quality strategies (Bronze → Silver → Gold)\n",
    "- Create composable and reusable expectation patterns\n",
    "- Monitor quality metrics and track violations\n",
    "- Test expectations before production deployment\n",
    "- Apply functional programming to data quality validation\n",
    "- Build declarative quality rules that align with business requirements\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Notebooks 6.1, 6.2, and 6.3\n",
    "- Understanding of Delta Lake and data quality concepts\n",
    "- Familiarity with SQL expressions and constraints\n",
    "- Knowledge of functional programming principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform setup detection\n",
    "# In Databricks: Keep commented\n",
    "# In Local: Uncomment this line\n",
    "# %run 00_Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from typing import Dict, List, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# In a real Lakeflow pipeline:\n",
    "# from pyspark import pipelines as dp\n",
    "\n",
    "print(\"✅ Imports complete - Ready for expectations demonstration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Migration from DLT to Lakeflow Expectations\n",
    "\n",
    "### Legacy DLT Expectations\n",
    "\n",
    "**Before (Delta Live Tables)**:\n",
    "```python\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# WARN: Monitor violations without blocking\n",
    "@dlt.table\n",
    "@dlt.expect(\"valid_email\", \"email IS NOT NULL\")\n",
    "def customers_bronze():\n",
    "    return spark.table(\"raw.customers\")\n",
    "\n",
    "# DROP: Remove violating records\n",
    "@dlt.table\n",
    "@dlt.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")\n",
    "def customers_silver():\n",
    "    return dlt.read(\"customers_bronze\")\n",
    "\n",
    "# FAIL: Stop pipeline on violation\n",
    "@dlt.table\n",
    "@dlt.expect_or_fail(\"unique_id\", \"customer_id IS NOT NULL\")\n",
    "def customers_gold():\n",
    "    return dlt.read(\"customers_silver\")\n",
    "```\n",
    "\n",
    "### Modern Lakeflow Expectations\n",
    "\n",
    "**After (Lakeflow Declarative Pipelines)**:\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# WARN: Monitor violations without blocking\n",
    "@dp.table\n",
    "@dp.expect(\"valid_email\", \"email IS NOT NULL\")\n",
    "def customers_bronze():\n",
    "    return spark.table(\"raw.customers\")\n",
    "\n",
    "# DROP: Remove violating records\n",
    "@dp.table\n",
    "@dp.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")\n",
    "def customers_silver():\n",
    "    return dp.read(\"customers_bronze\")\n",
    "\n",
    "# FAIL: Stop pipeline on violation\n",
    "@dp.table\n",
    "@dp.expect_or_fail(\"unique_id\", \"customer_id IS NOT NULL\")\n",
    "def customers_gold():\n",
    "    return dp.read(\"customers_silver\")\n",
    "```\n",
    "\n",
    "### Migration Changes\n",
    "\n",
    "| Legacy DLT | Modern Lakeflow | Notes |\n",
    "|------------|-----------------|-------|\n",
    "| `import dlt` | `from pyspark import pipelines as dp` | Module import change |\n",
    "| `@dlt.expect(...)` | `@dp.expect(...)` | WARN strategy |\n",
    "| `@dlt.expect_or_drop(...)` | `@dp.expect_or_drop(...)` | DROP strategy |\n",
    "| `@dlt.expect_or_fail(...)` | `@dp.expect_or_fail(...)` | FAIL strategy |\n",
    "| `dlt.read(\"table\")` | `dp.read(\"table\")` | Reading dependencies |\n",
    "\n",
    "**Migration is straightforward**: Simply replace `dlt` with `dp` throughout your pipeline code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Three Expectation Strategies\n",
    "\n",
    "### Strategy 1: WARN (`@dp.expect`)\n",
    "\n",
    "**Purpose**: Monitor quality issues without impacting data flow\n",
    "\n",
    "**Behavior**:\n",
    "- Records violations in quality metrics\n",
    "- Does NOT drop records\n",
    "- Does NOT fail pipeline\n",
    "- All data passes through unchanged\n",
    "\n",
    "**Use Cases**:\n",
    "- Monitoring data quality trends\n",
    "- Non-critical quality checks\n",
    "- Exploratory data quality assessment\n",
    "- Gradual quality improvement tracking\n",
    "\n",
    "```python\n",
    "@dp.table(\n",
    "    name=\"bronze_events\",\n",
    "    comment=\"Raw events with quality monitoring\"\n",
    ")\n",
    "@dp.expect(\"valid_email_format\", \n",
    "           \"email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'\")\n",
    "@dp.expect(\"valid_timestamp_format\",\n",
    "           \"timestamp RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}'\")\n",
    "@dp.expect(\"reasonable_amount\",\n",
    "           \"amount >= 0 AND amount < 1000000\")\n",
    "def bronze_events():\n",
    "    \"\"\"\n",
    "    Bronze layer: Monitor quality, don't block ingestion.\n",
    "    All records stored, violations tracked for analysis.\n",
    "    \"\"\"\n",
    "    return spark.table(\"raw.events\")\n",
    "\n",
    "# Result:\n",
    "# - All records ingested (even with violations)\n",
    "# - Quality metrics show violation rates\n",
    "# - Can analyze patterns in violations\n",
    "```\n",
    "\n",
    "### Strategy 2: DROP (`@dp.expect_or_drop`)\n",
    "\n",
    "**Purpose**: Enforce quality by removing violating records\n",
    "\n",
    "**Behavior**:\n",
    "- Filters out records that violate constraint\n",
    "- Continues processing with valid records\n",
    "- Tracks drop rate in metrics\n",
    "- Pipeline does NOT fail\n",
    "\n",
    "**Use Cases**:\n",
    "- Enforcing business rules\n",
    "- Data cleansing in Silver layer\n",
    "- Removing clearly invalid data\n",
    "- Preventing downstream contamination\n",
    "\n",
    "```python\n",
    "@dp.table(\n",
    "    name=\"silver_customers\",\n",
    "    comment=\"Cleaned customers with enforced quality\"\n",
    ")\n",
    "@dp.expect_or_drop(\"valid_age_range\",\n",
    "                    \"age >= 18 AND age <= 120\")\n",
    "@dp.expect_or_drop(\"valid_country_code\",\n",
    "                    \"country IN ('US', 'CA', 'UK', 'AU', 'DE', 'FR', 'JP')\")\n",
    "@dp.expect_or_drop(\"positive_account_balance\",\n",
    "                    \"account_balance >= 0\")\n",
    "@dp.expect_or_drop(\"non_null_email\",\n",
    "                    \"email IS NOT NULL AND length(email) > 0\")\n",
    "def silver_customers():\n",
    "    \"\"\"\n",
    "    Silver layer: Remove invalid records.\n",
    "    Only valid customers proceed to downstream tables.\n",
    "    \"\"\"\n",
    "    return dp.read(\"bronze_customers\")\n",
    "\n",
    "# Result:\n",
    "# - Invalid records dropped automatically\n",
    "# - Pipeline continues with clean data\n",
    "# - Metrics show how many dropped\n",
    "```\n",
    "\n",
    "### Strategy 3: FAIL (`@dp.expect_or_fail`)\n",
    "\n",
    "**Purpose**: Critical quality gates that must pass\n",
    "\n",
    "**Behavior**:\n",
    "- Stops entire pipeline execution if ANY record violates\n",
    "- No data written on violation\n",
    "- Provides clear error message\n",
    "- Requires manual intervention to fix\n",
    "\n",
    "**Use Cases**:\n",
    "- Critical data integrity checks\n",
    "- Unique key constraints\n",
    "- Required fields for downstream systems\n",
    "- Preventing data corruption\n",
    "\n",
    "```python\n",
    "@dp.table(\n",
    "    name=\"gold_customers\",\n",
    "    comment=\"Production-ready customers with strict quality gates\"\n",
    ")\n",
    "@dp.expect_or_fail(\"unique_customer_id\",\n",
    "                    \"customer_id IS NOT NULL\")\n",
    "@dp.expect_or_fail(\"required_fields_present\",\n",
    "                    \"customer_id IS NOT NULL AND name IS NOT NULL AND email IS NOT NULL\")\n",
    "@dp.expect_or_fail(\"no_future_dates\",\n",
    "                    \"signup_date <= current_date()\")\n",
    "def gold_customers():\n",
    "    \"\"\"\n",
    "    Gold layer: Fail fast on critical violations.\n",
    "    Guarantees downstream systems receive valid data.\n",
    "    \"\"\"\n",
    "    return dp.read(\"silver_customers\")\n",
    "\n",
    "# Result:\n",
    "# - ANY violation stops pipeline\n",
    "# - No partial data written\n",
    "# - Alert triggers for investigation\n",
    "```\n",
    "\n",
    "### Strategy Decision Matrix\n",
    "\n",
    "| Criteria | WARN | DROP | FAIL |\n",
    "|----------|------|------|------|\n",
    "| **Impact** | None | Removes records | Stops pipeline |\n",
    "| **Data Flow** | All data passes | Only valid data | No data on violation |\n",
    "| **Use Case** | Monitoring | Cleansing | Critical gates |\n",
    "| **Typical Layer** | Bronze | Silver | Gold |\n",
    "| **Business Tolerance** | High | Medium | Zero |\n",
    "| **Example** | Email format | Age range | Unique ID |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layered Quality Strategy: Bronze → Silver → Gold\n",
    "\n",
    "### Architecture Pattern\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                  LAYERED QUALITY STRATEGY                    │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                              │\n",
    "│  RAW DATA                                                    │\n",
    "│     ↓                                                        │\n",
    "│  ┌──────────────────────────────────────────────┐           │\n",
    "│  │ BRONZE: Monitor (WARN)                       │           │\n",
    "│  │ • Ingest all data                            │           │\n",
    "│  │ • Track quality issues                       │           │\n",
    "│  │ • No data loss                               │           │\n",
    "│  │ • Complete audit trail                       │           │\n",
    "│  └──────────────────────────────────────────────┘           │\n",
    "│     ↓                                                        │\n",
    "│  ┌──────────────────────────────────────────────┐           │\n",
    "│  │ SILVER: Cleanse (DROP)                       │           │\n",
    "│  │ • Remove invalid records                     │           │\n",
    "│  │ • Apply business rules                       │           │\n",
    "│  │ • Standardize formats                        │           │\n",
    "│  │ • Track drop rates                           │           │\n",
    "│  └──────────────────────────────────────────────┘           │\n",
    "│     ↓                                                        │\n",
    "│  ┌──────────────────────────────────────────────┐           │\n",
    "│  │ GOLD: Enforce (FAIL)                         │           │\n",
    "│  │ • Strict quality gates                       │           │\n",
    "│  │ • Production guarantees                      │           │\n",
    "│  │ • Zero tolerance for violations              │           │\n",
    "│  │ • Fail fast on issues                        │           │\n",
    "│  └──────────────────────────────────────────────┘           │\n",
    "│     ↓                                                        │\n",
    "│  BUSINESS APPLICATIONS                                       │\n",
    "│                                                              │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Complete Example: Customer Pipeline\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "# BRONZE LAYER: Monitor quality, ingest everything\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "@dp.table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Raw customer data with quality monitoring\"\n",
    ")\n",
    "@dp.expect(\"has_email\", \"email IS NOT NULL\")\n",
    "@dp.expect(\"has_name\", \"name IS NOT NULL\")\n",
    "@dp.expect(\"email_format_looks_valid\",\n",
    "           \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+$'\")\n",
    "@dp.expect(\"date_format_valid\",\n",
    "           \"signup_date RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\")\n",
    "def bronze_customers():\n",
    "    \"\"\"\n",
    "    Bronze: Complete data ingestion with monitoring.\n",
    "    - All records stored (even invalid)\n",
    "    - Quality issues tracked for root cause analysis\n",
    "    - Provides audit trail\n",
    "    \"\"\"\n",
    "    return spark.table(\"raw.customers\")\n",
    "\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "# SILVER LAYER: Clean and standardize\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "@dp.table(\n",
    "    name=\"silver_customers\",\n",
    "    comment=\"Cleaned customers with enforced business rules\"\n",
    ")\n",
    "@dp.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 120\")\n",
    "@dp.expect_or_drop(\"valid_country\",\n",
    "                    \"country IN ('US', 'CA', 'UK', 'AU', 'DE', 'FR', 'JP', 'CN', 'IN', 'BR')\")\n",
    "@dp.expect_or_drop(\"valid_tier\",\n",
    "                    \"tier IN ('Free', 'Basic', 'Premium', 'Enterprise')\")\n",
    "@dp.expect_or_drop(\"non_negative_balance\",\n",
    "                    \"account_balance >= 0\")\n",
    "@dp.expect_or_drop(\"valid_email_format\",\n",
    "                    \"email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'\")\n",
    "def silver_customers():\n",
    "    \"\"\"\n",
    "    Silver: Business rule enforcement.\n",
    "    - Invalid records dropped\n",
    "    - Data standardized\n",
    "    - Clean dataset for analytics\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read(\"bronze_customers\")\n",
    "        .withColumn(\"country\", F.upper(F.trim(F.col(\"country\"))))  # Standardize\n",
    "        .withColumn(\"email\", F.lower(F.trim(F.col(\"email\"))))      # Normalize\n",
    "        .withColumn(\"signup_date\", F.to_date(\"signup_date\"))       # Parse date\n",
    "    )\n",
    "\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "# GOLD LAYER: Production guarantees\n",
    "# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "@dp.table(\n",
    "    name=\"gold_customers\",\n",
    "    comment=\"Production-ready customers with strict quality gates\"\n",
    ")\n",
    "@dp.expect_or_fail(\"customer_id_present\",\n",
    "                    \"customer_id IS NOT NULL\")\n",
    "@dp.expect_or_fail(\"required_fields_complete\",\n",
    "                    \"customer_id IS NOT NULL AND name IS NOT NULL AND email IS NOT NULL\")\n",
    "@dp.expect_or_fail(\"no_duplicate_ids\",\n",
    "                    \"customer_id IS NOT NULL\")  # Uniqueness enforced at table level\n",
    "@dp.expect(\"high_quality_emails\",  # WARN in gold for monitoring\n",
    "           \"email LIKE '%@gmail.com' OR email LIKE '%@yahoo.com' OR email LIKE '%@company.com'\")\n",
    "def gold_customers():\n",
    "    \"\"\"\n",
    "    Gold: Zero-defect production data.\n",
    "    - Critical fields guaranteed non-null\n",
    "    - Deduplication applied\n",
    "    - Safe for downstream consumption\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read(\"silver_customers\")\n",
    "        .dropDuplicates([\"customer_id\"])  # Ensure uniqueness\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            \"name\",\n",
    "            \"email\",\n",
    "            \"age\",\n",
    "            \"country\",\n",
    "            \"tier\",\n",
    "            \"account_balance\",\n",
    "            \"signup_date\"\n",
    "        )\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Composable and Reusable Expectations\n",
    "\n",
    "### Pattern 1: Standard Expectation Library\n",
    "\n",
    "```python\n",
    "# Define reusable expectation configurations\n",
    "class StandardExpectations:\n",
    "    \"\"\"Library of reusable expectation definitions\"\"\"\n",
    "    \n",
    "    # Email validation\n",
    "    EMAIL_FORMAT = (\n",
    "        \"valid_email_format\",\n",
    "        \"email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'\"\n",
    "    )\n",
    "    \n",
    "    # Phone validation (US format)\n",
    "    PHONE_FORMAT = (\n",
    "        \"valid_phone_format\",\n",
    "        \"phone RLIKE '^\\\\+?1?[0-9]{10}$'\"\n",
    "    )\n",
    "    \n",
    "    # Age range\n",
    "    ADULT_AGE = (\n",
    "        \"adult_age_requirement\",\n",
    "        \"age >= 18 AND age <= 120\"\n",
    "    )\n",
    "    \n",
    "    # Non-negative amounts\n",
    "    POSITIVE_AMOUNT = (\n",
    "        \"positive_monetary_amount\",\n",
    "        \"amount >= 0 AND amount < 1000000000\"\n",
    "    )\n",
    "    \n",
    "    # Date not in future\n",
    "    PAST_DATE = (\n",
    "        \"date_not_future\",\n",
    "        \"date_column <= current_date()\"\n",
    "    )\n",
    "    \n",
    "    # Non-null required field\n",
    "    @staticmethod\n",
    "    def required_field(field_name: str):\n",
    "        return (\n",
    "            f\"{field_name}_required\",\n",
    "            f\"{field_name} IS NOT NULL AND length({field_name}) > 0\"\n",
    "        )\n",
    "    \n",
    "    # Value in allowed list\n",
    "    @staticmethod\n",
    "    def allowed_values(field_name: str, values: List[str]):\n",
    "        values_str = \"', '\".join(values)\n",
    "        return (\n",
    "            f\"{field_name}_allowed_values\",\n",
    "            f\"{field_name} IN ('{values_str}')\"\n",
    "        )\n",
    "\n",
    "# Use standard expectations\n",
    "@dp.table\n",
    "@dp.expect_or_drop(*StandardExpectations.EMAIL_FORMAT)\n",
    "@dp.expect_or_drop(*StandardExpectations.ADULT_AGE)\n",
    "@dp.expect_or_drop(*StandardExpectations.required_field(\"customer_id\"))\n",
    "@dp.expect_or_drop(*StandardExpectations.allowed_values(\n",
    "    \"country\", [\"US\", \"CA\", \"UK\", \"AU\"]\n",
    "))\n",
    "def validated_customers():\n",
    "    return dp.read(\"raw_customers\")\n",
    "```\n",
    "\n",
    "### Pattern 2: Expectation Builders\n",
    "\n",
    "```python\n",
    "from typing import Tuple\n",
    "\n",
    "class ExpectationBuilder:\n",
    "    \"\"\"Functional builder for creating expectations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def range_check(field: str, min_val: float, max_val: float) -> Tuple[str, str]:\n",
    "        \"\"\"Create range validation expectation\"\"\"\n",
    "        return (\n",
    "            f\"{field}_range_check\",\n",
    "            f\"{field} >= {min_val} AND {field} <= {max_val}\"\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def not_null(field: str) -> Tuple[str, str]:\n",
    "        \"\"\"Create not-null expectation\"\"\"\n",
    "        return (\n",
    "            f\"{field}_not_null\",\n",
    "            f\"{field} IS NOT NULL\"\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def regex_match(field: str, pattern: str, description: str) -> Tuple[str, str]:\n",
    "        \"\"\"Create regex validation expectation\"\"\"\n",
    "        return (\n",
    "            f\"{field}_{description}\",\n",
    "            f\"{field} RLIKE '{pattern}'\"\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def in_list(field: str, values: List[str]) -> Tuple[str, str]:\n",
    "        \"\"\"Create enumeration expectation\"\"\"\n",
    "        values_str = \"', '\".join(values)\n",
    "        return (\n",
    "            f\"{field}_in_allowed_list\",\n",
    "            f\"{field} IN ('{values_str}')\"\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def date_range(field: str, start_date: str, end_date: str) -> Tuple[str, str]:\n",
    "        \"\"\"Create date range expectation\"\"\"\n",
    "        return (\n",
    "            f\"{field}_date_range\",\n",
    "            f\"{field} >= '{start_date}' AND {field} <= '{end_date}'\"\n",
    "        )\n",
    "\n",
    "# Usage\n",
    "@dp.table\n",
    "@dp.expect_or_drop(*ExpectationBuilder.range_check(\"age\", 18, 120))\n",
    "@dp.expect_or_drop(*ExpectationBuilder.not_null(\"customer_id\"))\n",
    "@dp.expect_or_drop(*ExpectationBuilder.regex_match(\n",
    "    \"email\",\n",
    "    \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\",\n",
    "    \"email_format\"\n",
    "))\n",
    "@dp.expect_or_drop(*ExpectationBuilder.in_list(\n",
    "    \"tier\", [\"Free\", \"Basic\", \"Premium\"]\n",
    "))\n",
    "def customers_with_validations():\n",
    "    return dp.read(\"raw_customers\")\n",
    "```\n",
    "\n",
    "### Pattern 3: Domain-Specific Expectation Sets\n",
    "\n",
    "```python\n",
    "class CustomerExpectations:\n",
    "    \"\"\"Customer-specific expectation patterns\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def bronze_layer():\n",
    "        \"\"\"Expectations for bronze customer data\"\"\"\n",
    "        return [\n",
    "            (\"has_id\", \"customer_id IS NOT NULL\"),\n",
    "            (\"has_email\", \"email IS NOT NULL\"),\n",
    "            (\"has_name\", \"name IS NOT NULL\"),\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def silver_layer():\n",
    "        \"\"\"Expectations for silver customer data\"\"\"\n",
    "        return [\n",
    "            (\"valid_age\", \"age >= 18 AND age <= 120\"),\n",
    "            (\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+$'\"),\n",
    "            (\"valid_tier\", \"tier IN ('Free', 'Basic', 'Premium', 'Enterprise')\"),\n",
    "            (\"positive_balance\", \"account_balance >= 0\"),\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def gold_layer():\n",
    "        \"\"\"Expectations for gold customer data\"\"\"\n",
    "        return [\n",
    "            (\"id_required\", \"customer_id IS NOT NULL\"),\n",
    "            (\"complete_profile\", \n",
    "             \"customer_id IS NOT NULL AND name IS NOT NULL AND email IS NOT NULL\"),\n",
    "        ]\n",
    "\n",
    "# Apply expectation sets\n",
    "@dp.table\n",
    "def bronze_customers():\n",
    "    return spark.table(\"raw.customers\")\n",
    "\n",
    "# Dynamically apply expectations\n",
    "for name, constraint in CustomerExpectations.bronze_layer():\n",
    "    bronze_customers = dp.expect(name, constraint)(bronze_customers)\n",
    "\n",
    "@dp.table\n",
    "def silver_customers():\n",
    "    return dp.read(\"bronze_customers\")\n",
    "\n",
    "for name, constraint in CustomerExpectations.silver_layer():\n",
    "    silver_customers = dp.expect_or_drop(name, constraint)(silver_customers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Metrics and Monitoring\n",
    "\n",
    "### What Metrics Are Tracked?\n",
    "\n",
    "Lakeflow automatically collects quality metrics for each expectation:\n",
    "\n",
    "```python\n",
    "# Metrics collected automatically:\n",
    "{\n",
    "    \"expectation_name\": \"valid_email\",\n",
    "    \"dataset\": \"silver_customers\",\n",
    "    \"passed_records\": 9850,\n",
    "    \"failed_records\": 150,\n",
    "    \"total_records\": 10000,\n",
    "    \"pass_rate\": 0.985,\n",
    "    \"fail_rate\": 0.015,\n",
    "    \"action\": \"drop\",  # or \"warn\" or \"fail\"\n",
    "    \"timestamp\": \"2024-10-28T12:30:00Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Accessing Quality Metrics\n",
    "\n",
    "```python\n",
    "# In Databricks, metrics available via:\n",
    "# 1. Pipeline UI (Data Quality tab)\n",
    "# 2. Event logs (system.lakeflow.events table)\n",
    "# 3. Query the metrics directly\n",
    "\n",
    "# Example: Query quality metrics\n",
    "quality_metrics = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        expectation_name,\n",
    "        dataset,\n",
    "        SUM(failed_records) as total_failures,\n",
    "        AVG(fail_rate) as avg_fail_rate,\n",
    "        MAX(timestamp) as last_check\n",
    "    FROM system.lakeflow.event_log\n",
    "    WHERE event_type = 'expectation_result'\n",
    "    GROUP BY expectation_name, dataset\n",
    "    HAVING avg_fail_rate > 0.01  -- Alert if >1% failure\n",
    "    ORDER BY avg_fail_rate DESC\n",
    "\"\"\")\n",
    "\n",
    "quality_metrics.display()\n",
    "```\n",
    "\n",
    "### Setting Up Alerts\n",
    "\n",
    "```python\n",
    "# Monitor expectations and alert on issues\n",
    "@dp.table\n",
    "@dp.expect(\"critical_quality_check\", \"amount > 0\")\n",
    "def monitored_transactions():\n",
    "    return dp.read(\"raw_transactions\")\n",
    "\n",
    "# Configure alerts (in Lakeflow UI or via API):\n",
    "# - Alert if fail_rate > 5%\n",
    "# - Alert if failed_records > 1000\n",
    "# - Alert on any FAIL expectation violation\n",
    "# - Send to: Slack, Email, PagerDuty, etc.\n",
    "```\n",
    "\n",
    "### Quality Dashboards\n",
    "\n",
    "```python\n",
    "# Create quality monitoring dashboard\n",
    "@dp.materialized_view(\n",
    "    name=\"quality_dashboard\",\n",
    "    comment=\"Real-time quality metrics for monitoring\"\n",
    ")\n",
    "def quality_dashboard():\n",
    "    \"\"\"\n",
    "    Aggregated quality metrics across all expectations.\n",
    "    Refreshed on each pipeline run.\n",
    "    \"\"\"\n",
    "    return spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            dataset,\n",
    "            expectation_name,\n",
    "            action,\n",
    "            COUNT(*) as check_count,\n",
    "            SUM(total_records) as total_records_checked,\n",
    "            SUM(failed_records) as total_failures,\n",
    "            AVG(fail_rate) as avg_failure_rate,\n",
    "            MAX(fail_rate) as max_failure_rate,\n",
    "            MIN(timestamp) as first_check,\n",
    "            MAX(timestamp) as last_check\n",
    "        FROM system.lakeflow.event_log\n",
    "        WHERE event_type = 'expectation_result'\n",
    "          AND timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "        GROUP BY dataset, expectation_name, action\n",
    "        ORDER BY avg_failure_rate DESC\n",
    "    \"\"\")\n",
    "\n",
    "# Visualize in dashboards (Databricks SQL, PowerBI, Tableau, etc.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Expectations Before Deployment\n",
    "\n",
    "### Pattern 1: Unit Testing Expectation Logic\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def test_age_expectation(spark: SparkSession):\n",
    "    \"\"\"Test age range expectation with known data\"\"\"\n",
    "    \n",
    "    # Create test data with known violations\n",
    "    test_data = spark.createDataFrame([\n",
    "        (1, \"Alice\", 25),    # Valid\n",
    "        (2, \"Bob\", 17),      # Invalid (too young)\n",
    "        (3, \"Carol\", 45),    # Valid\n",
    "        (4, \"David\", 150),   # Invalid (too old)\n",
    "        (5, \"Eve\", 30),      # Valid\n",
    "    ], [\"id\", \"name\", \"age\"])\n",
    "    \n",
    "    # Apply expectation constraint\n",
    "    constraint = \"age >= 18 AND age <= 120\"\n",
    "    result = test_data.filter(constraint)\n",
    "    \n",
    "    # Verify correct filtering\n",
    "    assert result.count() == 3  # Only 3 valid records\n",
    "    valid_ids = [row.id for row in result.collect()]\n",
    "    assert set(valid_ids) == {1, 3, 5}\n",
    "\n",
    "def test_email_format_expectation(spark: SparkSession):\n",
    "    \"\"\"Test email format validation\"\"\"\n",
    "    \n",
    "    test_data = spark.createDataFrame([\n",
    "        (1, \"alice@example.com\"),     # Valid\n",
    "        (2, \"invalid-email\"),          # Invalid\n",
    "        (3, \"bob@test.org\"),           # Valid\n",
    "        (4, \"@no-user.com\"),           # Invalid\n",
    "        (5, \"carol@company.co.uk\"),    # Valid\n",
    "    ], [\"id\", \"email\"])\n",
    "    \n",
    "    constraint = \"email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'\"\n",
    "    result = test_data.filter(constraint)\n",
    "    \n",
    "    assert result.count() == 3\n",
    "    valid_ids = [row.id for row in result.collect()]\n",
    "    assert set(valid_ids) == {1, 3, 5}\n",
    "```\n",
    "\n",
    "### Pattern 2: Integration Testing Full Pipeline\n",
    "\n",
    "```python\n",
    "def test_customer_pipeline_quality(spark: SparkSession):\n",
    "    \"\"\"Test complete pipeline with expectations\"\"\"\n",
    "    \n",
    "    # Create bronze data with various quality issues\n",
    "    bronze_data = spark.createDataFrame([\n",
    "        (1, \"Alice\", \"alice@example.com\", 25, \"US\", 1000.0),     # All valid\n",
    "        (2, \"Bob\", \"invalid\", 17, \"CA\", -100.0),                # Multiple issues\n",
    "        (3, \"Carol\", \"carol@test.com\", 45, \"UK\", 2000.0),       # All valid\n",
    "        (4, None, \"david@example.com\", 30, \"InvalidCountry\", 500.0),  # Invalid name and country\n",
    "        (5, \"Eve\", \"eve@example.com\", 150, \"AU\", 3000.0),       # Invalid age\n",
    "    ], [\"customer_id\", \"name\", \"email\", \"age\", \"country\", \"balance\"])\n",
    "    \n",
    "    # Simulate silver layer expectations\n",
    "    silver_data = (\n",
    "        bronze_data\n",
    "        .filter(\"age >= 18 AND age <= 120\")  # Drop invalid ages\n",
    "        .filter(\"country IN ('US', 'CA', 'UK', 'AU', 'DE')\")  # Drop invalid countries\n",
    "        .filter(\"balance >= 0\")  # Drop negative balances\n",
    "        .filter(\"name IS NOT NULL\")  # Drop null names\n",
    "    )\n",
    "    \n",
    "    # Verify results\n",
    "    assert silver_data.count() == 2  # Only Alice and Carol pass\n",
    "    valid_names = [row.name for row in silver_data.collect()]\n",
    "    assert set(valid_names) == {\"Alice\", \"Carol\"}\n",
    "```\n",
    "\n",
    "### Pattern 3: Expectation Coverage Testing\n",
    "\n",
    "```python\n",
    "def test_all_expectations_covered():\n",
    "    \"\"\"Ensure critical fields have expectations defined\"\"\"\n",
    "    \n",
    "    # Define required expectations\n",
    "    required_expectations = {\n",
    "        \"bronze_customers\": [\"has_id\", \"has_email\"],\n",
    "        \"silver_customers\": [\"valid_age\", \"valid_email\", \"valid_tier\"],\n",
    "        \"gold_customers\": [\"id_required\", \"complete_profile\"],\n",
    "    }\n",
    "    \n",
    "    # Verify expectations are defined\n",
    "    # (In real implementation, parse pipeline definition)\n",
    "    defined_expectations = get_defined_expectations()  # Custom function\n",
    "    \n",
    "    for table, expectations in required_expectations.items():\n",
    "        for expectation in expectations:\n",
    "            assert expectation in defined_expectations[table], \\\n",
    "                f\"Missing expectation '{expectation}' for table '{table}'\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored data quality with expectations in Lakeflow:\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Migration from DLT to Lakeflow**\n",
    "   - Simple replacement: `dlt` → `dp` throughout code\n",
    "   - Identical functionality with open-source foundation\n",
    "   - Backward compatibility maintained\n",
    "\n",
    "2. **Three Expectation Strategies**\n",
    "   - WARN (`@dp.expect`): Monitor without blocking\n",
    "   - DROP (`@dp.expect_or_drop`): Remove violating records\n",
    "   - FAIL (`@dp.expect_or_fail`): Stop pipeline on violations\n",
    "\n",
    "3. **Layered Quality Architecture**\n",
    "   - Bronze: WARN expectations for monitoring\n",
    "   - Silver: DROP expectations for cleansing\n",
    "   - Gold: FAIL expectations for guarantees\n",
    "\n",
    "4. **Composable Expectations**\n",
    "   - Standard expectation libraries\n",
    "   - Expectation builders for reusability\n",
    "   - Domain-specific expectation sets\n",
    "\n",
    "5. **Quality Metrics and Monitoring**\n",
    "   - Automatic metrics collection\n",
    "   - Quality dashboards and reports\n",
    "   - Alert configuration for violations\n",
    "\n",
    "6. **Testing Strategies**\n",
    "   - Unit testing expectation logic\n",
    "   - Integration testing full pipelines\n",
    "   - Coverage testing for completeness\n",
    "\n",
    "### Functional Programming Benefits\n",
    "\n",
    "- **Declarative**: Expectations describe desired quality state\n",
    "- **Composable**: Expectations stack on table definitions\n",
    "- **Immutable**: Expectations don't modify pipeline code\n",
    "- **Pure**: Constraint evaluation is deterministic\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "✅ Use layered strategy (WARN → DROP → FAIL)\n",
    "✅ Create reusable expectation libraries\n",
    "✅ Monitor metrics and set up alerts\n",
    "✅ Test expectations with synthetic data\n",
    "✅ Document business rules in expectation names\n",
    "✅ Start with WARN, tighten to DROP/FAIL gradually\n",
    "✅ Keep constraints simple and testable\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **6.5**: Flows and advanced CDC patterns\n",
    "- **6.6**: Best practices and anti-patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Practice implementing expectations:\n",
    "\n",
    "**Exercise 1: Basic Expectations**\n",
    "- Define a table with 3 WARN expectations\n",
    "- Add 2 DROP expectations for business rules\n",
    "- Include 1 FAIL expectation for critical field\n",
    "\n",
    "**Exercise 2: Layered Pipeline**\n",
    "- Create bronze/silver/gold pipeline\n",
    "- Apply appropriate expectation strategy at each layer\n",
    "- Track quality metrics across layers\n",
    "\n",
    "**Exercise 3: Reusable Library**\n",
    "- Build expectation library for your domain\n",
    "- Create at least 5 reusable expectations\n",
    "- Use them across multiple tables\n",
    "\n",
    "**Exercise 4: Quality Monitoring**\n",
    "- Set up quality metrics dashboard\n",
    "- Identify top 3 quality issues\n",
    "- Configure alerts for critical violations\n",
    "\n",
    "**Exercise 5: Testing Strategy**\n",
    "- Write unit tests for 3 expectations\n",
    "- Create integration test for pipeline\n",
    "- Verify expected drop rates\n",
    "\n",
    "**Exercise 6: Migration Practice**\n",
    "- Take existing DLT pipeline with expectations\n",
    "- Migrate to Lakeflow (`dlt` → `dp`)\n",
    "- Verify identical behavior\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
