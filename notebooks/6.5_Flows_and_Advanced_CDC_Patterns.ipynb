{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Flows and Advanced CDC Patterns\n",
    "\n",
    "This notebook explores advanced data integration patterns using Lakeflow flows and change data capture (CDC). We'll learn how to use `dp.append_flow()` for incremental loading, `dp.create_auto_cdc_flow()` for automatic change tracking, and implement Type 1 and Type 2 slowly changing dimensions following functional programming principles.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "- Use `dp.append_flow()` for efficient incremental data loading\n",
    "- Implement automatic CDC with `dp.create_auto_cdc_flow()`\n",
    "- Process CDC from snapshot tables with `dp.create_auto_cdc_from_snapshot_flow()`\n",
    "- Handle inserts, updates, and deletes declaratively\n",
    "- Implement Type 1 slowly changing dimensions (overwrite)\n",
    "- Build Type 2 slowly changing dimensions (history tracking)\n",
    "- Design merge strategies and handle conflicts\n",
    "- Apply functional programming to CDC workflows\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Notebooks 6.1-6.4\n",
    "- Understanding of change data capture concepts\n",
    "- Knowledge of slowly changing dimensions\n",
    "- Familiarity with Delta Lake merge operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform setup detection\n",
    "# In Databricks: Keep commented\n",
    "# In Local: Uncomment this line\n",
    "# %run 00_Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# In a real Lakeflow pipeline:\n",
    "# from pyspark import pipelines as dp\n",
    "\n",
    "print(\"✅ Imports complete - Ready for flows and CDC demonstration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Flows in Lakeflow\n",
    "\n",
    "### What Are Flows?\n",
    "\n",
    "**Flows** are high-level abstractions for common data integration patterns. They encapsulate complex logic into simple, declarative API calls.\n",
    "\n",
    "### Types of Flows\n",
    "\n",
    "| Flow Type | Purpose | Use Case |\n",
    "|-----------|---------|----------|\n",
    "| **append_flow** | Incremental append | Growing event logs, audit trails |\n",
    "| **create_auto_cdc_flow** | Automatic CDC | Database replication, real-time sync |\n",
    "| **create_auto_cdc_from_snapshot_flow** | CDC from snapshots | Daily dumps, batch CDC |\n",
    "| **create_sink** | Custom output | External system integration |\n",
    "\n",
    "### Flows vs Tables\n",
    "\n",
    "**Traditional Table Definition**:\n",
    "```python\n",
    "@dp.streaming_table\n",
    "def my_table():\n",
    "    return spark.readStream.table(\"source\")\n",
    "    # Manual logic for transformations\n",
    "```\n",
    "\n",
    "**Flow-Based Pattern**:\n",
    "```python\n",
    "dp.append_flow(\n",
    "    source=dp.read(\"source\"),\n",
    "    target=\"my_table\",\n",
    "    target_columns=[\"col1\", \"col2\", \"col3\"]\n",
    ")\n",
    "# Automatic append logic with deduplication\n",
    "```\n",
    "\n",
    "**Benefits of Flows**:\n",
    "- Less boilerplate code\n",
    "- Built-in best practices\n",
    "- Automatic optimization\n",
    "- Standardized patterns\n",
    "- Easier maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Incremental Loading with append_flow()\n",
    "\n",
    "### Basic Append Flow\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "# Source: New events arriving continuously\n",
    "@dp.streaming_table\n",
    "def bronze_events():\n",
    "    return spark.readStream.table(\"raw.events\")\n",
    "\n",
    "# Target: Append only new events to silver layer\n",
    "dp.append_flow(\n",
    "    source=dp.read(\"bronze_events\"),\n",
    "    target=\"silver_events\",\n",
    "    target_columns=[\n",
    "        \"event_id\",\n",
    "        \"event_type\",\n",
    "        \"user_id\",\n",
    "        \"timestamp\",\n",
    "        \"metadata\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# What append_flow does automatically:\n",
    "# 1. Reads new records from source\n",
    "# 2. Selects only specified columns\n",
    "# 3. Appends to target table\n",
    "# 4. No duplicates (based on source processing)\n",
    "# 5. Maintains order\n",
    "```\n",
    "\n",
    "### Append Flow with Transformations\n",
    "\n",
    "```python\n",
    "# Create source with transformations\n",
    "@dp.streaming_table\n",
    "def bronze_logs_transformed():\n",
    "    \"\"\"\n",
    "    Apply transformations before append.\n",
    "    Keep transformations pure and testable.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.application_logs\")\n",
    "        .withColumn(\n",
    "            \"log_timestamp\",\n",
    "            F.to_timestamp(\"timestamp_str\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"log_level\",\n",
    "            F.upper(F.col(\"level\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"processed_at\",\n",
    "            F.current_timestamp()\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Append transformed data\n",
    "dp.append_flow(\n",
    "    source=dp.read(\"bronze_logs_transformed\"),\n",
    "    target=\"silver_application_logs\",\n",
    "    target_columns=[\n",
    "        \"log_id\",\n",
    "        \"log_timestamp\",\n",
    "        \"log_level\",\n",
    "        \"message\",\n",
    "        \"application\",\n",
    "        \"processed_at\"\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### Append Flow for Batch Data\n",
    "\n",
    "```python\n",
    "# Batch source (daily files)\n",
    "@dp.table\n",
    "def daily_sales_batch():\n",
    "    \"\"\"Daily sales files loaded in batch\"\"\"\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"parquet\")\n",
    "        .load(\"/mnt/sales/daily/*.parquet\")\n",
    "    )\n",
    "\n",
    "# Append to historical sales\n",
    "dp.append_flow(\n",
    "    source=dp.read(\"daily_sales_batch\"),\n",
    "    target=\"historical_sales\",\n",
    "    target_columns=[\n",
    "        \"sale_id\",\n",
    "        \"sale_date\",\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"quantity\",\n",
    "        \"amount\"\n",
    "    ]\n",
    ")\n",
    "# Each pipeline run appends new daily data\n",
    "```\n",
    "\n",
    "### When to Use append_flow()\n",
    "\n",
    "**✅ Use append_flow for:**\n",
    "- Event logs (clickstreams, audit logs, IoT data)\n",
    "- Transaction records (orders, payments, shipments)\n",
    "- Time-series data (metrics, sensor readings)\n",
    "- Append-only data sources\n",
    "- Growing datasets without updates\n",
    "\n",
    "**❌ Don't use append_flow for:**\n",
    "- Data requiring updates (use CDC flows)\n",
    "- Dimension tables with changes (use SCD patterns)\n",
    "- Data with deletions (use merge operations)\n",
    "- Full snapshot replacements (use regular tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic CDC with create_auto_cdc_flow()\n",
    "\n",
    "### What is Change Data Capture (CDC)?\n",
    "\n",
    "CDC tracks changes (inserts, updates, deletes) in source systems and replicates them to target systems.\n",
    "\n",
    "**CDC Change Events:**\n",
    "```\n",
    "Operation | customer_id | name    | email              | _change_type\n",
    "----------|-------------|---------|--------------------|--------------\n",
    "INSERT    | 1           | Alice   | alice@example.com  | insert\n",
    "UPDATE    | 1           | Alice J | alice@example.com  | update\n",
    "DELETE    | 1           | Alice J | alice@example.com  | delete\n",
    "```\n",
    "\n",
    "### Basic CDC Flow\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "# Source: CDC stream from database\n",
    "@dp.streaming_table\n",
    "def bronze_customer_changes():\n",
    "    \"\"\"\n",
    "    CDC events from source database.\n",
    "    Must have: key columns, _change_type column, sequence column\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .table(\"raw.customer_cdc\")\n",
    "    )\n",
    "\n",
    "# Automatically apply CDC to maintain current state\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"bronze_customer_changes\",\n",
    "    target=\"silver_customers\",\n",
    "    keys=[\"customer_id\"],              # Primary key for matching\n",
    "    sequence_by=\"update_timestamp\",    # Order operations\n",
    "    stored_as_scd_type=1               # Type 1: overwrite on update\n",
    ")\n",
    "\n",
    "# What auto_cdc_flow does automatically:\n",
    "# 1. Reads CDC stream from source\n",
    "# 2. Applies inserts (new records)\n",
    "# 3. Applies updates (modified records)\n",
    "# 4. Applies deletes (removed records)\n",
    "# 5. Handles out-of-order events using sequence_by\n",
    "# 6. Deduplicates based on keys\n",
    "# 7. Maintains current state in target\n",
    "```\n",
    "\n",
    "### CDC with Change Type Column\n",
    "\n",
    "```python\n",
    "# CDC source must include _change_type or operation column\n",
    "@dp.streaming_table\n",
    "def order_cdc_stream():\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.orders_cdc\")\n",
    "        # Expected schema:\n",
    "        # - order_id (key)\n",
    "        # - customer_id, product_id, amount, status (data)\n",
    "        # - _change_type: 'insert', 'update', 'delete'\n",
    "        # - update_timestamp (sequence)\n",
    "    )\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"order_cdc_stream\",\n",
    "    target=\"current_orders\",\n",
    "    keys=[\"order_id\"],\n",
    "    sequence_by=\"update_timestamp\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Handling Multiple Keys\n",
    "\n",
    "```python\n",
    "# Composite key example\n",
    "@dp.streaming_table\n",
    "def order_line_items_cdc():\n",
    "    return spark.readStream.table(\"raw.order_line_items_cdc\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"order_line_items_cdc\",\n",
    "    target=\"current_order_line_items\",\n",
    "    keys=[\"order_id\", \"line_item_id\"],  # Composite key\n",
    "    sequence_by=\"last_updated\"\n",
    ")\n",
    "```\n",
    "\n",
    "### CDC Flow Parameters\n",
    "\n",
    "```python\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"source_table\",              # Source table name\n",
    "    target=\"target_table\",              # Target table name\n",
    "    keys=[\"id\"],                        # Primary/composite keys\n",
    "    sequence_by=\"updated_at\",           # Column for ordering changes\n",
    "    stored_as_scd_type=1,               # 1 or 2 (default: 1)\n",
    "    track_history_column_list=None,     # Columns to track (for SCD Type 2)\n",
    "    ignore_null_updates=False,          # Skip null value updates\n",
    "    apply_as_deletes=None,              # Custom delete condition\n",
    "    apply_as_truncates=None,            # Custom truncate condition\n",
    "    column_list=None,                   # Subset of columns to sync\n",
    "    except_column_list=None             # Columns to exclude\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CDC from Snapshots\n",
    "\n",
    "### Snapshot-Based CDC Pattern\n",
    "\n",
    "When source system provides full snapshots instead of change events:\n",
    "\n",
    "```\n",
    "Day 1 Snapshot:          Day 2 Snapshot:\n",
    "id | name  | status      id | name    | status\n",
    "1  | Alice | active      1  | Alice J | active  <- Updated\n",
    "2  | Bob   | active      2  | Bob     | inactive <- Updated\n",
    "                         3  | Carol   | active  <- Inserted\n",
    "                         (id=3 missing = Deleted)\n",
    "```\n",
    "\n",
    "### Using create_auto_cdc_from_snapshot_flow()\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "# Source: Daily snapshots\n",
    "@dp.table\n",
    "def daily_customer_snapshot():\n",
    "    \"\"\"\n",
    "    Complete customer snapshot loaded daily.\n",
    "    Lakeflow will compare with previous snapshot to detect changes.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"parquet\")\n",
    "        .load(\"/mnt/snapshots/customers/latest/\")\n",
    "    )\n",
    "\n",
    "# Automatically detect and apply changes\n",
    "dp.create_auto_cdc_from_snapshot_flow(\n",
    "    source=\"daily_customer_snapshot\",\n",
    "    target=\"customers_current_state\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"snapshot_date\",  # When snapshot was taken\n",
    "    stored_as_scd_type=1\n",
    ")\n",
    "\n",
    "# What happens automatically:\n",
    "# 1. Compares new snapshot to previous state\n",
    "# 2. Detects inserts (new customer_ids)\n",
    "# 3. Detects updates (changed column values)\n",
    "# 4. Detects deletes (missing customer_ids)\n",
    "# 5. Applies changes to maintain current state\n",
    "```\n",
    "\n",
    "### Snapshot CDC with Partitioning\n",
    "\n",
    "```python\n",
    "# Partitioned snapshots (common pattern)\n",
    "@dp.table\n",
    "def partitioned_product_snapshot():\n",
    "    \"\"\"\n",
    "    Daily snapshots partitioned by date.\n",
    "    Lakeflow processes only latest partition.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .table(\"raw.product_snapshots\")\n",
    "        .filter(F.col(\"snapshot_date\") == F.current_date())\n",
    "    )\n",
    "\n",
    "dp.create_auto_cdc_from_snapshot_flow(\n",
    "    source=\"partitioned_product_snapshot\",\n",
    "    target=\"products_current\",\n",
    "    keys=[\"product_id\"],\n",
    "    sequence_by=\"snapshot_date\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Handling Soft Deletes\n",
    "\n",
    "```python\n",
    "# Source uses soft deletes (is_deleted flag)\n",
    "@dp.table\n",
    "def employee_snapshot_with_soft_deletes():\n",
    "    return spark.table(\"raw.employee_snapshots\")\n",
    "\n",
    "dp.create_auto_cdc_from_snapshot_flow(\n",
    "    source=\"employee_snapshot_with_soft_deletes\",\n",
    "    target=\"employees_active\",\n",
    "    keys=[\"employee_id\"],\n",
    "    sequence_by=\"snapshot_timestamp\",\n",
    "    apply_as_deletes=\"is_deleted = true\"  # Treat as delete\n",
    ")\n",
    "# Records with is_deleted=true removed from target\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Slowly Changing Dimensions (SCD)\n",
    "\n",
    "### SCD Type 1: Overwrite (No History)\n",
    "\n",
    "**Behavior**: Updates overwrite existing values, no history maintained.\n",
    "\n",
    "```python\n",
    "# Example: Customer current state\n",
    "# Updates simply overwrite old values\n",
    "\n",
    "@dp.streaming_table\n",
    "def customer_cdc():\n",
    "    return spark.readStream.table(\"raw.customer_changes\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"customer_cdc\",\n",
    "    target=\"customers\",  # Current state only\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"update_timestamp\",\n",
    "    stored_as_scd_type=1  # Type 1: Overwrite\n",
    ")\n",
    "\n",
    "# Result in target table:\n",
    "# customer_id | name    | email              | tier\n",
    "# 1           | Alice J | alice@example.com  | Premium\n",
    "#             ^^^^^^^^^ Updated value (old value lost)\n",
    "```\n",
    "\n",
    "**When to Use SCD Type 1:**\n",
    "- Current state is all that matters\n",
    "- Historical changes not needed\n",
    "- Storage efficiency important\n",
    "- Examples: Contact information, preferences, current status\n",
    "\n",
    "### SCD Type 2: Historical Tracking\n",
    "\n",
    "**Behavior**: Maintains complete history of changes with effective dates.\n",
    "\n",
    "```python\n",
    "# Example: Customer history with all changes tracked\n",
    "\n",
    "@dp.streaming_table\n",
    "def customer_cdc_for_history():\n",
    "    return spark.readStream.table(\"raw.customer_changes\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"customer_cdc_for_history\",\n",
    "    target=\"customers_history\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"update_timestamp\",\n",
    "    stored_as_scd_type=2,  # Type 2: Track history\n",
    "    track_history_column_list=[\"tier\", \"status\"]  # Track changes to these\n",
    ")\n",
    "\n",
    "# Result in target table:\n",
    "# customer_id | name    | tier    | status  | __start_at          | __end_at            | __current\n",
    "# 1           | Alice   | Free    | active  | 2024-01-01 00:00:00 | 2024-06-01 00:00:00 | false\n",
    "# 1           | Alice   | Premium | active  | 2024-06-01 00:00:00 | 2024-10-01 00:00:00 | false\n",
    "# 1           | Alice J | Premium | active  | 2024-10-01 00:00:00 | NULL                | true\n",
    "#                                                                    ^^^^                 ^^^^\n",
    "#                                                          Current record (no end date)\n",
    "```\n",
    "\n",
    "**SCD Type 2 Metadata Columns:**\n",
    "- `__start_at`: When this version became effective\n",
    "- `__end_at`: When this version was superseded (NULL for current)\n",
    "- `__current`: Boolean flag for current record\n",
    "\n",
    "**Querying SCD Type 2 Tables:**\n",
    "```python\n",
    "# Get current state only\n",
    "current_customers = (\n",
    "    spark.table(\"customers_history\")\n",
    "    .filter(F.col(\"__current\") == True)\n",
    ")\n",
    "\n",
    "# Get state at specific point in time\n",
    "customers_on_date = (\n",
    "    spark.table(\"customers_history\")\n",
    "    .filter(\n",
    "        (F.col(\"__start_at\") <= F.lit(\"2024-06-15\")) &\n",
    "        ((F.col(\"__end_at\") > F.lit(\"2024-06-15\")) | F.col(\"__end_at\").isNull())\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get complete history for a customer\n",
    "customer_history = (\n",
    "    spark.table(\"customers_history\")\n",
    "    .filter(F.col(\"customer_id\") == 1)\n",
    "    .orderBy(\"__start_at\")\n",
    ")\n",
    "```\n",
    "\n",
    "**When to Use SCD Type 2:**\n",
    "- Historical analysis required\n",
    "- Audit trail needed\n",
    "- Regulatory compliance (track changes)\n",
    "- Point-in-time queries important\n",
    "- Examples: Pricing history, customer tier changes, product classifications\n",
    "\n",
    "### SCD Type 1 vs Type 2 Decision Matrix\n",
    "\n",
    "| Criteria | SCD Type 1 | SCD Type 2 |\n",
    "|----------|------------|------------|\n",
    "| **History** | Not tracked | Full history |\n",
    "| **Storage** | Minimal | Higher |\n",
    "| **Queries** | Simple | More complex |\n",
    "| **Updates** | Overwrite | Insert new version |\n",
    "| **Use Case** | Current state | Historical analysis |\n",
    "| **Example** | Contact info | Pricing history |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced CDC Patterns\n",
    "\n",
    "### Pattern 1: Multi-Tier CDC\n",
    "\n",
    "```python\n",
    "# Bronze: Raw CDC events\n",
    "@dp.streaming_table\n",
    "def bronze_customer_cdc():\n",
    "    return spark.readStream.table(\"raw.customer_cdc\")\n",
    "\n",
    "# Silver: Apply CDC to maintain current state (Type 1)\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"bronze_customer_cdc\",\n",
    "    target=\"silver_customers_current\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"update_timestamp\",\n",
    "    stored_as_scd_type=1  # Current state\n",
    ")\n",
    "\n",
    "# Gold: Maintain history (Type 2) for analytics\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"bronze_customer_cdc\",\n",
    "    target=\"gold_customers_history\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"update_timestamp\",\n",
    "    stored_as_scd_type=2,  # Track history\n",
    "    track_history_column_list=[\"tier\", \"status\", \"country\"]\n",
    ")\n",
    "```\n",
    "\n",
    "### Pattern 2: Selective Column Tracking\n",
    "\n",
    "```python\n",
    "# Track history only for specific columns\n",
    "@dp.streaming_table\n",
    "def product_cdc():\n",
    "    return spark.readStream.table(\"raw.product_changes\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"product_cdc\",\n",
    "    target=\"products_with_price_history\",\n",
    "    keys=[\"product_id\"],\n",
    "    sequence_by=\"updated_at\",\n",
    "    stored_as_scd_type=2,\n",
    "    track_history_column_list=[\"price\", \"currency\"]  # Only track price changes\n",
    "    # Other columns (name, description) updated in place (Type 1)\n",
    ")\n",
    "```\n",
    "\n",
    "### Pattern 3: Ignore Null Updates\n",
    "\n",
    "```python\n",
    "# Don't update when change event has null values\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"customer_cdc\",\n",
    "    target=\"customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"update_timestamp\",\n",
    "    stored_as_scd_type=1,\n",
    "    ignore_null_updates=True  # Keep existing value if update is NULL\n",
    ")\n",
    "# Useful when CDC sends partial updates\n",
    "```\n",
    "\n",
    "### Pattern 4: Custom Delete Conditions\n",
    "\n",
    "```python\n",
    "# Define custom logic for what constitutes a \"delete\"\n",
    "@dp.streaming_table\n",
    "def account_cdc():\n",
    "    return spark.readStream.table(\"raw.account_changes\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"account_cdc\",\n",
    "    target=\"active_accounts\",\n",
    "    keys=[\"account_id\"],\n",
    "    sequence_by=\"modified_at\",\n",
    "    apply_as_deletes=\"status = 'CLOSED' OR is_deleted = true\"\n",
    "    # Remove records when status is CLOSED or is_deleted flag set\n",
    ")\n",
    "```\n",
    "\n",
    "### Pattern 5: Column Subsetting\n",
    "\n",
    "```python\n",
    "# Sync only specific columns to target\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"customer_cdc\",\n",
    "    target=\"customer_pii_subset\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"update_timestamp\",\n",
    "    stored_as_scd_type=1,\n",
    "    column_list=[\"customer_id\", \"name\", \"email\", \"phone\"],  # Only these columns\n",
    "    # OR\n",
    "    except_column_list=[\"internal_notes\", \"risk_score\"]  # Exclude these columns\n",
    ")\n",
    "```\n",
    "\n",
    "### Pattern 6: Handling Late Arrivals\n",
    "\n",
    "```python\n",
    "# CDC with watermarking for late events\n",
    "@dp.streaming_table\n",
    "def order_cdc_with_watermark():\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.order_cdc\")\n",
    "        .withWatermark(\"event_timestamp\", \"1 hour\")  # Accept events up to 1hr late\n",
    "    )\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"order_cdc_with_watermark\",\n",
    "    target=\"orders_current\",\n",
    "    keys=[\"order_id\"],\n",
    "    sequence_by=\"event_timestamp\"  # Watermarked column\n",
    ")\n",
    "# Late events processed correctly based on sequence_by\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge Strategies and Conflict Resolution\n",
    "\n",
    "### Understanding Merge Conflicts\n",
    "\n",
    "**Conflict Scenarios:**\n",
    "```\n",
    "Scenario 1: Out-of-order updates\n",
    "Event 1: customer_id=1, tier='Premium', timestamp=10:00\n",
    "Event 2: customer_id=1, tier='Free', timestamp=09:00  <- Older event arrives late\n",
    "Solution: Use sequence_by to apply in correct order\n",
    "\n",
    "Scenario 2: Duplicate keys in batch\n",
    "Event 1: customer_id=1, name='Alice'\n",
    "Event 2: customer_id=1, name='Alice J'  <- Same key, different value\n",
    "Solution: Lakeflow deduplicates based on sequence_by (keeps latest)\n",
    "\n",
    "Scenario 3: Concurrent updates\n",
    "Source A: customer_id=1, email='alice@new.com', timestamp=10:00:00\n",
    "Source B: customer_id=1, phone='555-1234', timestamp=10:00:00  <- Same timestamp\n",
    "Solution: Define tie-breaker logic or merge both updates\n",
    "```\n",
    "\n",
    "### Sequence-Based Conflict Resolution\n",
    "\n",
    "```python\n",
    "# Use timestamp to order operations\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"customer_cdc\",\n",
    "    target=\"customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"update_timestamp\",  # Latest timestamp wins\n",
    "    stored_as_scd_type=1\n",
    ")\n",
    "\n",
    "# If multiple events have same timestamp:\n",
    "# - Insert: Always applied\n",
    "# - Update: Last one processed wins (non-deterministic if truly concurrent)\n",
    "# - Delete: Removes record\n",
    "```\n",
    "\n",
    "### Deduplication Strategy\n",
    "\n",
    "```python\n",
    "# Lakeflow automatically deduplicates based on:\n",
    "# 1. Keys (primary key match)\n",
    "# 2. sequence_by (latest value kept)\n",
    "\n",
    "@dp.streaming_table\n",
    "def deduplicated_cdc():\n",
    "    \"\"\"\n",
    "    If CDC source has duplicates,\n",
    "    pre-deduplicate before auto_cdc_flow\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream.table(\"raw.cdc_with_duplicates\")\n",
    "        .withWatermark(\"event_time\", \"10 minutes\")\n",
    "        .dropDuplicates([\"customer_id\", \"event_time\"])  # Remove exact duplicates\n",
    "    )\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"deduplicated_cdc\",\n",
    "    target=\"customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"event_time\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Handling Partial Updates\n",
    "\n",
    "```python\n",
    "# CDC sends only changed columns (nulls for unchanged)\n",
    "dp.create_auto_cdc_flow(\n",
    "    source=\"partial_update_cdc\",\n",
    "    target=\"customers\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=\"update_timestamp\",\n",
    "    stored_as_scd_type=1,\n",
    "    ignore_null_updates=True  # Don't overwrite with NULL\n",
    ")\n",
    "\n",
    "# Example:\n",
    "# Existing: {id=1, name='Alice', email='alice@old.com', tier='Free'}\n",
    "# Update:   {id=1, name=NULL, email='alice@new.com', tier=NULL}\n",
    "# Result:   {id=1, name='Alice', email='alice@new.com', tier='Free'}\n",
    "#                  ^^^^^ Kept    ^^^^^^^^^^^^^ Updated  ^^^^ Kept\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored flows and advanced CDC patterns in Lakeflow:\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Lakeflow Flows**\n",
    "   - High-level abstractions for common patterns\n",
    "   - Less code, built-in best practices\n",
    "   - Automatic optimization\n",
    "\n",
    "2. **Append Flow (`dp.append_flow()`)**\n",
    "   - Incremental data loading\n",
    "   - Event logs and time-series data\n",
    "   - Efficient append-only pattern\n",
    "\n",
    "3. **Automatic CDC (`dp.create_auto_cdc_flow()`)**\n",
    "   - Change data capture automation\n",
    "   - Handles inserts, updates, deletes\n",
    "   - Sequence-based ordering\n",
    "   - Key-based deduplication\n",
    "\n",
    "4. **Snapshot CDC (`dp.create_auto_cdc_from_snapshot_flow()`)**\n",
    "   - CDC from full snapshots\n",
    "   - Automatic change detection\n",
    "   - Soft delete handling\n",
    "\n",
    "5. **Slowly Changing Dimensions**\n",
    "   - Type 1: Current state (overwrite)\n",
    "   - Type 2: Historical tracking (versioning)\n",
    "   - Selective column tracking\n",
    "   - Point-in-time queries\n",
    "\n",
    "6. **Advanced CDC Patterns**\n",
    "   - Multi-tier CDC architectures\n",
    "   - Selective column synchronization\n",
    "   - Custom delete conditions\n",
    "   - Late arrival handling\n",
    "\n",
    "7. **Merge Strategies**\n",
    "   - Conflict resolution with sequence_by\n",
    "   - Deduplication strategies\n",
    "   - Partial update handling\n",
    "   - Null value management\n",
    "\n",
    "### Functional Programming Benefits\n",
    "\n",
    "- **Declarative**: Define what to sync, not how\n",
    "- **Composable**: Flows integrate with table definitions\n",
    "- **Immutable**: Source data unchanged, target reflects changes\n",
    "- **Deterministic**: Sequence-based ordering ensures consistency\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "✅ Use append_flow for append-only data\n",
    "✅ Choose SCD Type 1 for current state, Type 2 for history\n",
    "✅ Always specify sequence_by for ordering\n",
    "✅ Define clear primary keys\n",
    "✅ Handle late arrivals with watermarking\n",
    "✅ Use ignore_null_updates for partial updates\n",
    "✅ Monitor CDC lag and apply rates\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **6.6**: Best practices and anti-patterns for Lakeflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Practice CDC patterns:\n",
    "\n",
    "**Exercise 1: Append Flow**\n",
    "- Create append flow for event log data\n",
    "- Apply transformations before appending\n",
    "- Monitor append rates and volumes\n",
    "\n",
    "**Exercise 2: Basic CDC**\n",
    "- Implement auto_cdc_flow with sample data\n",
    "- Test insert, update, and delete operations\n",
    "- Verify correct sequence handling\n",
    "\n",
    "**Exercise 3: SCD Type 1 vs Type 2**\n",
    "- Create same source with both Type 1 and Type 2\n",
    "- Apply sample changes\n",
    "- Compare storage and query patterns\n",
    "\n",
    "**Exercise 4: Snapshot CDC**\n",
    "- Process daily snapshots with CDC\n",
    "- Detect and apply all change types\n",
    "- Handle soft deletes\n",
    "\n",
    "**Exercise 5: Conflict Resolution**\n",
    "- Create CDC stream with out-of-order events\n",
    "- Implement sequence-based resolution\n",
    "- Test late arrival handling\n",
    "\n",
    "**Exercise 6: Multi-Tier CDC**\n",
    "- Design bronze/silver/gold CDC architecture\n",
    "- Apply Type 1 for current, Type 2 for history\n",
    "- Monitor quality at each layer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
