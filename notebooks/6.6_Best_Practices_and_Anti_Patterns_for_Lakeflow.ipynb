{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.6 Best Practices and Anti-Patterns for Lakeflow Declarative Pipelines\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand prohibited operations in declarative pipeline definitions\n",
    "- Apply decision criteria for table vs view vs temporary view selection\n",
    "- Recognize and avoid common performance anti-patterns\n",
    "- Implement effective testing strategies for declarative pipelines\n",
    "- Execute systematic migration from imperative to declarative patterns\n",
    "- Troubleshoot common pipeline issues with proven techniques\n",
    "\n",
    "**Prerequisites**: Completion of notebooks 6.1-6.5\n",
    "\n",
    "**Key Takeaway**: Following declarative best practices ensures maintainable, performant, and reliable data pipelines that leverage Lakeflow's automatic optimization and orchestration capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform setup: Uncomment for local development, keep commented in Databricks\n",
    "# %run ./00_Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark import pipelines as dp\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prohibited Operations in Pipeline Definitions\n",
    "\n",
    "### The Declarative Paradigm Contract\n",
    "\n",
    "Lakeflow declarative pipelines follow a **strict declarative contract**: pipeline definitions should describe **what** to compute, not **how** or **when** to execute operations.\n",
    "\n",
    "**Core Principle**: Pipeline functions return DataFrame transformations; the platform handles execution, materialization, and scheduling.\n",
    "\n",
    "### Prohibited Operations\n",
    "\n",
    "The following operations **trigger immediate execution** and **violate the declarative contract**:\n",
    "\n",
    "1. **Actions that Materialize Data**:\n",
    "   - `.collect()`, `.take()`, `.head()`, `.first()`, `.show()`\n",
    "   - `.count()`, `.countDistinct()`\n",
    "   - `.toPandas()`\n",
    "   \n",
    "2. **Write Operations**:\n",
    "   - `.write.format().save()`\n",
    "   - `.write.saveAsTable()`\n",
    "   - `.createOrReplaceTempView()`\n",
    "   \n",
    "3. **Cache Operations**:\n",
    "   - `.cache()`, `.persist()`\n",
    "   - `.unpersist()`\n",
    "   \n",
    "4. **Checkpoint Operations**:\n",
    "   - Manual `.checkpoint()` (Lakeflow manages checkpoints automatically)\n",
    "\n",
    "**Why Prohibited?**\n",
    "- **Breaks lazy evaluation**: Lakeflow optimizes the entire DAG; premature materialization prevents optimization\n",
    "- **Prevents orchestration**: Lakeflow manages when and how to execute; manual execution breaks dependency tracking\n",
    "- **Resource inefficiency**: Materializing data in pipeline definitions wastes compute during planning phase\n",
    "- **Inconsistent state**: Manual writes bypass Lakeflow's transaction and consistency guarantees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❌ Anti-Pattern: Materializing Data in Pipeline Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ ANTI-PATTERN: Using .count() to validate data\n",
    "# @dp.table\n",
    "# def bad_validation_pattern():\n",
    "#     df = spark.table(\"raw.customers\")\n",
    "#     \n",
    "#     # WRONG: This materializes data during pipeline definition\n",
    "#     if df.count() == 0:\n",
    "#         raise ValueError(\"No customer data found\")\n",
    "#     \n",
    "#     return df.filter(F.col(\"status\") == \"active\")\n",
    "\n",
    "# ❌ ANTI-PATTERN: Using .collect() for branching logic\n",
    "# @dp.table\n",
    "# def bad_branching_pattern():\n",
    "#     config_df = spark.table(\"config.settings\")\n",
    "#     \n",
    "#     # WRONG: Materializes data and breaks declarative contract\n",
    "#     config = config_df.collect()[0].asDict()\n",
    "#     threshold = config[\"revenue_threshold\"]\n",
    "#     \n",
    "#     return (\n",
    "#         spark.table(\"sales\")\n",
    "#         .filter(F.col(\"revenue\") > threshold)\n",
    "#     )\n",
    "\n",
    "# ❌ ANTI-PATTERN: Manual write operations\n",
    "# @dp.table\n",
    "# def bad_write_pattern():\n",
    "#     df = spark.table(\"raw.events\")\n",
    "#     \n",
    "#     # WRONG: Manual write bypasses Lakeflow orchestration\n",
    "#     df.write.format(\"delta\").mode(\"append\").save(\"/mnt/manual/events\")\n",
    "#     \n",
    "#     return df\n",
    "\n",
    "print(\"❌ These anti-patterns are commented out because they violate declarative principles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Correct Pattern: Pure Declarative Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ CORRECT: Use expectations for data validation\n",
    "@dp.table\n",
    "@dp.expect_or_fail(\"has_customer_data\", \"customer_id IS NOT NULL\")\n",
    "def good_validation_pattern():\n",
    "    \"\"\"Data validation through declarative expectations.\"\"\"\n",
    "    return (\n",
    "        spark.table(\"raw.customers\")\n",
    "        .filter(F.col(\"status\") == \"active\")\n",
    "    )\n",
    "\n",
    "# ✅ CORRECT: Use joins for configuration-driven logic\n",
    "@dp.table\n",
    "def config_driven_thresholds():\n",
    "    \"\"\"Configuration-driven filtering using joins.\"\"\"\n",
    "    return (\n",
    "        spark.table(\"sales\")\n",
    "        .join(\n",
    "            spark.table(\"config.settings\").select(\n",
    "                F.col(\"revenue_threshold\")\n",
    "            ),\n",
    "            how=\"cross\"\n",
    "        )\n",
    "        .filter(F.col(\"revenue\") > F.col(\"revenue_threshold\"))\n",
    "        .drop(\"revenue_threshold\")\n",
    "    )\n",
    "\n",
    "# ✅ CORRECT: Let Lakeflow handle writes\n",
    "@dp.table(\n",
    "    name=\"processed_events\",\n",
    "    path=\"/mnt/processed/events\"  # Lakeflow writes to this location\n",
    ")\n",
    "def good_write_pattern():\n",
    "    \"\"\"Lakeflow manages writes automatically.\"\"\"\n",
    "    return spark.table(\"raw.events\")\n",
    "\n",
    "print(\"✅ Declarative patterns preserve lazy evaluation and platform orchestration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Table vs View vs Temporary View Selection Criteria\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "| Criterion | @dp.table | @dp.materialized_view | @dp.temporary_view |\n",
    "|-----------|-----------|----------------------|--------------------|\n",
    "| **Persistence** | Durable storage | Durable storage | Logical only (no storage) |\n",
    "| **Performance** | Pre-computed | Pre-computed | Computed on-demand |\n",
    "| **Downstream Reads** | Many | Many | Few (1-2) |\n",
    "| **Update Frequency** | Any | Any | Not applicable |\n",
    "| **Storage Cost** | Yes | Yes | No |\n",
    "| **Compute Cost** | Once per update | Once per update | Every read |\n",
    "| **Use Case** | Source of truth | Aggregations | Intermediate logic |\n",
    "| **Incremental** | Yes (streaming) | No | No |\n",
    "| **Partitioning** | Yes | Yes | No |\n",
    "| **Time Travel** | Yes | Yes | No |\n",
    "\n",
    "### Selection Guidelines\n",
    "\n",
    "**Use `@dp.table` when**:\n",
    "- This is a **source of truth** for downstream consumers\n",
    "- Data will be **read multiple times** by different pipelines\n",
    "- You need **time travel** or **data versioning**\n",
    "- You need **partitioning** for performance\n",
    "- Data requires **incremental updates** (streaming)\n",
    "\n",
    "**Use `@dp.materialized_view` when**:\n",
    "- This is an **expensive aggregation** used by multiple consumers\n",
    "- Source data changes **less frequently** than reads\n",
    "- You want **pre-computed results** for BI dashboards\n",
    "- Storage cost is acceptable for **query performance gains**\n",
    "\n",
    "**Use `@dp.temporary_view` when**:\n",
    "- This is **intermediate logic** used by only 1-2 downstream tables\n",
    "- Computation is **cheap** (simple filters, selects)\n",
    "- You want to **avoid storage costs**\n",
    "- Logic is only for **code organization** (breaking complex logic into steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Examples with Selection Rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Table: Source of truth, read by many pipelines\n",
    "@dp.table(\n",
    "    partition_cols=[\"country\", \"year\"],\n",
    "    comment=\"Customer master table - source of truth\"\n",
    ")\n",
    "def customers():\n",
    "    \"\"\"\n",
    "    Why @dp.table?\n",
    "    - Source of truth for customer data\n",
    "    - Read by multiple downstream pipelines (orders, analytics, ML)\n",
    "    - Needs partitioning for performance\n",
    "    - Requires time travel for compliance\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.table(\"raw.customers\")\n",
    "        .withColumn(\"year\", F.year(\"signup_date\"))\n",
    "    )\n",
    "\n",
    "# ✅ Temporary View: Simple filter, used by one downstream table\n",
    "@dp.temporary_view\n",
    "def active_customers():\n",
    "    \"\"\"\n",
    "    Why @dp.temporary_view?\n",
    "    - Simple filter operation (cheap to compute)\n",
    "    - Only used by premium_customers table below\n",
    "    - No need to persist (avoids storage cost)\n",
    "    - Logical view for code organization\n",
    "    \"\"\"\n",
    "    return dp.read(\"customers\").filter(F.col(\"status\") == \"active\")\n",
    "\n",
    "# ✅ Table: Final output consumed by analytics\n",
    "@dp.table\n",
    "def premium_customers():\n",
    "    \"\"\"\n",
    "    Why @dp.table?\n",
    "    - Final output for analytics dashboards\n",
    "    - Multiple downstream consumers (BI, ML, reports)\n",
    "    - Needs durability and performance\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read(\"active_customers\")\n",
    "        .filter(F.col(\"lifetime_value\") > 10000)\n",
    "    )\n",
    "\n",
    "# ✅ Materialized View: Expensive aggregation, read frequently\n",
    "@dp.materialized_view\n",
    "def customer_lifetime_metrics():\n",
    "    \"\"\"\n",
    "    Why @dp.materialized_view?\n",
    "    - Expensive aggregation across orders (millions of rows)\n",
    "    - Read by multiple dashboards and reports\n",
    "    - Source data (orders) changes less frequently than reads\n",
    "    - Pre-computation saves significant query time\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read(\"customers\")\n",
    "        .join(dp.read(\"orders\"), \"customer_id\")\n",
    "        .groupBy(\"customer_id\", \"country\")\n",
    "        .agg(\n",
    "            F.sum(\"order_total\").alias(\"lifetime_value\"),\n",
    "            F.count(\"order_id\").alias(\"order_count\"),\n",
    "            F.avg(\"order_total\").alias(\"avg_order_value\"),\n",
    "            F.min(\"order_date\").alias(\"first_order_date\"),\n",
    "            F.max(\"order_date\").alias(\"last_order_date\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"✅ Selection criteria:\")\n",
    "print(\"   - Table: Source of truth, multiple readers, needs persistence\")\n",
    "print(\"   - Materialized View: Expensive aggregation, read frequently\")\n",
    "print(\"   - Temporary View: Simple logic, single reader, avoid storage cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Anti-Patterns and Optimization Strategies\n",
    "\n",
    "### Common Performance Anti-Patterns\n",
    "\n",
    "#### Anti-Pattern 1: Redundant Materialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ ANTI-PATTERN: Materializing simple filters as tables\n",
    "# @dp.table\n",
    "# def customers_us():  # Unnecessary table\n",
    "#     return dp.read(\"customers\").filter(F.col(\"country\") == \"US\")\n",
    "# \n",
    "# @dp.table\n",
    "# def customers_uk():  # Unnecessary table\n",
    "#     return dp.read(\"customers\").filter(F.col(\"country\") == \"UK\")\n",
    "# \n",
    "# @dp.table\n",
    "# def customers_ca():  # Unnecessary table\n",
    "#     return dp.read(\"customers\").filter(F.col(\"country\") == \"CA\")\n",
    "\n",
    "# ✅ CORRECT: Use temporary views or compute on-demand\n",
    "@dp.table(\n",
    "    partition_cols=[\"country\"],  # Partition for efficient filtering\n",
    "    comment=\"All customers partitioned by country\"\n",
    ")\n",
    "def customers_partitioned():\n",
    "    \"\"\"Single table with efficient partitioning.\"\"\"\n",
    "    return spark.table(\"raw.customers\")\n",
    "\n",
    "# Downstream consumers filter efficiently using partition pruning\n",
    "@dp.table\n",
    "def us_customer_analysis():\n",
    "    \"\"\"Partition pruning eliminates need for separate table.\"\"\"\n",
    "    return (\n",
    "        dp.read(\"customers_partitioned\")\n",
    "        .filter(F.col(\"country\") == \"US\")  # Efficient: reads only US partition\n",
    "    )\n",
    "\n",
    "print(\"✅ Optimization: Use partitioning instead of separate tables\")\n",
    "print(\"   - Reduces storage cost (no duplicated data)\")\n",
    "print(\"   - Reduces maintenance (single table to manage)\")\n",
    "print(\"   - Maintains performance (partition pruning is efficient)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anti-Pattern 2: Over-Materialization of Temporary Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ ANTI-PATTERN: Materializing every intermediate step\n",
    "# @dp.table  # Unnecessary materialization\n",
    "# def step1_filter():\n",
    "#     return spark.table(\"raw.orders\").filter(F.col(\"status\") != \"cancelled\")\n",
    "# \n",
    "# @dp.table  # Unnecessary materialization\n",
    "# def step2_enrich():\n",
    "#     return (\n",
    "#         dp.read(\"step1_filter\")\n",
    "#         .withColumn(\"order_year\", F.year(\"order_date\"))\n",
    "#     )\n",
    "# \n",
    "# @dp.table  # Only this needs materialization\n",
    "# def step3_aggregate():\n",
    "#     return (\n",
    "#         dp.read(\"step2_enrich\")\n",
    "#         .groupBy(\"order_year\").agg(F.sum(\"total\").alias(\"yearly_revenue\"))\n",
    "#     )\n",
    "\n",
    "# ✅ CORRECT: Use temporary views for intermediate steps\n",
    "@dp.temporary_view\n",
    "def filtered_orders():\n",
    "    \"\"\"Temporary view: cheap filter, single use.\"\"\"\n",
    "    return spark.table(\"raw.orders\").filter(F.col(\"status\") != \"cancelled\")\n",
    "\n",
    "@dp.temporary_view\n",
    "def enriched_orders():\n",
    "    \"\"\"Temporary view: simple transformation, single use.\"\"\"\n",
    "    return (\n",
    "        dp.read(\"filtered_orders\")\n",
    "        .withColumn(\"order_year\", F.year(\"order_date\"))\n",
    "    )\n",
    "\n",
    "@dp.table  # Only final result materialized\n",
    "def yearly_revenue():\n",
    "    \"\"\"Table: final aggregation for analytics consumption.\"\"\"\n",
    "    return (\n",
    "        dp.read(\"enriched_orders\")\n",
    "        .groupBy(\"order_year\")\n",
    "        .agg(F.sum(\"total\").alias(\"yearly_revenue\"))\n",
    "    )\n",
    "\n",
    "print(\"✅ Optimization: Temporary views for linear pipelines\")\n",
    "print(\"   - Catalyst optimizer fuses operations into single plan\")\n",
    "print(\"   - Reduces storage cost (no intermediate tables)\")\n",
    "print(\"   - Maintains or improves performance (fewer reads/writes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anti-Pattern 3: Missing Partitioning on Large Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ ANTI-PATTERN: No partitioning on large time-series table\n",
    "# @dp.table  # Missing partition_cols\n",
    "# def events_no_partitions():\n",
    "#     \"\"\"Problem: Full table scans for date-based queries.\"\"\"\n",
    "#     return spark.table(\"raw.events\")\n",
    "\n",
    "# ✅ CORRECT: Partition by common filter columns\n",
    "@dp.table(\n",
    "    partition_cols=[\"event_date\"],  # Partition by date for time-series queries\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "def events_partitioned():\n",
    "    \"\"\"\n",
    "    Partitioning strategy:\n",
    "    - event_date: Common filter in analytics queries\n",
    "    - Enables partition pruning (reads only relevant partitions)\n",
    "    - Auto-optimize maintains healthy file sizes\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.table(\"raw.events\")\n",
    "        .withColumn(\"event_date\", F.to_date(\"event_timestamp\"))\n",
    "    )\n",
    "\n",
    "# Downstream query benefits from partition pruning\n",
    "@dp.table\n",
    "def last_7_days_events():\n",
    "    \"\"\"\n",
    "    Efficient: Reads only 7 partitions instead of entire table.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read(\"events_partitioned\")\n",
    "        .filter(\n",
    "            F.col(\"event_date\") >= F.current_date() - F.expr(\"INTERVAL 7 DAYS\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"✅ Optimization: Strategic partitioning\")\n",
    "print(\"   - Choose partition columns based on common filter patterns\")\n",
    "print(\"   - Avoid over-partitioning (target: 128MB-1GB per partition)\")\n",
    "print(\"   - Enable auto-optimize for partition management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anti-Pattern 4: Inefficient Join Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ ANTI-PATTERN: Large table joined to small table without broadcast hint\n",
    "# @dp.table\n",
    "# def orders_with_country():\n",
    "#     \"\"\"Problem: Shuffle join instead of broadcast join.\"\"\"\n",
    "#     return (\n",
    "#         dp.read(\"orders\")  # Large: millions of rows\n",
    "#         .join(\n",
    "#             dp.read(\"countries\"),  # Small: ~200 rows, but not broadcast\n",
    "#             \"country_code\"\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# ✅ CORRECT: Broadcast small dimension tables\n",
    "@dp.table\n",
    "def orders_with_country_optimized():\n",
    "    \"\"\"\n",
    "    Optimization: Broadcast small dimension table.\n",
    "    - Avoids shuffle of large orders table\n",
    "    - Reduces network I/O\n",
    "    - Faster execution\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read(\"orders\")\n",
    "        .join(\n",
    "            F.broadcast(dp.read(\"countries\")),  # Broadcast hint for small table\n",
    "            \"country_code\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# ✅ ALTERNATIVE: Configure auto-broadcast threshold\n",
    "# In pipeline configuration (not in function definition):\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10MB\n",
    "\n",
    "print(\"✅ Optimization: Broadcast joins for dimension tables\")\n",
    "print(\"   - Use F.broadcast() for small tables (<10MB)\")\n",
    "print(\"   - Avoids expensive shuffle operations\")\n",
    "print(\"   - Configure autoBroadcastJoinThreshold for automatic detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Strategies for Declarative Pipelines\n",
    "\n",
    "### Testing Philosophy\n",
    "\n",
    "Declarative pipelines require a **different testing approach** than imperative code:\n",
    "\n",
    "1. **Unit Tests**: Test pure transformation functions in isolation\n",
    "2. **Integration Tests**: Test table dependencies and data flow\n",
    "3. **Expectation Tests**: Validate data quality rules before deployment\n",
    "4. **End-to-End Tests**: Test full pipeline execution in development environment\n",
    "\n",
    "### Pattern 1: Unit Testing Pure Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline definition with pure transformation\n",
    "@dp.table\n",
    "def customer_segments():\n",
    "    \"\"\"Segment customers by lifetime value.\"\"\"\n",
    "    return apply_segmentation(spark.table(\"raw.customers\"))\n",
    "\n",
    "def apply_segmentation(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pure function: Testable independently of pipeline.\n",
    "    \n",
    "    Segments:\n",
    "    - Premium: lifetime_value >= 10000\n",
    "    - Standard: 1000 <= lifetime_value < 10000\n",
    "    - Basic: lifetime_value < 1000\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "        \"segment\",\n",
    "        F.when(F.col(\"lifetime_value\") >= 10000, \"Premium\")\n",
    "        .when(F.col(\"lifetime_value\") >= 1000, \"Standard\")\n",
    "        .otherwise(\"Basic\")\n",
    "    )\n",
    "\n",
    "# Unit test (in tests/test_transformations.py)\n",
    "\"\"\"\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from chispa.dataframe_comparer import assert_df_equality\n",
    "\n",
    "def test_apply_segmentation(spark: SparkSession):\n",
    "    # Arrange: Create test data\n",
    "    input_data = [\n",
    "        (1, \"Alice\", 15000),\n",
    "        (2, \"Bob\", 5000),\n",
    "        (3, \"Charlie\", 500)\n",
    "    ]\n",
    "    input_df = spark.createDataFrame(\n",
    "        input_data, \n",
    "        [\"customer_id\", \"name\", \"lifetime_value\"]\n",
    "    )\n",
    "    \n",
    "    # Act: Apply transformation\n",
    "    result_df = apply_segmentation(input_df)\n",
    "    \n",
    "    # Assert: Validate segmentation\n",
    "    expected_data = [\n",
    "        (1, \"Alice\", 15000, \"Premium\"),\n",
    "        (2, \"Bob\", 5000, \"Standard\"),\n",
    "        (3, \"Charlie\", 500, \"Basic\")\n",
    "    ]\n",
    "    expected_df = spark.createDataFrame(\n",
    "        expected_data,\n",
    "        [\"customer_id\", \"name\", \"lifetime_value\", \"segment\"]\n",
    "    )\n",
    "    \n",
    "    assert_df_equality(result_df, expected_df, ignore_nullable=True)\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ Testing Pattern: Extract pure functions for unit testing\")\n",
    "print(\"   - Transformation logic in pure function (testable)\")\n",
    "print(\"   - Pipeline definition delegates to pure function\")\n",
    "print(\"   - Use pytest + chispa for DataFrame assertions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Testing Expectations Before Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with expectations\n",
    "@dp.table\n",
    "@dp.expect(\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+'\")\n",
    "@dp.expect_or_drop(\"adult_age\", \"age >= 18 AND age <= 120\")\n",
    "@dp.expect_or_fail(\"required_fields\", \"customer_id IS NOT NULL AND name IS NOT NULL\")\n",
    "def validated_customers():\n",
    "    \"\"\"Customer table with quality expectations.\"\"\"\n",
    "    return spark.table(\"raw.customers\")\n",
    "\n",
    "# Test expectations against sample data (in tests/test_expectations.py)\n",
    "\"\"\"\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def test_customer_expectations(spark: SparkSession):\n",
    "    # Test data with various quality issues\n",
    "    test_data = [\n",
    "        (1, \"Alice\", \"alice@example.com\", 25),      # Valid\n",
    "        (2, \"Bob\", \"invalid-email\", 30),            # Invalid email (WARN)\n",
    "        (3, \"Charlie\", \"charlie@example.com\", 15),  # Underage (DROP)\n",
    "        (None, \"Dave\", \"dave@example.com\", 35),     # Null ID (FAIL)\n",
    "    ]\n",
    "    test_df = spark.createDataFrame(\n",
    "        test_data,\n",
    "        [\"customer_id\", \"name\", \"email\", \"age\"]\n",
    "    )\n",
    "    \n",
    "    # Create temporary table for testing\n",
    "    test_df.createOrReplaceTempView(\"raw.customers\")\n",
    "    \n",
    "    # Test expectations\n",
    "    # 1. WARN expectation: count violations but don't drop\n",
    "    invalid_email_count = test_df.filter(\n",
    "        ~F.col(\"email\").rlike(\"^[^@]+@[^@]+\\\\.[^@]+\")\n",
    "    ).count()\n",
    "    assert invalid_email_count == 1, \"Expected 1 invalid email\"\n",
    "    \n",
    "    # 2. DROP expectation: verify rows would be dropped\n",
    "    underage_count = test_df.filter(\n",
    "        ~((F.col(\"age\") >= 18) & (F.col(\"age\") <= 120))\n",
    "    ).count()\n",
    "    assert underage_count == 1, \"Expected 1 underage customer to be dropped\"\n",
    "    \n",
    "    # 3. FAIL expectation: verify pipeline would fail\n",
    "    null_id_count = test_df.filter(\n",
    "        F.col(\"customer_id\").isNull() | F.col(\"name\").isNull()\n",
    "    ).count()\n",
    "    assert null_id_count == 1, \"Expected 1 null ID to cause pipeline failure\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ Testing Pattern: Validate expectations with test data\")\n",
    "print(\"   - Create test datasets with known quality issues\")\n",
    "print(\"   - Verify expectation logic before deployment\")\n",
    "print(\"   - Prevent surprises in production pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Integration Testing with Mock Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with dependencies\n",
    "@dp.table\n",
    "def customer_order_summary():\n",
    "    \"\"\"Aggregate orders by customer.\"\"\"\n",
    "    return (\n",
    "        dp.read(\"customers\")\n",
    "        .join(dp.read(\"orders\"), \"customer_id\")\n",
    "        .groupBy(\"customer_id\", \"name\")\n",
    "        .agg(\n",
    "            F.sum(\"order_total\").alias(\"total_spent\"),\n",
    "            F.count(\"order_id\").alias(\"order_count\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Integration test with mock tables (in tests/test_integration.py)\n",
    "\"\"\"\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from chispa.dataframe_comparer import assert_df_equality\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_customers(spark: SparkSession):\n",
    "    \"\"\"Mock customers table.\"\"\"\n",
    "    data = [\n",
    "        (1, \"Alice\"),\n",
    "        (2, \"Bob\")\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"customer_id\", \"name\"])\n",
    "    df.createOrReplaceTempView(\"customers\")\n",
    "    return df\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_orders(spark: SparkSession):\n",
    "    \"\"\"Mock orders table.\"\"\"\n",
    "    data = [\n",
    "        (101, 1, 100.0),\n",
    "        (102, 1, 200.0),\n",
    "        (103, 2, 150.0)\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"order_id\", \"customer_id\", \"order_total\"])\n",
    "    df.createOrReplaceTempView(\"orders\")\n",
    "    return df\n",
    "\n",
    "def test_customer_order_summary(spark: SparkSession, mock_customers, mock_orders):\n",
    "    # Act: Execute pipeline logic\n",
    "    result_df = (\n",
    "        spark.table(\"customers\")\n",
    "        .join(spark.table(\"orders\"), \"customer_id\")\n",
    "        .groupBy(\"customer_id\", \"name\")\n",
    "        .agg(\n",
    "            F.sum(\"order_total\").alias(\"total_spent\"),\n",
    "            F.count(\"order_id\").alias(\"order_count\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Assert: Validate aggregation\n",
    "    expected_data = [\n",
    "        (1, \"Alice\", 300.0, 2),\n",
    "        (2, \"Bob\", 150.0, 1)\n",
    "    ]\n",
    "    expected_df = spark.createDataFrame(\n",
    "        expected_data,\n",
    "        [\"customer_id\", \"name\", \"total_spent\", \"order_count\"]\n",
    "    )\n",
    "    \n",
    "    assert_df_equality(result_df, expected_df, ignore_nullable=True)\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ Testing Pattern: Mock dependencies for integration tests\")\n",
    "print(\"   - Use pytest fixtures to create mock tables\")\n",
    "print(\"   - Test joins and aggregations with controlled data\")\n",
    "print(\"   - Validate end-to-end data flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Migration Checklist: Imperative to Declarative\n",
    "\n",
    "### Step-by-Step Migration Process\n",
    "\n",
    "**Phase 1: Preparation**\n",
    "1. ✅ **Audit current pipeline**: Document all transformations, dependencies, and schedules\n",
    "2. ✅ **Identify prohibited operations**: Find `.collect()`, `.write()`, `.cache()` calls\n",
    "3. ✅ **Review table materialization**: Identify which intermediate steps need persistence\n",
    "4. ✅ **Document data quality rules**: Extract validation logic for expectation migration\n",
    "\n",
    "**Phase 2: Code Migration**\n",
    "5. ✅ **Update imports**: Change `import dlt` to `from pyspark import pipelines as dp`\n",
    "6. ✅ **Convert decorators**: Replace `@dlt.table` with `@dp.table`, etc.\n",
    "7. ✅ **Remove prohibited operations**: Refactor materialization and writes\n",
    "8. ✅ **Add table configurations**: Include partition_cols, table_properties, paths\n",
    "9. ✅ **Migrate expectations**: Convert validation logic to `@dp.expect` patterns\n",
    "10. ✅ **Extract pure functions**: Separate transformation logic for testing\n",
    "\n",
    "**Phase 3: Optimization**\n",
    "11. ✅ **Apply table type optimization**: Convert unnecessary tables to temporary views\n",
    "12. ✅ **Add partitioning**: Partition large tables by common filter columns\n",
    "13. ✅ **Optimize joins**: Add broadcast hints for dimension tables\n",
    "14. ✅ **Configure auto-optimize**: Enable Delta Lake auto-compaction\n",
    "\n",
    "**Phase 4: Testing**\n",
    "15. ✅ **Create unit tests**: Test pure transformation functions\n",
    "16. ✅ **Test expectations**: Validate quality rules with sample data\n",
    "17. ✅ **Integration tests**: Test table dependencies with mocks\n",
    "18. ✅ **Development deployment**: Run pipeline in development environment\n",
    "\n",
    "**Phase 5: Deployment**\n",
    "19. ✅ **Deploy to staging**: Validate with production-like data\n",
    "20. ✅ **Monitor metrics**: Check execution time, data quality, resource usage\n",
    "21. ✅ **Production deployment**: Blue-green or canary deployment strategy\n",
    "22. ✅ **Post-deployment validation**: Verify data correctness and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migration Example: Before and After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ BEFORE: Imperative pipeline with anti-patterns\n",
    "\"\"\"\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dlt.table\n",
    "def raw_customers():\n",
    "    # Read data\n",
    "    df = spark.read.format(\"parquet\").load(\"/mnt/raw/customers\")\n",
    "    \n",
    "    # Prohibited operation: count for validation\n",
    "    if df.count() == 0:\n",
    "        raise ValueError(\"No customer data\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "@dlt.table\n",
    "def filtered_customers():  # Unnecessary materialization\n",
    "    return (\n",
    "        dlt.read(\"raw_customers\")\n",
    "        .filter(F.col(\"status\") == \"active\")\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def premium_customers():  # No expectations\n",
    "    df = (\n",
    "        dlt.read(\"filtered_customers\")\n",
    "        .filter(F.col(\"lifetime_value\") > 10000)\n",
    "    )\n",
    "    \n",
    "    # Prohibited operation: manual write\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/gold/premium\")\n",
    "    \n",
    "    return df\n",
    "\"\"\"\n",
    "\n",
    "# ✅ AFTER: Declarative pipeline with best practices\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dp.table(\n",
    "    name=\"raw_customers\",\n",
    "    partition_cols=[\"country\"],\n",
    "    comment=\"Raw customer data from source system\"\n",
    ")\n",
    "@dp.expect_or_fail(\"has_data\", \"customer_id IS NOT NULL\")  # Declarative validation\n",
    "def raw_customers():\n",
    "    \"\"\"Source table with data quality enforcement.\"\"\"\n",
    "    return spark.read.format(\"parquet\").load(\"/mnt/raw/customers\")\n",
    "\n",
    "@dp.temporary_view  # Optimized: temporary view for simple filter\n",
    "def filtered_customers():\n",
    "    \"\"\"Active customers - logical view for code organization.\"\"\"\n",
    "    return dp.read(\"raw_customers\").filter(F.col(\"status\") == \"active\")\n",
    "\n",
    "@dp.table(\n",
    "    name=\"premium_customers\",\n",
    "    path=\"/mnt/gold/premium\",  # Lakeflow manages writes\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    },\n",
    "    comment=\"Premium tier customers (LTV > $10,000)\"\n",
    ")\n",
    "@dp.expect(\"high_value\", \"lifetime_value > 10000\")  # Quality expectation\n",
    "@dp.expect_or_drop(\"valid_contact\", \"email IS NOT NULL AND phone IS NOT NULL\")\n",
    "def premium_customers():\n",
    "    \"\"\"Premium customer segment with quality guarantees.\"\"\"\n",
    "    return apply_premium_filter(dp.read(\"filtered_customers\"))\n",
    "\n",
    "def apply_premium_filter(df):\n",
    "    \"\"\"Pure function: testable premium customer logic.\"\"\"\n",
    "    return df.filter(F.col(\"lifetime_value\") > 10000)\n",
    "\n",
    "print(\"✅ Migration improvements:\")\n",
    "print(\"   - Removed prohibited operations (.count(), .write())\")\n",
    "print(\"   - Replaced validation with expectations\")\n",
    "print(\"   - Converted intermediate table to temporary view\")\n",
    "print(\"   - Added partitioning and table properties\")\n",
    "print(\"   - Extracted testable pure function\")\n",
    "print(\"   - Added comprehensive metadata (comments, expectations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Troubleshooting Common Pipeline Issues\n",
    "\n",
    "### Issue 1: Pipeline Fails with \"Cannot perform action in table definition\"\n",
    "\n",
    "**Symptom**: Error during pipeline creation or deployment\n",
    "```\n",
    "Error: Cannot perform action 'count' in table definition for 'my_table'\n",
    "```\n",
    "\n",
    "**Root Cause**: Prohibited operation (`.count()`, `.collect()`, `.show()`) in pipeline function\n",
    "\n",
    "**Solution**: Remove action and use declarative alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ Problematic code\n",
    "# @dp.table\n",
    "# def my_table():\n",
    "#     df = spark.table(\"source\")\n",
    "#     if df.count() == 0:  # Prohibited action\n",
    "#         raise ValueError(\"No data\")\n",
    "#     return df\n",
    "\n",
    "# ✅ Solution: Use expectations\n",
    "@dp.table\n",
    "@dp.expect_or_fail(\"has_data\", \"id IS NOT NULL\")\n",
    "def my_table_fixed():\n",
    "    \"\"\"Pipeline fails if no valid data (declarative validation).\"\"\"\n",
    "    return spark.table(\"source\")\n",
    "\n",
    "print(\"✅ Solution: Replace actions with expectations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 2: Slow Pipeline Performance\n",
    "\n",
    "**Symptom**: Pipeline takes significantly longer than expected\n",
    "\n",
    "**Diagnosis Checklist**:\n",
    "1. ✅ Check for missing partitioning on large tables\n",
    "2. ✅ Verify broadcast hints for small dimension joins\n",
    "3. ✅ Look for over-materialization (too many tables instead of views)\n",
    "4. ✅ Review shuffle operations in Spark UI\n",
    "5. ✅ Confirm auto-optimize is enabled\n",
    "\n",
    "**Solution Pattern**: Add partitioning and broadcast hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ Slow: Missing optimizations\n",
    "# @dp.table  # No partitioning\n",
    "# def slow_events():\n",
    "#     return (\n",
    "#         spark.table(\"raw.events\")  # Large table\n",
    "#         .join(spark.table(\"dim.countries\"), \"country_code\")  # Small table, no broadcast\n",
    "#     )\n",
    "\n",
    "# ✅ Fast: Optimized version\n",
    "@dp.table(\n",
    "    partition_cols=[\"event_date\"],  # Partition for queries\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "def fast_events():\n",
    "    \"\"\"Optimized with partitioning and broadcast join.\"\"\"\n",
    "    return (\n",
    "        spark.table(\"raw.events\")\n",
    "        .withColumn(\"event_date\", F.to_date(\"event_timestamp\"))\n",
    "        .join(\n",
    "            F.broadcast(spark.table(\"dim.countries\")),  # Broadcast small table\n",
    "            \"country_code\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"✅ Solution: Add partitioning and broadcast joins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 3: Expectation Failures in Production\n",
    "\n",
    "**Symptom**: Pipeline fails with expectation violations\n",
    "```\n",
    "Error: Expectation 'valid_age' failed: 150 violations found\n",
    "```\n",
    "\n",
    "**Diagnosis**:\n",
    "1. Review expectation metrics in Lakeflow UI\n",
    "2. Query violation records for patterns\n",
    "3. Assess if expectations are too strict or data has quality issues\n",
    "\n",
    "**Solution Options**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Option 1: Change FAIL to WARN for monitoring\n",
    "@dp.table\n",
    "@dp.expect(\"valid_age\", \"age >= 0 AND age <= 120\")  # WARN: monitor but don't fail\n",
    "def customers_with_monitoring():\n",
    "    \"\"\"Monitor age violations without failing pipeline.\"\"\"\n",
    "    return spark.table(\"raw.customers\")\n",
    "\n",
    "# ✅ Option 2: Change FAIL to DROP for cleansing\n",
    "@dp.table\n",
    "@dp.expect_or_drop(\"valid_age\", \"age >= 0 AND age <= 120\")  # DROP: remove bad rows\n",
    "def customers_with_cleansing():\n",
    "    \"\"\"Drop invalid age records automatically.\"\"\"\n",
    "    return spark.table(\"raw.customers\")\n",
    "\n",
    "# ✅ Option 3: Adjust expectation logic\n",
    "@dp.table\n",
    "@dp.expect_or_fail(\n",
    "    \"valid_age_or_null\",\n",
    "    \"age IS NULL OR (age >= 0 AND age <= 120)\"  # Allow nulls\n",
    ")\n",
    "def customers_with_relaxed_rule():\n",
    "    \"\"\"Accept null ages, fail only on invalid values.\"\"\"\n",
    "    return spark.table(\"raw.customers\")\n",
    "\n",
    "print(\"✅ Solution: Adjust expectation strategy based on business requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 4: Table Dependency Errors\n",
    "\n",
    "**Symptom**: Error about missing or circular dependencies\n",
    "```\n",
    "Error: Table 'orders_summary' depends on 'orders' which is not defined\n",
    "```\n",
    "\n",
    "**Root Causes**:\n",
    "1. Table name mismatch (function name vs configured name)\n",
    "2. Circular dependency (A depends on B, B depends on A)\n",
    "3. Cross-pipeline dependency not properly configured\n",
    "\n",
    "**Solution**: Verify table names and dependency graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ Problem: Name mismatch\n",
    "# @dp.table(name=\"orders_clean\")  # Configured name\n",
    "# def orders():  # Function name\n",
    "#     return spark.table(\"raw.orders\")\n",
    "# \n",
    "# @dp.table\n",
    "# def orders_summary():\n",
    "#     return dp.read(\"orders\")  # Error: references function name, not configured name\n",
    "\n",
    "# ✅ Solution: Use consistent naming\n",
    "@dp.table(name=\"orders_clean\")\n",
    "def orders_clean_table():\n",
    "    \"\"\"Use configured name consistently.\"\"\"\n",
    "    return spark.table(\"raw.orders\")\n",
    "\n",
    "@dp.table\n",
    "def orders_summary_fixed():\n",
    "    \"\"\"Reference configured name, not function name.\"\"\"\n",
    "    return dp.read(\"orders_clean\")  # Correct: uses configured name\n",
    "\n",
    "print(\"✅ Solution: Match dp.read() calls to configured table names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 5: Streaming Table Checkpoint Errors\n",
    "\n",
    "**Symptom**: Streaming table fails with checkpoint errors\n",
    "```\n",
    "Error: Checkpoint directory '/tmp/checkpoints/my_stream' is corrupt\n",
    "```\n",
    "\n",
    "**Root Causes**:\n",
    "1. Schema changes in source without checkpoint reset\n",
    "2. Manual checkpoint directory deletion\n",
    "3. Multiple pipelines using same checkpoint location\n",
    "\n",
    "**Solution**: Configure unique checkpoint paths and handle schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Best practice: Unique checkpoint paths per table\n",
    "@dp.streaming_table(\n",
    "    name=\"events_stream\",\n",
    "    table_properties={\n",
    "        \"pipelines.checkpointLocation\": \"/mnt/checkpoints/events_stream\",  # Unique path\n",
    "        \"pipelines.reset.allowed\": \"true\"  # Allow checkpoint reset if needed\n",
    "    }\n",
    ")\n",
    "def events_stream():\n",
    "    \"\"\"\n",
    "    Streaming table with explicit checkpoint configuration.\n",
    "    - Unique checkpoint path prevents conflicts\n",
    "    - reset.allowed enables recovery from schema changes\n",
    "    \"\"\"\n",
    "    return spark.readStream.table(\"raw.events\")\n",
    "\n",
    "print(\"✅ Solution: Configure unique checkpoint paths and allow resets\")\n",
    "print(\"   - Each streaming table needs unique checkpoint location\")\n",
    "print(\"   - Set reset.allowed for schema evolution scenarios\")\n",
    "print(\"   - Monitor checkpoint health in Lakeflow UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Lakeflow Best Practices\n",
    "\n",
    "### ✅ Do's\n",
    "\n",
    "**Pipeline Design**:\n",
    "- ✅ Use pure functions that return DataFrames\n",
    "- ✅ Let Lakeflow manage execution, writes, and checkpoints\n",
    "- ✅ Use expectations for data quality validation\n",
    "- ✅ Choose appropriate table types (table vs materialized view vs temporary view)\n",
    "\n",
    "**Performance Optimization**:\n",
    "- ✅ Partition large tables by common filter columns\n",
    "- ✅ Use broadcast joins for small dimension tables\n",
    "- ✅ Enable Delta Lake auto-optimize\n",
    "- ✅ Use temporary views for simple intermediate steps\n",
    "\n",
    "**Testing & Quality**:\n",
    "- ✅ Extract pure transformation functions for unit testing\n",
    "- ✅ Test expectations with sample data before deployment\n",
    "- ✅ Use integration tests with mock dependencies\n",
    "- ✅ Monitor expectation metrics in production\n",
    "\n",
    "**Code Organization**:\n",
    "- ✅ Add comprehensive metadata (comments, table properties)\n",
    "- ✅ Use consistent naming between function names and table names\n",
    "- ✅ Document dependencies and data lineage\n",
    "- ✅ Version control pipeline code\n",
    "\n",
    "### ❌ Don'ts\n",
    "\n",
    "**Prohibited Operations**:\n",
    "- ❌ Never use `.collect()`, `.count()`, `.show()` in pipeline functions\n",
    "- ❌ Never use `.write()` or `.save()` - let Lakeflow manage writes\n",
    "- ❌ Never use `.cache()` or `.persist()` - Lakeflow optimizes caching\n",
    "- ❌ Never manually manage checkpoints for streaming tables\n",
    "\n",
    "**Anti-Patterns**:\n",
    "- ❌ Don't materialize every intermediate step as a table\n",
    "- ❌ Don't skip partitioning on large tables\n",
    "- ❌ Don't use shuffle joins for small dimension tables\n",
    "- ❌ Don't mix imperative and declarative patterns\n",
    "\n",
    "**Testing & Deployment**:\n",
    "- ❌ Don't deploy without testing expectations\n",
    "- ❌ Don't skip integration testing with realistic data\n",
    "- ❌ Don't use FAIL strategy for exploratory data quality\n",
    "- ❌ Don't ignore expectation violation metrics\n",
    "\n",
    "### Key Functional Programming Alignment\n",
    "\n",
    "Lakeflow declarative pipelines **embody functional programming principles**:\n",
    "\n",
    "1. **Pure Functions**: Pipeline functions return DataFrames without side effects\n",
    "2. **Immutability**: DataFrames are immutable; transformations create new DataFrames\n",
    "3. **Lazy Evaluation**: Lakeflow optimizes the entire DAG before execution\n",
    "4. **Declarative Composition**: Describe what to compute, not how to execute\n",
    "5. **Testability**: Pure functions are naturally testable in isolation\n",
    "\n",
    "**Result**: Maintainable, performant, and reliable data pipelines with automatic optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Identify Anti-Patterns\n",
    "\n",
    "Review the following pipeline code and identify all anti-patterns. Then refactor to best practices.\n",
    "\n",
    "```python\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dlt.table\n",
    "def raw_sales():\n",
    "    df = spark.read.parquet(\"/mnt/raw/sales\")\n",
    "    if df.count() < 1000:\n",
    "        print(\"Warning: Low sales volume\")\n",
    "    return df\n",
    "\n",
    "@dlt.table\n",
    "def filtered_sales():\n",
    "    return dlt.read(\"raw_sales\").filter(F.col(\"amount\") > 0)\n",
    "\n",
    "@dlt.table\n",
    "def enriched_sales():\n",
    "    return (\n",
    "        dlt.read(\"filtered_sales\")\n",
    "        .withColumn(\"year\", F.year(\"sale_date\"))\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def sales_summary():\n",
    "    df = (\n",
    "        dlt.read(\"enriched_sales\")\n",
    "        .groupBy(\"year\").agg(F.sum(\"amount\").alias(\"total\"))\n",
    "    )\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/gold/summary\")\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Tasks**:\n",
    "1. List all anti-patterns found\n",
    "2. Refactor to use `pyspark.pipelines` (dp)\n",
    "3. Add appropriate table types (table vs temporary view)\n",
    "4. Add data quality expectations\n",
    "5. Add performance optimizations\n",
    "\n",
    "### Exercise 2: Design Table Type Strategy\n",
    "\n",
    "Given the following pipeline requirements, choose the appropriate table type for each and justify your decision:\n",
    "\n",
    "1. **customer_master**: Customer records (10M rows), read by 5+ downstream pipelines, needs partitioning by country\n",
    "2. **active_customers**: Simple filter of customer_master (status='active'), used by only 1 downstream table\n",
    "3. **customer_segments**: Expensive segmentation logic joining customers with order history, used by multiple dashboards\n",
    "4. **temp_date_filter**: Adds year/month columns for filtering, used only in next transformation step\n",
    "5. **gold_customer_analytics**: Final output for BI consumption\n",
    "\n",
    "**Tasks**:\n",
    "- For each table, choose: @dp.table, @dp.materialized_view, or @dp.temporary_view\n",
    "- Justify your choice using the decision matrix\n",
    "- Add appropriate configurations (partitioning, table properties)\n",
    "\n",
    "### Exercise 3: Testing Strategy Implementation\n",
    "\n",
    "Create a complete testing suite for the following pipeline:\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "@dp.table\n",
    "@dp.expect(\"valid_email\", \"email RLIKE '^[^@]+@[^@]+\\\\.[^@]+'\")\n",
    "@dp.expect_or_drop(\"adult_age\", \"age >= 18\")\n",
    "def validated_users():\n",
    "    return spark.table(\"raw.users\")\n",
    "\n",
    "@dp.table\n",
    "def user_purchase_summary():\n",
    "    return (\n",
    "        dp.read(\"validated_users\")\n",
    "        .join(dp.read(\"purchases\"), \"user_id\")\n",
    "        .groupBy(\"user_id\", \"email\")\n",
    "        .agg(F.sum(\"amount\").alias(\"total_spent\"))\n",
    "    )\n",
    "```\n",
    "\n",
    "**Tasks**:\n",
    "1. Write unit tests for transformation logic\n",
    "2. Create expectation tests with edge cases (invalid email, underage users)\n",
    "3. Build integration tests with mock tables\n",
    "4. Document test data requirements\n",
    "\n",
    "### Exercise 4: Performance Optimization\n",
    "\n",
    "Optimize the following slow pipeline:\n",
    "\n",
    "```python\n",
    "@dp.table\n",
    "def slow_events():\n",
    "    return spark.table(\"raw.events\")  # 100M rows, daily queries by date\n",
    "\n",
    "@dp.table\n",
    "def events_with_location():\n",
    "    return (\n",
    "        dp.read(\"slow_events\")\n",
    "        .join(spark.table(\"dim.locations\"), \"location_id\")  # 500 rows\n",
    "    )\n",
    "```\n",
    "\n",
    "**Tasks**:\n",
    "1. Add partitioning strategy\n",
    "2. Optimize the join\n",
    "3. Add Delta Lake optimization settings\n",
    "4. Estimate performance improvement\n",
    "\n",
    "### Exercise 5: Migration Planning\n",
    "\n",
    "Plan a migration from the following imperative code to declarative Lakeflow:\n",
    "\n",
    "```python\n",
    "# Current imperative code\n",
    "df_raw = spark.read.parquet(\"/mnt/raw/transactions\")\n",
    "df_filtered = df_raw.filter(F.col(\"amount\") > 0)\n",
    "df_filtered.write.format(\"delta\").mode(\"append\").save(\"/mnt/bronze/transactions\")\n",
    "\n",
    "df_bronze = spark.read.format(\"delta\").load(\"/mnt/bronze/transactions\")\n",
    "if df_bronze.filter(F.col(\"customer_id\").isNull()).count() > 0:\n",
    "    raise ValueError(\"Null customer_id found\")\n",
    "\n",
    "df_enriched = df_bronze.join(\n",
    "    spark.table(\"dim.customers\"),\n",
    "    \"customer_id\"\n",
    ")\n",
    "df_enriched.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/silver/transactions\")\n",
    "```\n",
    "\n",
    "**Tasks**:\n",
    "1. Create migration checklist\n",
    "2. Identify all prohibited operations\n",
    "3. Design declarative pipeline with expectations\n",
    "4. Add testing strategy\n",
    "5. Document rollback plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Congratulations!** You've completed Section 6 on Declarative Pipelines with `pyspark.pipelines`.\n",
    "\n",
    "**You now understand**:\n",
    "- ✅ Prohibited operations in declarative pipelines and their declarative alternatives\n",
    "- ✅ Decision criteria for table vs materialized view vs temporary view selection\n",
    "- ✅ Common performance anti-patterns and optimization strategies\n",
    "- ✅ Comprehensive testing approaches for declarative pipelines\n",
    "- ✅ Systematic migration process from imperative to declarative patterns\n",
    "- ✅ Troubleshooting techniques for common pipeline issues\n",
    "\n",
    "**Continue your learning**:\n",
    "- **Appendix 1.1**: Modular Design and Project Structure\n",
    "- **Appendix 1.2**: Dependency Management and Package Distribution\n",
    "- **Practice**: Apply these patterns to your production pipelines\n",
    "- **Databricks Documentation**: [Lakeflow Declarative Pipelines Guide](https://docs.databricks.com/workflows/delta-live-tables/index.html)\n",
    "\n",
    "**Recommended actions**:\n",
    "1. Review your existing pipelines for anti-patterns\n",
    "2. Create migration plan for imperative code\n",
    "3. Implement testing suite for critical pipelines\n",
    "4. Monitor expectation metrics in production\n",
    "\n",
    "**Remember**: Declarative pipelines leverage functional programming principles to create maintainable, testable, and performant data workflows with automatic platform optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
