{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Modular Design and Project Structure\n",
    "\n",
    "This notebook demonstrates how to organize functional PySpark code into modular, reusable, and maintainable projects.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how to structure PySpark projects for scalability\n",
    "- Learn to create reusable functional modules\n",
    "- Master abstraction layers and separation of concerns\n",
    "- Design idempotent transformations for fault tolerance\n",
    "- Build production-ready project structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local development: Uncomment the next line\n",
    "# %run 00_Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Modular Design Matters\n",
    "\n",
    "As PySpark projects grow from simple notebooks to production data pipelines, proper modular design becomes critical:\n",
    "\n",
    "**Benefits of Modular Design**:\n",
    "1. **Reusability**: Transform once, use everywhere\n",
    "2. **Testability**: Pure functions are easy to unit test\n",
    "3. **Maintainability**: Smaller, focused modules reduce cognitive load\n",
    "4. **Collaboration**: Teams can work on separate modules independently\n",
    "5. **Scalability**: Add new features without breaking existing code\n",
    "\n",
    "**Functional Programming Alignment**:\n",
    "- Modular design naturally supports pure functions\n",
    "- Separation of concerns isolates side effects\n",
    "- Composable modules enable powerful pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from typing import List, Dict, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "# Create sample customer transaction data\n",
    "def create_sample_transactions(num_records: int = 10000) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Generate sample e-commerce transaction data.\n",
    "    Pure function - deterministic data generation for examples.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    random.seed(42)  # Deterministic for reproducibility\n",
    "    \n",
    "    data = []\n",
    "    products = ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse', 'Headphones']\n",
    "    categories = ['Electronics', 'Accessories', 'Computers']\n",
    "    statuses = ['completed', 'pending', 'cancelled', 'returned']\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        product = random.choice(products)\n",
    "        data.append((\n",
    "            i + 1,  # transaction_id\n",
    "            f\"CUST_{random.randint(1, num_records // 10):05d}\",  # customer_id\n",
    "            product,  # product_name\n",
    "            random.choice(categories),  # category\n",
    "            round(random.uniform(50, 2000), 2),  # amount\n",
    "            random.randint(1, 5),  # quantity\n",
    "            random.choice(statuses),  # status\n",
    "            f\"2023-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}\",  # transaction_date\n",
    "            random.choice(['US', 'CA', 'UK', 'DE', 'FR'])  # country\n",
    "        ))\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"transaction_id\", IntegerType(), False),\n",
    "        StructField(\"customer_id\", StringType(), False),\n",
    "        StructField(\"product_name\", StringType(), False),\n",
    "        StructField(\"category\", StringType(), False),\n",
    "        StructField(\"amount\", DoubleType(), False),\n",
    "        StructField(\"quantity\", IntegerType(), False),\n",
    "        StructField(\"status\", StringType(), False),\n",
    "        StructField(\"transaction_date\", StringType(), False),\n",
    "        StructField(\"country\", StringType(), False)\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "# Generate sample data\n",
    "transactions_df = create_sample_transactions(5000)\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", F.to_date(\"transaction_date\"))\n",
    "\n",
    "print(f\"Generated {transactions_df.count():,} transactions\")\n",
    "print(\"\\nSample data:\")\n",
    "transactions_df.show(5)\n",
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anti-Pattern: Monolithic Notebook Pipelines\n",
    "\n",
    "Let's first see what NOT to do - a monolithic pipeline with everything in one place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== âŒ ANTI-PATTERN: Monolithic Pipeline ===\")\n",
    "\n",
    "# âŒ BAD: Everything in one massive function with hardcoded logic\n",
    "def monolithic_pipeline(df):\n",
    "    \"\"\"\n",
    "    ANTI-PATTERN: All logic in one function\n",
    "    Problems:\n",
    "    - Hard to test individual steps\n",
    "    - No reusability\n",
    "    - Difficult to debug\n",
    "    - Hardcoded configuration\n",
    "    - Side effects mixed with transformations\n",
    "    \"\"\"\n",
    "    # Hardcoded filtering\n",
    "    df = df.filter(F.col(\"status\") == \"completed\")\n",
    "    \n",
    "    # Hardcoded tax calculation\n",
    "    df = df.withColumn(\"tax\", F.col(\"amount\") * 0.08)\n",
    "    df = df.withColumn(\"total\", F.col(\"amount\") + F.col(\"tax\"))\n",
    "    \n",
    "    # More hardcoded logic\n",
    "    df = df.withColumn(\"revenue\", F.col(\"total\") * F.col(\"quantity\"))\n",
    "    \n",
    "    # Aggregation mixed in\n",
    "    summary = df.groupBy(\"category\").agg(\n",
    "        F.sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        F.count(\"*\").alias(\"transaction_count\")\n",
    "    )\n",
    "    \n",
    "    # Side effect: showing data (breaks lazy evaluation)\n",
    "    summary.show()  # âŒ Action inside transformation!\n",
    "    \n",
    "    return summary\n",
    "\n",
    "result = monolithic_pipeline(transactions_df)\n",
    "\n",
    "print(\"\\nâš ï¸  Problems with monolithic design:\")\n",
    "print(\"  - Cannot test individual transformation steps\")\n",
    "print(\"  - Cannot reuse logic in other pipelines\")\n",
    "print(\"  - Hardcoded values (tax rate, status filter)\")\n",
    "print(\"  - Side effects (show()) break testability\")\n",
    "print(\"  - Single point of failure\")\n",
    "print(\"  - Difficult to extend or modify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular Design Pattern: Separation of Concerns\n",
    "\n",
    "Let's refactor the monolithic pipeline into modular, reusable components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== âœ… BEST PRACTICE: Modular Design ===\")\n",
    "\n",
    "# ========================================\n",
    "# MODULE 1: Data Cleaning\n",
    "# ========================================\n",
    "\n",
    "class DataCleaning:\n",
    "    \"\"\"\n",
    "    Module for data cleaning and validation transformations.\n",
    "    All methods are pure functions (DataFrame in, DataFrame out).\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_by_status(df: DataFrame, status: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Filter transactions by status.\n",
    "        Configurable and reusable.\n",
    "        \"\"\"\n",
    "        return df.filter(F.col(\"status\") == status)\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_valid_amounts(df: DataFrame, min_amount: float = 0) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Filter out invalid or negative amounts.\n",
    "        \"\"\"\n",
    "        return df.filter(\n",
    "            (F.col(\"amount\") > min_amount) & \n",
    "            (F.col(\"amount\").isNotNull())\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_duplicates(df: DataFrame, key_columns: List[str]) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Remove duplicate records based on key columns.\n",
    "        \"\"\"\n",
    "        return df.dropDuplicates(key_columns)\n",
    "    \n",
    "    @staticmethod\n",
    "    def standardize_dates(df: DataFrame, date_column: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Ensure date column is in proper date format.\n",
    "        \"\"\"\n",
    "        return df.withColumn(date_column, F.to_date(F.col(date_column)))\n",
    "\n",
    "# ========================================\n",
    "# MODULE 2: Business Logic\n",
    "# ========================================\n",
    "\n",
    "class BusinessLogic:\n",
    "    \"\"\"\n",
    "    Module for business logic transformations.\n",
    "    All methods are pure functions with configurable parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_tax(df: DataFrame, tax_rate: float) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Calculate tax based on configurable rate.\n",
    "        \"\"\"\n",
    "        return df.withColumn(\"tax\", F.col(\"amount\") * F.lit(tax_rate))\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_total(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Calculate total including tax.\n",
    "        Assumes 'amount' and 'tax' columns exist.\n",
    "        \"\"\"\n",
    "        return df.withColumn(\"total\", F.col(\"amount\") + F.col(\"tax\"))\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_revenue(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Calculate revenue (total * quantity).\n",
    "        \"\"\"\n",
    "        return df.withColumn(\"revenue\", F.col(\"total\") * F.col(\"quantity\"))\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_discount(df: DataFrame, discount_rate: float, \n",
    "                      min_amount: float = 100) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Apply discount to high-value transactions.\n",
    "        \"\"\"\n",
    "        return df.withColumn(\"discount\",\n",
    "            F.when(F.col(\"amount\") >= min_amount, \n",
    "                   F.col(\"amount\") * F.lit(discount_rate))\n",
    "             .otherwise(0))\n",
    "\n",
    "# ========================================\n",
    "# MODULE 3: Analytics\n",
    "# ========================================\n",
    "\n",
    "class Analytics:\n",
    "    \"\"\"\n",
    "    Module for analytical aggregations and metrics.\n",
    "    All methods are pure functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def revenue_by_category(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Aggregate revenue by category.\n",
    "        \"\"\"\n",
    "        return df.groupBy(\"category\").agg(\n",
    "            F.sum(\"revenue\").alias(\"total_revenue\"),\n",
    "            F.count(\"*\").alias(\"transaction_count\"),\n",
    "            F.avg(\"revenue\").alias(\"avg_revenue\")\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def revenue_by_country(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Aggregate revenue by country.\n",
    "        \"\"\"\n",
    "        return df.groupBy(\"country\").agg(\n",
    "            F.sum(\"revenue\").alias(\"total_revenue\"),\n",
    "            F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def customer_lifetime_value(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Pure function: Calculate customer lifetime value metrics.\n",
    "        \"\"\"\n",
    "        return df.groupBy(\"customer_id\").agg(\n",
    "            F.sum(\"revenue\").alias(\"lifetime_value\"),\n",
    "            F.count(\"*\").alias(\"purchase_count\"),\n",
    "            F.avg(\"revenue\").alias(\"avg_purchase_value\"),\n",
    "            F.min(\"transaction_date\").alias(\"first_purchase\"),\n",
    "            F.max(\"transaction_date\").alias(\"last_purchase\")\n",
    "        )\n",
    "\n",
    "print(\"âœ… Modular design created with three separate modules:\")\n",
    "print(\"  1. DataCleaning - Pure data cleaning functions\")\n",
    "print(\"  2. BusinessLogic - Pure business transformation functions\")\n",
    "print(\"  3. Analytics - Pure analytical aggregation functions\")\n",
    "print(\"\\nâœ… Benefits:\")\n",
    "print(\"  - Each function is independently testable\")\n",
    "print(\"  - Functions are reusable across pipelines\")\n",
    "print(\"  - Configuration through parameters (not hardcoded)\")\n",
    "print(\"  - Clear separation of concerns\")\n",
    "print(\"  - Easy to extend and maintain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing Modular Functions into Pipelines\n",
    "\n",
    "Now let's compose these modules into complete, readable pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Composing Modular Functions ===\")\n",
    "\n",
    "# âœ… GOOD: Pipeline composition with modular functions\n",
    "def build_revenue_pipeline(df: DataFrame, tax_rate: float = 0.08) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pure function: Compose modular functions into a revenue calculation pipeline.\n",
    "    No side effects - returns transformed DataFrame.\n",
    "    \"\"\"\n",
    "    return (df\n",
    "        .transform(DataCleaning.filter_by_status, \"completed\")\n",
    "        .transform(DataCleaning.filter_valid_amounts)\n",
    "        .transform(DataCleaning.remove_duplicates, [\"transaction_id\"])\n",
    "        .transform(BusinessLogic.calculate_tax, tax_rate)\n",
    "        .transform(BusinessLogic.calculate_total)\n",
    "        .transform(BusinessLogic.calculate_revenue)\n",
    "    )\n",
    "\n",
    "# âœ… GOOD: Different pipeline for different analysis\n",
    "def build_discount_pipeline(df: DataFrame, tax_rate: float = 0.08, \n",
    "                           discount_rate: float = 0.10) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pure function: Pipeline with discount calculation.\n",
    "    Reuses existing modules and adds discount logic.\n",
    "    \"\"\"\n",
    "    return (df\n",
    "        .transform(DataCleaning.filter_by_status, \"completed\")\n",
    "        .transform(DataCleaning.filter_valid_amounts)\n",
    "        .transform(BusinessLogic.calculate_tax, tax_rate)\n",
    "        .transform(BusinessLogic.calculate_total)\n",
    "        .transform(BusinessLogic.apply_discount, discount_rate)\n",
    "        .transform(BusinessLogic.calculate_revenue)\n",
    "    )\n",
    "\n",
    "# Test the modular pipelines\n",
    "print(\"\\n1. Testing revenue pipeline:\")\n",
    "revenue_df = build_revenue_pipeline(transactions_df, tax_rate=0.10)\n",
    "print(f\"Processed {revenue_df.count():,} transactions\")\n",
    "revenue_df.select(\"transaction_id\", \"amount\", \"tax\", \"total\", \"revenue\").show(5)\n",
    "\n",
    "print(\"\\n2. Testing discount pipeline:\")\n",
    "discount_df = build_discount_pipeline(transactions_df, tax_rate=0.10, discount_rate=0.15)\n",
    "print(f\"Processed {discount_df.count():,} transactions with discounts\")\n",
    "discount_df.select(\"transaction_id\", \"amount\", \"discount\", \"total\", \"revenue\").show(5)\n",
    "\n",
    "# Apply analytics\n",
    "print(\"\\n3. Applying analytics:\")\n",
    "category_summary = Analytics.revenue_by_category(revenue_df)\n",
    "print(\"Revenue by Category:\")\n",
    "category_summary.show()\n",
    "\n",
    "country_summary = Analytics.revenue_by_country(revenue_df)\n",
    "print(\"Revenue by Country:\")\n",
    "country_summary.show()\n",
    "\n",
    "print(\"\\nâœ… Modular pipelines demonstrate:\")\n",
    "print(\"  - Functions are reusable across different pipelines\")\n",
    "print(\"  - Easy to test individual components\")\n",
    "print(\"  - Configuration through parameters\")\n",
    "print(\"  - Clear data flow and transformations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Management with Immutable Dataclasses\n",
    "\n",
    "Use immutable configuration objects for pipeline parameterization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Configuration Management Pattern ===\")\n",
    "\n",
    "@dataclass(frozen=True)  # Immutable configuration\n",
    "class PipelineConfig:\n",
    "    \"\"\"\n",
    "    Immutable configuration for revenue pipeline.\n",
    "    Frozen dataclass ensures no accidental mutations.\n",
    "    \"\"\"\n",
    "    tax_rate: float = 0.08\n",
    "    discount_rate: float = 0.10\n",
    "    min_discount_amount: float = 100.0\n",
    "    status_filter: str = \"completed\"\n",
    "    min_valid_amount: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration on initialization\"\"\"\n",
    "        if self.tax_rate < 0 or self.tax_rate > 1:\n",
    "            raise ValueError(f\"Tax rate must be between 0 and 1, got {self.tax_rate}\")\n",
    "        if self.discount_rate < 0 or self.discount_rate > 1:\n",
    "            raise ValueError(f\"Discount rate must be between 0 and 1, got {self.discount_rate}\")\n",
    "\n",
    "# âœ… GOOD: Pipeline using configuration object\n",
    "def build_configurable_pipeline(df: DataFrame, config: PipelineConfig) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pure function: Pipeline driven by immutable configuration.\n",
    "    Enables different configurations without code changes.\n",
    "    \"\"\"\n",
    "    return (df\n",
    "        .transform(DataCleaning.filter_by_status, config.status_filter)\n",
    "        .transform(DataCleaning.filter_valid_amounts, config.min_valid_amount)\n",
    "        .transform(DataCleaning.remove_duplicates, [\"transaction_id\"])\n",
    "        .transform(BusinessLogic.calculate_tax, config.tax_rate)\n",
    "        .transform(BusinessLogic.calculate_total)\n",
    "        .transform(BusinessLogic.apply_discount, config.discount_rate, config.min_discount_amount)\n",
    "        .transform(BusinessLogic.calculate_revenue)\n",
    "    )\n",
    "\n",
    "# Test with different configurations\n",
    "print(\"\\n1. Standard configuration:\")\n",
    "standard_config = PipelineConfig()\n",
    "standard_result = build_configurable_pipeline(transactions_df, standard_config)\n",
    "print(f\"Tax rate: {standard_config.tax_rate}, Discount: {standard_config.discount_rate}\")\n",
    "print(f\"Processed: {standard_result.count():,} transactions\")\n",
    "\n",
    "print(\"\\n2. High-tax configuration:\")\n",
    "high_tax_config = PipelineConfig(tax_rate=0.15, discount_rate=0.05)\n",
    "high_tax_result = build_configurable_pipeline(transactions_df, high_tax_config)\n",
    "print(f\"Tax rate: {high_tax_config.tax_rate}, Discount: {high_tax_config.discount_rate}\")\n",
    "print(f\"Processed: {high_tax_result.count():,} transactions\")\n",
    "\n",
    "print(\"\\n3. Low-discount threshold configuration:\")\n",
    "low_threshold_config = PipelineConfig(discount_rate=0.20, min_discount_amount=50.0)\n",
    "low_threshold_result = build_configurable_pipeline(transactions_df, low_threshold_config)\n",
    "print(f\"Discount: {low_threshold_config.discount_rate}, Min amount: ${low_threshold_config.min_discount_amount}\")\n",
    "print(f\"Processed: {low_threshold_result.count():,} transactions\")\n",
    "\n",
    "# Compare average discounts\n",
    "print(\"\\nComparing average discounts:\")\n",
    "for name, result in [(\"Standard\", standard_result), \n",
    "                     (\"High Tax\", high_tax_result),\n",
    "                     (\"Low Threshold\", low_threshold_result)]:\n",
    "    avg_discount = result.agg(F.avg(\"discount\")).collect()[0][0]\n",
    "    print(f\"  {name}: ${avg_discount:.2f} average discount\")\n",
    "\n",
    "print(\"\\nâœ… Configuration benefits:\")\n",
    "print(\"  - Immutable configuration prevents accidental changes\")\n",
    "print(\"  - Easy to test with different configurations\")\n",
    "print(\"  - Validation at configuration creation time\")\n",
    "print(\"  - Type-safe parameter passing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstraction Layers: Extract-Transform-Load Pattern\n",
    "\n",
    "Separate data extraction, transformation, and loading into distinct layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Abstraction Layers: ETL Pattern ===\")\n",
    "\n",
    "# ========================================\n",
    "# LAYER 1: Extraction\n",
    "# ========================================\n",
    "\n",
    "class DataExtraction:\n",
    "    \"\"\"\n",
    "    Extraction layer: Responsible for reading data from sources.\n",
    "    Encapsulates all I/O operations in one place.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_transactions(path: str, format: str = \"parquet\") -> DataFrame:\n",
    "        \"\"\"\n",
    "        Read transaction data from storage.\n",
    "        Centralized data reading logic.\n",
    "        \"\"\"\n",
    "        return spark.read.format(format).load(path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_with_schema(path: str, schema: StructType, format: str = \"parquet\") -> DataFrame:\n",
    "        \"\"\"\n",
    "        Read data with explicit schema enforcement.\n",
    "        \"\"\"\n",
    "        return spark.read.format(format).schema(schema).load(path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_incremental(path: str, checkpoint_column: str, \n",
    "                        last_checkpoint: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Read incremental data based on checkpoint.\n",
    "        \"\"\"\n",
    "        df = spark.read.format(\"delta\").load(path)\n",
    "        return df.filter(F.col(checkpoint_column) > F.lit(last_checkpoint))\n",
    "\n",
    "# ========================================\n",
    "# LAYER 2: Transformation (Already defined)\n",
    "# ========================================\n",
    "# Uses DataCleaning, BusinessLogic, Analytics modules\n",
    "\n",
    "# ========================================\n",
    "# LAYER 3: Loading\n",
    "# ========================================\n",
    "\n",
    "class DataLoading:\n",
    "    \"\"\"\n",
    "    Loading layer: Responsible for writing data to destinations.\n",
    "    Encapsulates all write operations and side effects.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_to_delta(df: DataFrame, path: str, mode: str = \"overwrite\",\n",
    "                      partition_by: Optional[List[str]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Write DataFrame to Delta Lake table.\n",
    "        Side effect: Persists data to storage.\n",
    "        \"\"\"\n",
    "        writer = df.write.format(\"delta\").mode(mode)\n",
    "        if partition_by:\n",
    "            writer = writer.partitionBy(*partition_by)\n",
    "        writer.save(path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_to_parquet(df: DataFrame, path: str, mode: str = \"overwrite\") -> None:\n",
    "        \"\"\"\n",
    "        Write DataFrame to Parquet format.\n",
    "        \"\"\"\n",
    "        df.write.format(\"parquet\").mode(mode).save(path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_or_replace_view(df: DataFrame, view_name: str, \n",
    "                              global_view: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Create temporary view for SQL access.\n",
    "        \"\"\"\n",
    "        if global_view:\n",
    "            df.createOrReplaceGlobalTempView(view_name)\n",
    "        else:\n",
    "            df.createOrReplaceTempView(view_name)\n",
    "\n",
    "# ========================================\n",
    "# ORCHESTRATION LAYER\n",
    "# ========================================\n",
    "\n",
    "class PipelineOrchestrator:\n",
    "    \"\"\"\n",
    "    Orchestration layer: Coordinates ETL flow.\n",
    "    Separates pure transformation logic from I/O side effects.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def run_revenue_analytics_pipeline(source_path: str, destination_path: str,\n",
    "                                      config: PipelineConfig) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Complete ETL pipeline with clear separation of concerns.\n",
    "        Returns metrics about the pipeline execution.\n",
    "        \"\"\"\n",
    "        # EXTRACT (Side effect: I/O)\n",
    "        print(\"ðŸ“¥ EXTRACT: Reading source data...\")\n",
    "        raw_df = DataExtraction.read_transactions(source_path)\n",
    "        input_count = raw_df.count()\n",
    "        \n",
    "        # TRANSFORM (Pure functions - no side effects)\n",
    "        print(\"ðŸ”„ TRANSFORM: Applying transformations...\")\n",
    "        transformed_df = build_configurable_pipeline(raw_df, config)\n",
    "        output_count = transformed_df.count()\n",
    "        \n",
    "        # Analytical transformations\n",
    "        category_summary = Analytics.revenue_by_category(transformed_df)\n",
    "        country_summary = Analytics.revenue_by_country(transformed_df)\n",
    "        \n",
    "        # LOAD (Side effects: I/O and view creation)\n",
    "        print(\"ðŸ’¾ LOAD: Writing results...\")\n",
    "        DataLoading.write_to_delta(transformed_df, f\"{destination_path}/transactions\", \n",
    "                                  partition_by=[\"country\"])\n",
    "        DataLoading.write_to_delta(category_summary, f\"{destination_path}/category_summary\")\n",
    "        DataLoading.write_to_delta(country_summary, f\"{destination_path}/country_summary\")\n",
    "        DataLoading.create_or_replace_view(transformed_df, \"revenue_transactions\")\n",
    "        \n",
    "        # Return metrics (not side effect - just return value)\n",
    "        return {\n",
    "            \"input_records\": input_count,\n",
    "            \"output_records\": output_count,\n",
    "            \"filtered_records\": input_count - output_count,\n",
    "            \"categories\": category_summary.count(),\n",
    "            \"countries\": country_summary.count()\n",
    "        }\n",
    "\n",
    "# Demonstrate the layered architecture\n",
    "print(\"\\nDemonstrating layered ETL architecture:\")\n",
    "\n",
    "# First, write some sample data for extraction\n",
    "sample_path = \"/tmp/sample_transactions\"\n",
    "output_path = \"/tmp/revenue_analytics\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.rm(sample_path, True)\n",
    "    dbutils.fs.rm(output_path, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Write sample data\n",
    "transactions_df.write.format(\"parquet\").mode(\"overwrite\").save(sample_path)\n",
    "print(f\"âœ… Sample data written to {sample_path}\")\n",
    "\n",
    "# Run the orchestrated pipeline\n",
    "print(\"\\nðŸš€ Running orchestrated ETL pipeline...\")\n",
    "config = PipelineConfig(tax_rate=0.10, discount_rate=0.15)\n",
    "metrics = PipelineOrchestrator.run_revenue_analytics_pipeline(\n",
    "    source_path=sample_path,\n",
    "    destination_path=output_path,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Pipeline Execution Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key}: {value:,}\")\n",
    "\n",
    "print(\"\\nâœ… Layered architecture benefits:\")\n",
    "print(\"  - Clear separation: Extract, Transform, Load\")\n",
    "print(\"  - Side effects isolated to Extract and Load layers\")\n",
    "print(\"  - Transform layer is pure and testable\")\n",
    "print(\"  - Orchestrator coordinates the flow\")\n",
    "print(\"  - Easy to swap implementations (e.g., different sources)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idempotent Transformations for Fault Tolerance\n",
    "\n",
    "Design transformations that produce the same result when applied multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Idempotent Transformations ===\")\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ProcessingMetadata:\n",
    "    \"\"\"\n",
    "    Metadata to track processing for idempotency.\n",
    "    \"\"\"\n",
    "    processed_at: str\n",
    "    processing_version: str\n",
    "    is_processed: bool = True\n",
    "\n",
    "class IdempotentTransformations:\n",
    "    \"\"\"\n",
    "    Idempotent transformation patterns.\n",
    "    Multiple applications yield the same result.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mark_as_processed(df: DataFrame, version: str = \"1.0\") -> DataFrame:\n",
    "        \"\"\"\n",
    "        Idempotent: Mark records as processed.\n",
    "        Multiple applications don't change already marked records.\n",
    "        \"\"\"\n",
    "        return df.withColumn(\"processed_at\", \n",
    "            F.when(F.col(\"processed_at\").isNull(), F.current_timestamp())\n",
    "             .otherwise(F.col(\"processed_at\"))\n",
    "        ).withColumn(\"processing_version\",\n",
    "            F.when(F.col(\"processing_version\").isNull(), F.lit(version))\n",
    "             .otherwise(F.col(\"processing_version\"))\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def deduplicate_by_key(df: DataFrame, key_columns: List[str], \n",
    "                          order_by: str = \"transaction_date\") -> DataFrame:\n",
    "        \"\"\"\n",
    "        Idempotent: Remove duplicates, keeping most recent.\n",
    "        Multiple applications yield the same result.\n",
    "        \"\"\"\n",
    "        from pyspark.sql.window import Window\n",
    "        \n",
    "        window = Window.partitionBy(*key_columns).orderBy(F.desc(order_by))\n",
    "        \n",
    "        return (df\n",
    "            .withColumn(\"row_num\", F.row_number().over(window))\n",
    "            .filter(F.col(\"row_num\") == 1)\n",
    "            .drop(\"row_num\")\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def standardize_column_names(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Idempotent: Standardize column names to lowercase with underscores.\n",
    "        Multiple applications don't change already standardized names.\n",
    "        \"\"\"\n",
    "        for col_name in df.columns:\n",
    "            # Convert to lowercase and replace spaces with underscores\n",
    "            new_name = col_name.lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "            if new_name != col_name:\n",
    "                df = df.withColumnRenamed(col_name, new_name)\n",
    "        return df\n",
    "\n",
    "# Test idempotency\n",
    "print(\"\\nTesting idempotent transformations:\")\n",
    "\n",
    "test_df = transactions_df.limit(100)\n",
    "\n",
    "# Apply idempotent transformation once\n",
    "print(\"\\n1. First application:\")\n",
    "result1 = IdempotentTransformations.mark_as_processed(test_df)\n",
    "processed_count1 = result1.filter(F.col(\"processed_at\").isNotNull()).count()\n",
    "print(f\"Records with processed_at: {processed_count1}\")\n",
    "\n",
    "# Apply same transformation again (idempotent)\n",
    "print(\"\\n2. Second application (should be idempotent):\")\n",
    "result2 = IdempotentTransformations.mark_as_processed(result1)\n",
    "processed_count2 = result2.filter(F.col(\"processed_at\").isNotNull()).count()\n",
    "print(f\"Records with processed_at: {processed_count2}\")\n",
    "\n",
    "# Verify idempotency\n",
    "sample1 = result1.select(\"transaction_id\", \"processed_at\", \"processing_version\").collect()[0]\n",
    "sample2 = result2.select(\"transaction_id\", \"processed_at\", \"processing_version\").collect()[0]\n",
    "\n",
    "print(f\"\\nâœ… Idempotency verified:\")\n",
    "print(f\"  First application: {sample1['processed_at']}\")\n",
    "print(f\"  Second application: {sample2['processed_at']}\")\n",
    "print(f\"  Values unchanged: {sample1['processed_at'] == sample2['processed_at']}\")\n",
    "\n",
    "# Test deduplication idempotency\n",
    "print(\"\\n3. Testing deduplication idempotency:\")\n",
    "\n",
    "# Create data with duplicates\n",
    "with_duplicates = test_df.union(test_df.limit(10))  # Add 10 duplicates\n",
    "print(f\"Original with duplicates: {with_duplicates.count()} records\")\n",
    "\n",
    "# First deduplication\n",
    "dedup1 = IdempotentTransformations.deduplicate_by_key(with_duplicates, [\"transaction_id\"])\n",
    "print(f\"After first deduplication: {dedup1.count()} records\")\n",
    "\n",
    "# Second deduplication (should yield same result)\n",
    "dedup2 = IdempotentTransformations.deduplicate_by_key(dedup1, [\"transaction_id\"])\n",
    "print(f\"After second deduplication: {dedup2.count()} records\")\n",
    "print(f\"âœ… Idempotent: {dedup1.count() == dedup2.count()}\")\n",
    "\n",
    "print(\"\\nâœ… Idempotent transformation benefits:\")\n",
    "print(\"  - Safe to retry failed jobs\")\n",
    "print(\"  - No unintended side effects from re-processing\")\n",
    "print(\"  - Fault-tolerant pipeline design\")\n",
    "print(\"  - Deterministic behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure Best Practices\n",
    "\n",
    "Recommended directory structure for production PySpark projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Recommended Project Structure ===\")\n",
    "\n",
    "project_structure = \"\"\"\n",
    "my_pyspark_project/\n",
    "â”‚\n",
    "â”œâ”€â”€ src/                           # Source code\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ extraction/                # Data extraction layer\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ readers.py            # Pure functions for reading data\n",
    "â”‚   â”‚   â””â”€â”€ sources.py            # Source configurations\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ transformation/            # Transformation layer\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ cleaning.py           # Data cleaning module\n",
    "â”‚   â”‚   â”œâ”€â”€ business_logic.py     # Business transformations\n",
    "â”‚   â”‚   â”œâ”€â”€ enrichment.py         # Data enrichment functions\n",
    "â”‚   â”‚   â””â”€â”€ analytics.py          # Analytical aggregations\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ loading/                   # Data loading layer\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ writers.py            # Data writers\n",
    "â”‚   â”‚   â””â”€â”€ destinations.py       # Destination configurations\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ pipelines/                 # Pipeline orchestration\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ revenue_pipeline.py   # Revenue analytics pipeline\n",
    "â”‚   â”‚   â””â”€â”€ customer_pipeline.py  # Customer analytics pipeline\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ utils/                     # Utility functions\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ spark_utils.py        # Spark session management\n",
    "â”‚   â”‚   â”œâ”€â”€ schema_utils.py       # Schema definitions and validation\n",
    "â”‚   â”‚   â””â”€â”€ config_utils.py       # Configuration management\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ config/                    # Configuration files\n",
    "â”‚       â”œâ”€â”€ __init__.py\n",
    "â”‚       â”œâ”€â”€ pipeline_config.py    # Pipeline configurations\n",
    "â”‚       â””â”€â”€ environments.py       # Environment-specific configs\n",
    "â”‚\n",
    "â”œâ”€â”€ tests/                         # Test directory\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ conftest.py               # Pytest fixtures\n",
    "â”‚   â”œâ”€â”€ unit/                     # Unit tests\n",
    "â”‚   â”‚   â”œâ”€â”€ test_cleaning.py\n",
    "â”‚   â”‚   â”œâ”€â”€ test_business_logic.py\n",
    "â”‚   â”‚   â””â”€â”€ test_analytics.py\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ integration/              # Integration tests\n",
    "â”‚   â”‚   â”œâ”€â”€ test_revenue_pipeline.py\n",
    "â”‚   â”‚   â””â”€â”€ test_customer_pipeline.py\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ fixtures/                 # Test data fixtures\n",
    "â”‚       â””â”€â”€ sample_data.py\n",
    "â”‚\n",
    "â”œâ”€â”€ notebooks/                     # Databricks notebooks\n",
    "â”‚   â”œâ”€â”€ exploratory/              # Exploratory analysis\n",
    "â”‚   â”œâ”€â”€ production/               # Production job notebooks\n",
    "â”‚   â””â”€â”€ validation/               # Data validation notebooks\n",
    "â”‚\n",
    "â”œâ”€â”€ scripts/                       # Utility scripts\n",
    "â”‚   â”œâ”€â”€ setup_environment.py\n",
    "â”‚   â””â”€â”€ run_tests.py\n",
    "â”‚\n",
    "â”œâ”€â”€ docs/                          # Documentation\n",
    "â”‚   â”œâ”€â”€ architecture.md\n",
    "â”‚   â”œâ”€â”€ pipeline_design.md\n",
    "â”‚   â””â”€â”€ deployment.md\n",
    "â”‚\n",
    "â”œâ”€â”€ .github/                       # CI/CD configuration\n",
    "â”‚   â””â”€â”€ workflows/\n",
    "â”‚       â””â”€â”€ ci.yml\n",
    "â”‚\n",
    "â”œâ”€â”€ requirements.txt               # Python dependencies\n",
    "â”œâ”€â”€ setup.py                       # Package setup\n",
    "â”œâ”€â”€ pytest.ini                     # Pytest configuration\n",
    "â”œâ”€â”€ .gitignore                     # Git ignore rules\n",
    "â””â”€â”€ README.md                      # Project documentation\n",
    "\"\"\"\n",
    "\n",
    "print(project_structure)\n",
    "\n",
    "print(\"\\nâœ… Project Structure Principles:\")\n",
    "print(\"\\n1. Separation of Concerns:\")\n",
    "print(\"   - extraction/ - Data reading logic\")\n",
    "print(\"   - transformation/ - Pure transformation functions\")\n",
    "print(\"   - loading/ - Data writing logic\")\n",
    "print(\"   - pipelines/ - Orchestration\")\n",
    "\n",
    "print(\"\\n2. Modular Organization:\")\n",
    "print(\"   - Each module has single responsibility\")\n",
    "print(\"   - Related functions grouped together\")\n",
    "print(\"   - Clear import paths\")\n",
    "\n",
    "print(\"\\n3. Testability:\")\n",
    "print(\"   - tests/ mirrors src/ structure\")\n",
    "print(\"   - Unit tests for pure functions\")\n",
    "print(\"   - Integration tests for pipelines\")\n",
    "print(\"   - Shared fixtures in conftest.py\")\n",
    "\n",
    "print(\"\\n4. Configuration Management:\")\n",
    "print(\"   - Centralized config/ directory\")\n",
    "print(\"   - Environment-specific configurations\")\n",
    "print(\"   - Immutable configuration objects\")\n",
    "\n",
    "print(\"\\n5. Documentation:\")\n",
    "print(\"   - README.md for project overview\")\n",
    "print(\"   - docs/ for detailed documentation\")\n",
    "print(\"   - Docstrings in all functions\")\n",
    "\n",
    "print(\"\\n6. CI/CD Integration:\")\n",
    "print(\"   - .github/workflows/ for automation\")\n",
    "print(\"   - Automated testing on commits\")\n",
    "print(\"   - Deployment scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Modular Design Benefits**:\n",
    "   - Reusability across projects and pipelines\n",
    "   - Independent testing of components\n",
    "   - Easier maintenance and debugging\n",
    "   - Clear separation of concerns\n",
    "   - Scalable team collaboration\n",
    "\n",
    "2. **Functional Module Patterns**:\n",
    "   - Pure functions for all transformations\n",
    "   - Immutable configuration objects\n",
    "   - Composable pipeline building\n",
    "   - Parameter injection over hardcoding\n",
    "\n",
    "3. **Abstraction Layers**:\n",
    "   - Extract layer: I/O for reading data\n",
    "   - Transform layer: Pure business logic\n",
    "   - Load layer: I/O for writing data\n",
    "   - Orchestration layer: Coordinates flow\n",
    "\n",
    "4. **Idempotent Transformations**:\n",
    "   - Safe to retry on failure\n",
    "   - Deterministic behavior\n",
    "   - Fault-tolerant design\n",
    "   - Production-ready patterns\n",
    "\n",
    "5. **Project Structure**:\n",
    "   - Organized by layer and concern\n",
    "   - Testable architecture\n",
    "   - Clear module boundaries\n",
    "   - CI/CD ready\n",
    "\n",
    "**Best Practices for Modular PySpark**:\n",
    "- Keep transformation modules pure (DataFrame in, DataFrame out)\n",
    "- Isolate side effects to extraction and loading layers\n",
    "- Use immutable configuration objects\n",
    "- Design idempotent transformations for fault tolerance\n",
    "- Structure projects for team collaboration\n",
    "- Write comprehensive unit tests for all modules\n",
    "- Document module responsibilities clearly\n",
    "\n",
    "**Next Steps**: In the next notebook (6.2), we'll explore dependency management and package distribution for PySpark projects in Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Practice modular design with your own data:\n",
    "\n",
    "1. Create a modular data cleaning module with at least 3 pure functions\n",
    "2. Create a business logic module specific to your domain\n",
    "3. Create an analytics module with aggregation functions\n",
    "4. Build a configurable pipeline using immutable configuration\n",
    "5. Implement idempotent transformations for your use case\n",
    "6. Design extraction and loading layers\n",
    "7. Write unit tests for each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "\n",
    "# 1. Create your data cleaning module\n",
    "class MyDataCleaning:\n",
    "    \"\"\"Your data cleaning transformations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def your_cleaning_function(df: DataFrame) -> DataFrame:\n",
    "        # Your implementation\n",
    "        pass\n",
    "\n",
    "# 2. Create your business logic module\n",
    "class MyBusinessLogic:\n",
    "    \"\"\"Your business transformations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def your_business_function(df: DataFrame) -> DataFrame:\n",
    "        # Your implementation\n",
    "        pass\n",
    "\n",
    "# 3. Create your analytics module\n",
    "class MyAnalytics:\n",
    "    \"\"\"Your analytical aggregations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def your_analytics_function(df: DataFrame) -> DataFrame:\n",
    "        # Your implementation\n",
    "        pass\n",
    "\n",
    "# 4. Create your configuration\n",
    "@dataclass(frozen=True)\n",
    "class MyPipelineConfig:\n",
    "    \"\"\"Your pipeline configuration\"\"\"\n",
    "    # Your configuration parameters\n",
    "    pass\n",
    "\n",
    "# 5. Build your pipeline\n",
    "def build_my_pipeline(df: DataFrame, config: MyPipelineConfig) -> DataFrame:\n",
    "    \"\"\"Your modular pipeline\"\"\"\n",
    "    # Your pipeline composition\n",
    "    pass\n",
    "\n",
    "# 6. Test your modules\n",
    "def test_my_modules():\n",
    "    \"\"\"Test your modular functions\"\"\"\n",
    "    # Your tests\n",
    "    pass\n",
    "\n",
    "# Run your exercise\n",
    "# test_my_modules()\n",
    "# my_config = MyPipelineConfig(...)\n",
    "# result = build_my_pipeline(your_data, my_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
