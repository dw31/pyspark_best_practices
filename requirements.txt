# PySpark Best Practices - Local Development Dependencies
# Compatible with both local Python environments and Databricks Runtime

# Core PySpark and Spark
pyspark>=3.4.0,<4.0.0

# Table Formats - Delta Lake and Apache Iceberg
delta-spark>=3.0.0  # Delta Lake for ACID transactions
pyiceberg[pyarrow,sql]>=0.5.0  # Apache Iceberg for data lakehouse

# Data Processing
pandas>=1.5.0,<3.0.0
pyarrow>=12.0.0  # Arrow for efficient data transfer
numpy>=1.23.0,<2.0.0

# Testing Framework
pytest>=7.4.0
pytest-cov>=4.1.0
pytest-xdist>=3.3.0  # Parallel test execution
pytest-mock>=3.11.0
chispa>=0.9.0  # PySpark DataFrame testing utilities

# Data Quality and Validation
great-expectations>=0.17.0  # Optional: Advanced data validation

# Configuration Management
pyyaml>=6.0
python-dotenv>=1.0.0

# Development Tools
jupyter>=1.0.0
jupyterlab>=4.0.0
notebook>=7.0.0

# Code Quality (Optional)
black>=23.0.0  # Code formatting
ruff>=0.1.0  # Fast Python linter
mypy>=1.5.0  # Static type checking

# Documentation (Optional)
mkdocs>=1.5.0  # Documentation generation
mkdocs-material>=9.0.0

# Additional utilities
tabulate>=0.9.0  # Pretty-print tabular data
